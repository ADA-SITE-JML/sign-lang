{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in 0.14. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages/torchvision/transforms/_transforms_video.py:25: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in 0.14. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torch.nn.functional import InterpolationMode\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import torch\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "import torchviz\n",
    "from pytorchvideo.data import LabeledVideoDataset, make_clip_sampler\n",
    "from torchvision.models import squeezenet1_1, SqueezeNet1_1_Weights\n",
    "from torchvision import transforms\n",
    "\n",
    "from pytorch_lightning import LightningModule, seed_everything, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "from sklearn.metrics import classification_report\n",
    "import torchmetrics\n",
    "\n",
    "from pytorchvideo.data import labeled_video_dataset\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    "    Permute,   \n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomAdjustSharpness,\n",
    "    Resize,\n",
    "    RandomHorizontalFlip\n",
    ")\n",
    "\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "video_transform = Compose([\n",
    "    ApplyTransformToKey(key=\"video\",\n",
    "    transform=Compose([\n",
    "        UniformTemporalSubsample(25),\n",
    "        Lambda(lambda x: x/255),\n",
    "        Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "        # RandomShortSideScale(min_size=256, max_size=512),\n",
    "        CenterCropVideo(256),\n",
    "        RandomHorizontalFlip(p=0.5),\n",
    "    ]),\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VideoModel(LightningModule):\n",
    "    def __init__(self):\n",
    "        super(VideoModel, self).__init__()\n",
    "        \n",
    "        self.video_model = torch.hub.load('facebookresearch/pytorchvideo', 'efficient_x3d_xs', pretrained=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(400, 1)\n",
    "\n",
    "        self.lr = 1e-3\n",
    "        self.batch_size = 8\n",
    "        self.num_worker = 4\n",
    "        self.num_steps_train = 0\n",
    "        self.num_steps_val = 0\n",
    "\n",
    "        # self.metric = torchmetrics.classification.MultilabelAccuracy(num_labels=num_classes)\n",
    "        self.metric = torchmetrics.Accuracy()\n",
    "        \n",
    "        #loss\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.video_model(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(params=self.parameters(), lr = self.lr)\n",
    "        scheduler = ReduceLROnPlateau(opt, mode=\"min\", factor=0.05, patience=2, min_lr=1e-6)\n",
    "        # scheduler = CosineAnnealingLR(opt, T_max=10, eta_min=1e-6, last_epoch=-1)\n",
    "        return {'optimizer': opt,\n",
    "                'lr_scheduler': scheduler, \n",
    "                \"monitor\": \"val_loss\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def word_level_prediction(path_to_model, frames_list):\n",
    "    \n",
    "    model = VideoModel()\n",
    "    model.load_state_dict(torch.load(path_to_model))\n",
    "    # model = VideoModel.load_from_checkpoint(\n",
    "    # checkpoint_path=\"/home/toghrul/SLR/sign-lang/checkpoints/epoch=14-step=375.ckpt\",\n",
    "    # hparams_file=\"/home/toghrul/SLR/sign-lang/lightning_logs/version_45/hparams.yaml\",\n",
    "    # map_location=None,\n",
    "    # )\n",
    "    \n",
    "    model = model.cuda()\n",
    "    \n",
    "    # print(video['label'])\n",
    "    video = torch.stack(frames_list)\n",
    "    video = video.permute(3, 0, 1, 2)\n",
    "    video_data = {\"video\": video}\n",
    "    video_data = video_transform(video_data)\n",
    "\n",
    "    inputs = video_data[\"video\"].cuda()\n",
    "    inputs = inputs.unsqueeze(0)\n",
    "    \n",
    "    preds = model(inputs).detach().cpu().numpy()\n",
    "    # preds = np.where(preds > 0, 1, 0)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../models/eff3d_bin.pt\"\n",
    "video_path = \"../data/binary-data/val/VAR/2022-05-21 15-53-16.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/toghrul/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/toghrul/SLR/sign-lang/inference_from_frame_list.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/inference_from_frame_list.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m word_level_prediction(model_path, video_path)\n",
      "\u001b[1;32m/home/toghrul/SLR/sign-lang/inference_from_frame_list.ipynb Cell 7\u001b[0m in \u001b[0;36mword_level_prediction\u001b[0;34m(path_to_model, frames_list)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/inference_from_frame_list.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_level_prediction\u001b[39m(path_to_model, frames_list):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/inference_from_frame_list.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     model \u001b[39m=\u001b[39m VideoModel()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/inference_from_frame_list.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(path_to_model))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/inference_from_frame_list.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# model = VideoModel.load_from_checkpoint(\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/inference_from_frame_list.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# checkpoint_path=\"/home/toghrul/SLR/sign-lang/checkpoints/epoch=14-step=375.ckpt\",\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/inference_from_frame_list.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# hparams_file=\"/home/toghrul/SLR/sign-lang/lightning_logs/version_45/hparams.yaml\",\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/inference_from_frame_list.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# map_location=None,\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/inference_from_frame_list.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# )\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/inference_from_frame_list.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mcuda()\n",
      "File \u001b[0;32m~/anaconda3/envs/sign-lang/lib/python3.9/site-packages/torch/serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m             opened_file\u001b[39m.\u001b[39mseek(orig_position)\n\u001b[1;32m    711\u001b[0m             \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 712\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    713\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m~/anaconda3/envs/sign-lang/lib/python3.9/site-packages/torch/serialization.py:1049\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1047\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1048\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1049\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1051\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1053\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/sign-lang/lib/python3.9/site-packages/torch/serialization.py:1019\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m loaded_storages:\n\u001b[1;32m   1018\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1019\u001b[0m     load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1021\u001b[0m \u001b[39mreturn\u001b[39;00m loaded_storages[key]\n",
      "File \u001b[0;32m~/anaconda3/envs/sign-lang/lib/python3.9/site-packages/torch/serialization.py:1001\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m    997\u001b[0m storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39m_UntypedStorage)\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39m_untyped()\n\u001b[1;32m    998\u001b[0m \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m    999\u001b[0m \u001b[39m# stop wrapping with _TypedStorage\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m loaded_storages[key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39m_TypedStorage(\n\u001b[0;32m-> 1001\u001b[0m     wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1002\u001b[0m     dtype\u001b[39m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/anaconda3/envs/sign-lang/lib/python3.9/site-packages/torch/serialization.py:175\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    174\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 175\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[1;32m    176\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/sign-lang/lib/python3.9/site-packages/torch/serialization.py:157\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_UntypedStorage(obj\u001b[39m.\u001b[39mnbytes(), device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(location))\n\u001b[1;32m    156\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39;49mcuda(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/sign-lang/lib/python3.9/site-packages/torch/_utils.py:78\u001b[0m, in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[39mreturn\u001b[39;00m new_type(indices, values, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m     77\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_UntypedStorage(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize(), device\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m))\u001b[39m.\u001b[39mcopy_(\u001b[39mself\u001b[39m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "word_level_prediction(model_path, video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "frames = []\n",
    "\n",
    "if (cap.isOpened()== False): \n",
    "  print(\"Error opening video stream or file\")\n",
    "ret = True\n",
    "# # Read until video is completed\n",
    "while(ret):\n",
    "  # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        frames.append(torch.from_numpy(frame))\n",
    " \n",
    "    # Display the resulting frame\n",
    "    # cv2.imshow('Frame',frame)\n",
    "\n",
    "    # Press Q on keyboard to  exit\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    " \n",
    "cap.release()\n",
    " \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([230, 960, 1280, 3])\n",
      "torch.Size([3, 230, 960, 1280])\n"
     ]
    }
   ],
   "source": [
    "video = torch.stack(frames)\n",
    "print(video.shape)\n",
    "video = video.permute(3, 0, 1, 2)\n",
    "print(video.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[210, 188, 185],\n",
      "         [210, 188, 185],\n",
      "         [206, 188, 185],\n",
      "         ...,\n",
      "         [174, 171, 170],\n",
      "         [174, 171, 170],\n",
      "         [174, 171, 170]],\n",
      "\n",
      "        [[210, 188, 185],\n",
      "         [210, 188, 185],\n",
      "         [206, 188, 185],\n",
      "         ...,\n",
      "         [174, 171, 170],\n",
      "         [174, 171, 170],\n",
      "         [174, 171, 170]],\n",
      "\n",
      "        [[210, 188, 185],\n",
      "         [210, 188, 185],\n",
      "         [206, 188, 185],\n",
      "         ...,\n",
      "         [174, 171, 170],\n",
      "         [174, 171, 170],\n",
      "         [174, 171, 170]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[144, 141, 142],\n",
      "         [144, 141, 142],\n",
      "         [142, 139, 140],\n",
      "         ...,\n",
      "         [ 94, 100, 109],\n",
      "         [ 95, 101, 110],\n",
      "         [ 96, 102, 111]],\n",
      "\n",
      "        [[160, 157, 158],\n",
      "         [160, 157, 158],\n",
      "         [159, 156, 157],\n",
      "         ...,\n",
      "         [ 99, 105, 114],\n",
      "         [ 98, 104, 113],\n",
      "         [ 96, 102, 111]],\n",
      "\n",
      "        [[154, 151, 152],\n",
      "         [154, 151, 152],\n",
      "         [153, 150, 151],\n",
      "         ...,\n",
      "         [106, 112, 121],\n",
      "         [106, 112, 121],\n",
      "         [106, 112, 121]]], dtype=torch.uint8)\n",
      "torch.Size([960, 1280, 3])\n",
      "tensor([[[210, 210, 206,  ..., 174, 174, 174],\n",
      "         [210, 210, 206,  ..., 174, 174, 174],\n",
      "         [210, 210, 206,  ..., 174, 174, 174],\n",
      "         ...,\n",
      "         [144, 144, 142,  ...,  94,  95,  96],\n",
      "         [160, 160, 159,  ...,  99,  98,  96],\n",
      "         [154, 154, 153,  ..., 106, 106, 106]],\n",
      "\n",
      "        [[188, 188, 188,  ..., 171, 171, 171],\n",
      "         [188, 188, 188,  ..., 171, 171, 171],\n",
      "         [188, 188, 188,  ..., 171, 171, 171],\n",
      "         ...,\n",
      "         [141, 141, 139,  ..., 100, 101, 102],\n",
      "         [157, 157, 156,  ..., 105, 104, 102],\n",
      "         [151, 151, 150,  ..., 112, 112, 112]],\n",
      "\n",
      "        [[185, 185, 185,  ..., 170, 170, 170],\n",
      "         [185, 185, 185,  ..., 170, 170, 170],\n",
      "         [185, 185, 185,  ..., 170, 170, 170],\n",
      "         ...,\n",
      "         [142, 142, 140,  ..., 109, 110, 111],\n",
      "         [158, 158, 157,  ..., 114, 113, 111],\n",
      "         [152, 152, 151,  ..., 121, 121, 121]]], dtype=torch.uint8)\n",
      "torch.Size([3, 960, 1280])\n"
     ]
    }
   ],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "frame = frames[0]\n",
    "pp.pprint(frame)\n",
    "print(frame.shape)\n",
    "frame_perm = frames[0].permute(2, 0, 1)\n",
    "pp.pprint(frame_perm)\n",
    "print(frame_perm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchvideo.data.encoded_video import EncodedVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"../data/cam2/100/2022-05-31 12-49-06.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = cv2.VideoCapture(video_path)\n",
    "\n",
    "# count the number of frames\n",
    "frames = data.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "fps = data.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# calculate duration of the video\n",
    "seconds = round(frames / fps)\n",
    "seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 230, 960, 1280])\n"
     ]
    }
   ],
   "source": [
    "video = EncodedVideo.from_path(video_path)\n",
    "\n",
    "video_data = video.get_clip(0, 9)\n",
    "print(video_data['video'].shape)\n",
    "# video_data = video_transform(video_data)\n",
    "\n",
    "# inputs = video_data[\"video\"].cuda()\n",
    "# inputs = inputs.unsqueeze(0)\n",
    "# inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_duration = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 50, 960, 1280])\n",
      "torch.Size([3, 50, 960, 1280])\n",
      "torch.Size([3, 50, 960, 1280])\n",
      "torch.Size([3, 50, 960, 1280])\n",
      "torch.Size([3, 30, 960, 1280])\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, video_data['video'].shape[1], clip_duration):\n",
    "    \n",
    "    video = video_data['video'][:, i: i+clip_duration, :, :]\n",
    "    print(video.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/toghrul/.cache/torch/hub/facebookresearch_pytorchvideo_main\n",
      "Using cache found in /home/toghrul/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-5.427988]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_data = {\"video\": video}\n",
    "\n",
    "model = VideoModel()\n",
    "# model.load_state_dict(torch.load(path_to_model))\n",
    "\n",
    "model = VideoModel.load_from_checkpoint(\n",
    "checkpoint_path=\"/home/toghrul/SLR/sign-lang/checkpoints/epoch=14-step=375.ckpt\",\n",
    "hparams_file=\"/home/toghrul/SLR/sign-lang/lightning_logs/version_45/hparams.yaml\",\n",
    "map_location=None,\n",
    ")\n",
    "\n",
    "model = model.cuda()\n",
    "\n",
    "# video = EncodedVideo.from_path(path_to_video)\n",
    "# print(video['label'])\n",
    "# video_data = video.get_clip(0, 1)\n",
    "video_data = video_transform(video_data)\n",
    "\n",
    "inputs = video_data[\"video\"].cuda()\n",
    "inputs = inputs.unsqueeze(0)\n",
    "\n",
    "preds = model(inputs).detach().cpu().numpy()\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 25, 256, 256])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_data['video'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sign-lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96a4e4d8fc4dcb6ce321df308d690f3398dc6d289b3efb6c91f90112c618c739"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
