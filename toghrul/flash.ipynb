{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "import os\n",
    "import cv2\n",
    "import pdb\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torch.nn.functional import InterpolationMode\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import logging\n",
    "import datetime\n",
    "import sys\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flash\n",
    "from flash.core.data.utils import download_data\n",
    "from flash.video import VideoClassificationData, VideoClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The format for the transform isn't correct. Found <function VideoClassificationInputTransform at 0x7fe9f440da60>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/toghrul/SLR/sign-lang/flash.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/flash.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m datamodule \u001b[39m=\u001b[39m VideoClassificationData\u001b[39m.\u001b[39;49mfrom_folders(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/flash.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     train_folder\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m../data/train\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/flash.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     val_folder\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m../data/val\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/flash.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     clip_sampler\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39muniform\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/flash.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     clip_duration\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/flash.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     decode_audio\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/flash.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/flash.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/sign-lang/lib/python3.9/site-packages/flash/video/classification/data.py:362\u001b[0m, in \u001b[0;36mVideoClassificationData.from_folders\u001b[0;34m(cls, train_folder, val_folder, test_folder, predict_folder, target_formatter, clip_sampler, clip_duration, clip_sampler_kwargs, video_sampler, decode_audio, decoder, input_cls, predict_input_cls, transform, transform_kwargs, **data_module_kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m train_input \u001b[39m=\u001b[39m input_cls(\n\u001b[1;32m    354\u001b[0m     RunningStage\u001b[39m.\u001b[39mTRAINING,\n\u001b[1;32m    355\u001b[0m     train_folder,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mds_kw,\n\u001b[1;32m    359\u001b[0m )\n\u001b[1;32m    360\u001b[0m target_formatter \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(train_input, \u001b[39m\"\u001b[39m\u001b[39mtarget_formatter\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 362\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\n\u001b[1;32m    363\u001b[0m     train_input,\n\u001b[1;32m    364\u001b[0m     input_cls(\n\u001b[1;32m    365\u001b[0m         RunningStage\u001b[39m.\u001b[39;49mVALIDATING,\n\u001b[1;32m    366\u001b[0m         val_folder,\n\u001b[1;32m    367\u001b[0m         video_sampler\u001b[39m=\u001b[39;49mvideo_sampler,\n\u001b[1;32m    368\u001b[0m         target_formatter\u001b[39m=\u001b[39;49mtarget_formatter,\n\u001b[1;32m    369\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mds_kw,\n\u001b[1;32m    370\u001b[0m     ),\n\u001b[1;32m    371\u001b[0m     input_cls(\n\u001b[1;32m    372\u001b[0m         RunningStage\u001b[39m.\u001b[39;49mTESTING,\n\u001b[1;32m    373\u001b[0m         test_folder,\n\u001b[1;32m    374\u001b[0m         video_sampler\u001b[39m=\u001b[39;49mvideo_sampler,\n\u001b[1;32m    375\u001b[0m         target_formatter\u001b[39m=\u001b[39;49mtarget_formatter,\n\u001b[1;32m    376\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mds_kw,\n\u001b[1;32m    377\u001b[0m     ),\n\u001b[1;32m    378\u001b[0m     predict_input_cls(RunningStage\u001b[39m.\u001b[39;49mPREDICTING, predict_folder, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mds_kw),\n\u001b[1;32m    379\u001b[0m     transform\u001b[39m=\u001b[39;49mtransform,\n\u001b[1;32m    380\u001b[0m     transform_kwargs\u001b[39m=\u001b[39;49mtransform_kwargs,\n\u001b[1;32m    381\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdata_module_kwargs,\n\u001b[1;32m    382\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/sign-lang/lib/python3.9/site-packages/flash/core/data/data_module.py:134\u001b[0m, in \u001b[0;36mDataModule.__init__\u001b[0;34m(self, train_input, val_input, test_input, predict_input, data_fetcher, transform, transform_kwargs, val_split, batch_size, num_workers, sampler, pin_memory, persistent_workers)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mif\u001b[39;00m flash\u001b[39m.\u001b[39m_IS_TESTING \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m    132\u001b[0m     batch_size \u001b[39m=\u001b[39m \u001b[39m16\u001b[39m\n\u001b[0;32m--> 134\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_transform \u001b[39m=\u001b[39m create_or_configure_input_transform(\n\u001b[1;32m    135\u001b[0m     transform\u001b[39m=\u001b[39;49mtransform, transform_kwargs\u001b[39m=\u001b[39;49mtransform_kwargs\n\u001b[1;32m    136\u001b[0m )\n\u001b[1;32m    138\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mviz: Optional[BaseVisualization] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_input \u001b[39m=\u001b[39m train_input\n",
      "File \u001b[0;32m~/anaconda3/envs/sign-lang/lib/python3.9/site-packages/flash/core/data/io/input_transform.py:753\u001b[0m, in \u001b[0;36mcreate_or_configure_input_transform\u001b[0;34m(transform, transform_kwargs)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m transform:\n\u001b[1;32m    751\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 753\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe format for the transform isn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt correct. Found \u001b[39m\u001b[39m{\u001b[39;00mtransform\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The format for the transform isn't correct. Found <function VideoClassificationInputTransform at 0x7fe9f440da60>"
     ]
    }
   ],
   "source": [
    "datamodule = VideoClassificationData.from_folders(\n",
    "    train_folder=\"../data/train\",\n",
    "    val_folder=\"../data/val\",\n",
    "    clip_sampler=\"uniform\",\n",
    "    clip_duration=2,\n",
    "    decode_audio=False,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gloss_df_path = \"data_validation/processed_gloss.csv\"\n",
    "gloss_df = pd.read_csv(gloss_df_path)\n",
    "gloss_df.dropna(inplace=True)\n",
    "gloss_df.replace(to_replace=\"ASHAG\", value=\"AŞAĞI\", inplace=True)\n",
    "gloss_df['glossRange'] = gloss_df['glossEnd'] - gloss_df['glossStart']\n",
    "# gloss_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchviz\n",
    "from pytorchvideo.data import LabeledVideoDataset, make_clip_sampler\n",
    "from torchvision.models import squeezenet1_1, SqueezeNet1_1_Weights\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in 0.14. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages/torchvision/transforms/_transforms_video.py:25: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in 0.14. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pytorchvideo.data import labeled_video_dataset\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    "    Permute\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomAdjustSharpness,\n",
    "    Resize\n",
    ")\n",
    "\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_transform = Compose([\n",
    "    ApplyTransformToKey(key=\"video\",\n",
    "    transform=Compose([\n",
    "        UniformTemporalSubsample(25),\n",
    "        Lambda(lambda x: x/255),\n",
    "        Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "        RandomShortSideScale(min_size=224, max_size=256),\n",
    "        CenterCropVideo(224),\n",
    "    ]),\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../data/gloss_train\"\n",
    "val_path = \"../data/gloss_val\"\n",
    "test_path = \"../data/gloss_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/toghrul/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "source": [
    "video_model = torch.hub.load('facebookresearch/pytorchvideo', 'efficient_x3d_xs', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.metrics import classification_report\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoModel():\n",
    "    def __init__(self, ):\n",
    "        super(VideoModel, self).__init__()\n",
    "\n",
    "        self.video_model = torch.hub.load('facebookresearch/pytorchvideo', 'efficient_x3d_xs', pretrained=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(400, 1)\n",
    "\n",
    "        self.lr = 1e-3\n",
    "        self.batch_size = 8\n",
    "        self.num_worker = 4\n",
    "\n",
    "        # self.metric = torchmetrics.F1Score(task=\"multiclass\", num_classes=3)\n",
    "\n",
    "        #loss\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.video_model(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(params=self.parameters(), lr = self.lr)\n",
    "        scheduler = CosineAnnealingLR(opt, T_max=10, eta_min=1e-6, last_epoch=-1)\n",
    "        return {'optimizer': opt,\n",
    "                'lr_scheduler': scheduler}\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = labeled_video_dataset(train_path, clip_sampler=make_clip_sampler('random', 2),\n",
    "                                         transform=video_transform, decode_audio=False)\n",
    "\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, num_workers=self.num_worker, pin_memory=True)\n",
    "\n",
    "        return loader\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        video, label = batch['video'], batch['label']\n",
    "        out = self(video)\n",
    "        print(\"Pred:\", out)\n",
    "        print(\"GT:\", label.float().unsqueeze(1))\n",
    "        # print(f\"Pred:\\n{out}\")\n",
    "        # print(f\"Pred shape:\\n{out.shape}\")\n",
    "        # print(f\"Label:\\n{label}\")\n",
    "        # print(f\"Label shape:\\n{label.shape}\")\n",
    "        # print(\">>> INFO: Computing Training Loss\")\n",
    "        loss = self.criterion(out, label.float().unsqueeze(1))\n",
    "        print(f\">>>{loss}\")\n",
    "        \n",
    "        # print(\">>> INFO: Training Loss Computed\")\n",
    "        # print(\">>> INFO: Computing Training Metric\")\n",
    "        # metric = self.metric(out, label.to(torch.int64))\n",
    "\n",
    "        # return {\"loss\": loss,\n",
    "        #         \"metric\": metric.detach()}\n",
    "        \n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        loss = torch.stack([x['loss'] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        # metric = torch.stack([x['metric'] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        self.log('training_loss', loss)\n",
    "        print(f\">>>Epoch end loss: {loss}\")\n",
    "        # self.log('training_metric', metric)\n",
    "        \n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataset = labeled_video_dataset(val_path, clip_sampler=make_clip_sampler('random', 2),\n",
    "                                         transform=video_transform, decode_audio=False)\n",
    "\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, num_workers=self.num_worker, pin_memory=True)\n",
    "\n",
    "        return loader\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        video, label = batch['video'], batch['label']\n",
    "        out = self(video)\n",
    "        # print(\">>> INFO: Computing Val Loss\")\n",
    "        loss = self.criterion(out, label.float().unsqueeze(1))\n",
    "        # print(\">>> INFO: Val Loss Computed\")\n",
    "        # print(\">>> INFO: Computing Val Metric\")\n",
    "        # metric = self.metric(out, label.to(torch.int64))\n",
    "\n",
    "        # return {\"loss\": loss,\n",
    "        #         \"metric\": metric.detach()}\n",
    "        \n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        loss = torch.stack([x['loss'] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        # metric = torch.stack([x['metric'] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        self.log('val_loss', loss)\n",
    "        # self.log('val_metric', metric)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        dataset = labeled_video_dataset(test_path, clip_sampler=make_clip_sampler('random', 2),\n",
    "                                         transform=video_transform, decode_audio=False)\n",
    "\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, num_workers=self.num_worker, pin_memory=True)\n",
    "\n",
    "        return loader\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        video, label = batch['video'], batch['label']\n",
    "        out = self.forward(video)\n",
    "\n",
    "        return {\"label\": label,\n",
    "                \"pred\": out.detach()}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        label=torch.cat([x['label'] for x in outputs]).cpu().numpy()\n",
    "        pred = torch.cat([x['pred'] for x in outputs]).cpu().numpy()\n",
    "\n",
    "        print(classification_report(label, pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/toghrul/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "source": [
    "model = VideoModel()\n",
    "# optimizer = optim.SGD(model.parameters(),lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "print_every = 10\n",
    "train_losses = []\n",
    "steps = 0\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    for inputs, labels in trainloader:\n",
    "        # inputs = inputs.to(device)\n",
    "        # labels = labels.to(device)\n",
    "        # print(inputs)\n",
    "        # print(labels)\n",
    "\n",
    "        # dummy_labels = F.one_hot(labels, num_classes=num_classes)\n",
    "        steps += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        visual_ft, outputs = TempConv.forward(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        accuracy = Accuracy().to(device)\n",
    "        epoch_acc += accuracy(outputs, labels)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # print(f\"Out: {outputs[0]}; Label: {labels[0]}\")\n",
    "        # print(labels)\n",
    "\n",
    "    # break\n",
    "    print(\"Epoch: {} Loss: {}; Accuracy: {}\".format(epoch, epoch_loss/len(trainloader), epoch_acc/len(trainloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('sign-lang')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96a4e4d8fc4dcb6ce321df308d690f3398dc6d289b3efb6c91f90112c618c739"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
