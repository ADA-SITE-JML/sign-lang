{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "import os\n",
    "import cv2\n",
    "import pdb\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torch.nn.functional import InterpolationMode\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import logging \n",
    "import datetime\n",
    "import sys\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gloss_df_path = \"data_validation/processed_gloss.csv\"\n",
    "gloss_df = pd.read_csv(gloss_df_path)\n",
    "gloss_df.dropna(inplace=True)\n",
    "gloss_df.replace(to_replace=\"ASHAG\", value=\"AŞAĞI\", inplace=True)\n",
    "gloss_df['glossRange'] = gloss_df['glossEnd'] - gloss_df['glossStart']\n",
    "# gloss_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(gloss_df.gloss.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchviz\n",
    "from pytorchvideo.data import LabeledVideoDataset, make_clip_sampler\n",
    "from torchvision.models import squeezenet1_1, SqueezeNet1_1_Weights\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchvideo.data import labeled_video_dataset\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    "    Permute\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomAdjustSharpness,\n",
    "    Resize\n",
    ")\n",
    "\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_transform = Compose([\n",
    "    ApplyTransformToKey(key=\"video\",\n",
    "    transform=Compose([\n",
    "        UniformTemporalSubsample(25),\n",
    "        Lambda(lambda x: x/255),\n",
    "        Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "        RandomShortSideScale(min_size=224, max_size=256),\n",
    "        CenterCropVideo(224),\n",
    "    ]),\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/home/toghrul/SLR/data/train\"\n",
    "val_path = \"/home/toghrul/SLR/data/val\"\n",
    "test_path = \"/home/toghrul/SLR/data/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningModule, seed_everything, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "from sklearn.metrics import classification_report\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss', dirpath=\"checkpoints\", \n",
    "                                    verbose=True, save_last=True, save_top_k=2)\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"epoch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/toghrul/.cache/torch/hub/facebookresearch_pytorchvideo_main\n",
      "Global seed set to 0\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "seed_everything(0)\n",
    "\n",
    "trainer = Trainer(max_epochs=15,\n",
    "                accelerator=\"gpu\", devices=-1,\n",
    "                precision=16,\n",
    "                # accumulate_grad_batches=2,\n",
    "                enable_progress_bar=True,\n",
    "                # num_sanity_val_steps=0,\n",
    "                callbacks=[lr_monitor, checkpoint_callback],\n",
    "                log_every_n_steps=5,\n",
    "                limit_train_batches=25,\n",
    "                limit_val_batches=10,\n",
    "                limit_test_batches=10,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/toghrul/SLR/sign-lang/checkpoints/last.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type               | Params\n",
      "---------------------------------------------------\n",
      "0 | video_model | EfficientX3d       | 3.8 M \n",
      "1 | relu        | ReLU               | 0     \n",
      "2 | fc          | Linear             | 93.8 K\n",
      "3 | metric      | MultilabelAccuracy | 0     \n",
      "4 | criterion   | BCEWithLogitsLoss  | 0     \n",
      "---------------------------------------------------\n",
      "3.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.9 M     Total params\n",
      "7.776     Total estimated model params size (MB)\n",
      "Restored all states from the checkpoint file at /home/toghrul/SLR/sign-lang/checkpoints/last.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fde07a2a63a4978b74a0bd5b5f3cd5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.0:\n",
      "Loss: 0.030196867883205414\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Validation step No.1:\n",
      "Loss: 0.03355490788817406\n",
      "Accuracy: 0.9951923489570618\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2029a1729d4e4888bec24042886ab89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 50it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training step No.0:\n",
      "Loss: 0.019229505211114883\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.1:\n",
      "Loss: 0.0268651582300663\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.2:\n",
      "Loss: 0.0164125245064497\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.3:\n",
      "Loss: 0.02180781215429306\n",
      "Accuracy: 0.9962607026100159\n",
      ">>> Training step No.4:\n",
      "Loss: 0.02651321515440941\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.5:\n",
      "Loss: 0.025027191266417503\n",
      "Accuracy: 0.995726466178894\n",
      ">>> Training step No.6:\n",
      "Loss: 0.02019244246184826\n",
      "Accuracy: 0.9962607026100159\n",
      ">>> Training step No.7:\n",
      "Loss: 0.02203991636633873\n",
      "Accuracy: 0.9962607026100159\n",
      ">>> Training step No.8:\n",
      "Loss: 0.02086511068046093\n",
      "Accuracy: 0.9967949390411377\n",
      ">>> Training step No.9:\n",
      "Loss: 0.017803754657506943\n",
      "Accuracy: 0.9962607622146606\n",
      ">>> Training step No.10:\n",
      "Loss: 0.02290123887360096\n",
      "Accuracy: 0.9962607026100159\n",
      ">>> Training step No.11:\n",
      "Loss: 0.026080913841724396\n",
      "Accuracy: 0.9962607622146606\n",
      ">>> Training step No.12:\n",
      "Loss: 0.02605668269097805\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Training step No.13:\n",
      "Loss: 0.023120030760765076\n",
      "Accuracy: 0.9962607622146606\n",
      ">>> Training step No.14:\n",
      "Loss: 0.02482731081545353\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.15:\n",
      "Loss: 0.026201995089650154\n",
      "Accuracy: 0.9962607026100159\n",
      ">>> Training step No.16:\n",
      "Loss: 0.02969047985970974\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.17:\n",
      "Loss: 0.023474814370274544\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.18:\n",
      "Loss: 0.020133985206484795\n",
      "Accuracy: 0.9967949390411377\n",
      ">>> Training step No.19:\n",
      "Loss: 0.024991333484649658\n",
      "Accuracy: 0.9962606430053711\n",
      ">>> Training step No.20:\n",
      "Loss: 0.027591777965426445\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.21:\n",
      "Loss: 0.025511467829346657\n",
      "Accuracy: 0.9951923489570618\n",
      ">>> Training step No.22:\n",
      "Loss: 0.024821290746331215\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Training step No.23:\n",
      "Loss: 0.024950334802269936\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Training step No.24:\n",
      "Loss: 0.029885128140449524\n",
      "Accuracy: 0.9957265257835388\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3650d16f75c3477d838cfe2733a7a4a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.2:\n",
      "Loss: 0.02907145954668522\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Validation step No.3:\n",
      "Loss: 0.029871029779314995\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Validation step No.4:\n",
      "Loss: 0.03142330422997475\n",
      "Accuracy: 0.9951923489570618\n",
      ">>> Validation step No.5:\n",
      "Loss: 0.03066307120025158\n",
      "Accuracy: 0.995192289352417\n",
      ">>> Validation step No.6:\n",
      "Loss: 0.03302751109004021\n",
      "Accuracy: 0.9946581125259399\n",
      ">>> Validation step No.7:\n",
      "Loss: 0.031101427972316742\n",
      "Accuracy: 0.9946582317352295\n",
      ">>> Validation step No.8:\n",
      "Loss: 0.032832905650138855\n",
      "Accuracy: 0.995192289352417\n",
      ">>> Validation step No.9:\n",
      "Loss: 0.03092200867831707\n",
      "Accuracy: 0.9946581125259399\n",
      ">>> Validation step No.10:\n",
      "Loss: 0.028463589027523994\n",
      "Accuracy: 0.9951923489570618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 175: 'val_loss' was not in top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.11:\n",
      "Loss: 0.03156976401805878\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Epoch end loss: 0.019999999552965164\n",
      ">>> Training step No.25:\n",
      "Loss: 0.023513279855251312\n",
      "Accuracy: 0.9962607026100159\n",
      ">>> Training step No.26:\n",
      "Loss: 0.01939724199473858\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Training step No.27:\n",
      "Loss: 0.02543318271636963\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.28:\n",
      "Loss: 0.023127995431423187\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.29:\n",
      "Loss: 0.02252042666077614\n",
      "Accuracy: 0.9962607622146606\n",
      ">>> Training step No.30:\n",
      "Loss: 0.016421256586909294\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.31:\n",
      "Loss: 0.02178790606558323\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.32:\n",
      "Loss: 0.025772666558623314\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.33:\n",
      "Loss: 0.017129290848970413\n",
      "Accuracy: 0.9962607622146606\n",
      ">>> Training step No.34:\n",
      "Loss: 0.02106698416173458\n",
      "Accuracy: 0.9967948198318481\n",
      ">>> Training step No.35:\n",
      "Loss: 0.027266182005405426\n",
      "Accuracy: 0.9951923489570618\n",
      ">>> Training step No.36:\n",
      "Loss: 0.023170704022049904\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.37:\n",
      "Loss: 0.02356480062007904\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Training step No.38:\n",
      "Loss: 0.025960704311728477\n",
      "Accuracy: 0.9962607026100159\n",
      ">>> Training step No.39:\n",
      "Loss: 0.02555994875729084\n",
      "Accuracy: 0.9962607622146606\n",
      ">>> Training step No.40:\n",
      "Loss: 0.02571791224181652\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.41:\n",
      "Loss: 0.022625049576163292\n",
      "Accuracy: 0.9962607026100159\n",
      ">>> Training step No.42:\n",
      "Loss: 0.02874290756881237\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Training step No.43:\n",
      "Loss: 0.027318747714161873\n",
      "Accuracy: 0.9951924085617065\n",
      ">>> Training step No.44:\n",
      "Loss: 0.02237098664045334\n",
      "Accuracy: 0.995726466178894\n",
      ">>> Training step No.45:\n",
      "Loss: 0.024614494293928146\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Training step No.46:\n",
      "Loss: 0.027395615354180336\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Training step No.47:\n",
      "Loss: 0.01632046326994896\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Training step No.48:\n",
      "Loss: 0.028141768649220467\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.49:\n",
      "Loss: 0.024821529164910316\n",
      "Accuracy: 0.9957265257835388\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e0b92799624616a1a4e1483fcced82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.12:\n",
      "Loss: 0.03041241504251957\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Validation step No.13:\n",
      "Loss: 0.027635902166366577\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Validation step No.14:\n",
      "Loss: 0.029976190999150276\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Validation step No.15:\n",
      "Loss: 0.03118453547358513\n",
      "Accuracy: 0.9951923489570618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:653: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "# trainer.fit(model, ckpt_path=\"/home/toghrul/SLR/sign-lang/checkpoints/last.ckpt\")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/toghrul/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "source": [
    "model = VideoModel.load_from_checkpoint(\n",
    "    checkpoint_path=\"/home/toghrul/SLR/sign-lang/checkpoints/last.ckpt\",\n",
    "    hparams_file=\"/home/toghrul/SLR/sign-lang/lightning_logs/version_38/hparams.yaml\",\n",
    "    map_location=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "102e8e8634bb4a6988b25c0ad215f39f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Label: [190 135 103  72  50  68 171 130 168  31 108  34 124  77  13 108 108  68\n",
      " 188 194 132 174  16 101  55  21  47  46  75 108 186   2  42  61  50 108\n",
      " 108  55   8  14]\n",
      "Pred: [120 166 166 120 120 166 166 166 120 120  27 120 120 166 166  27 166 166\n",
      " 166   6 120 120   6 166 166 126 166  27 120 120 158 120 120 120 120  27\n",
      " 166 166 120 120]\n"
     ]
    }
   ],
   "source": [
    "test_res = trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6936d2533d634f118324eb3f4b673cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.0:\n",
      "Loss: 0.02925325557589531\n",
      "Accuracy: 0.9951923489570618\n",
      ">>> Validation step No.1:\n",
      "Loss: 0.02883106842637062\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Validation step No.2:\n",
      "Loss: 0.033470068126916885\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Validation step No.3:\n",
      "Loss: 0.031304895877838135\n",
      "Accuracy: 0.9951923489570618\n",
      ">>> Validation step No.4:\n",
      "Loss: 0.03123767301440239\n",
      "Accuracy: 0.9957265853881836\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        val_loss           0.029999999329447746\n",
      "       val_metric                   1.0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "val_res = trainer.validate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.1373],\n",
       "         [ 1.2810],\n",
       "         [ 0.6021],\n",
       "         [-0.2140],\n",
       "         [-0.1674],\n",
       "         [-0.0197],\n",
       "         [ 0.5376],\n",
       "         [-0.6672]]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(8, 1)\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.88778575, -1.98079647, -0.34791215,  0.15634897],\n",
       "        [ 1.23029068,  1.20237985, -0.38732682, -0.30230275],\n",
       "        [-1.04855297, -1.42001794, -1.70627019,  1.9507754 ],\n",
       "        [-0.50965218, -0.4380743 , -1.25279536,  0.77749036],\n",
       "        [-1.61389785, -0.21274028, -0.89546656,  0.3869025 ],\n",
       "        [-0.51080514, -1.18063218, -0.02818223,  0.42833187],\n",
       "        [ 0.06651722,  0.3024719 , -0.63432209, -0.36274117],\n",
       "        [-0.67246045, -0.35955316, -0.81314628, -1.7262826 ]]),\n",
       " array([3, 0, 3, 3, 3, 3, 1, 1]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.randn(8, 4)\n",
    "a, a.argmax(axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('sign-lang')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Jun  1 2022, 11:38:51) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96a4e4d8fc4dcb6ce321df308d690f3398dc6d289b3efb6c91f90112c618c739"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
