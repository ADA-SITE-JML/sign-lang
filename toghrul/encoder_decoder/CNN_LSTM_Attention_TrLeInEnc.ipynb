{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaMOmJAf2Xn4"
      },
      "source": [
        "In this notebook, I took out the feature extractor (a pre-trained model) from the Data Loader and added it into the Encoder. The name of the notebook also holds this: *_TRansferLEarningInENCoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "StQdMEpvC6Vr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import (CosineAnnealingLR,\n",
        "                                      CosineAnnealingWarmRestarts,\n",
        "                                      StepLR,\n",
        "                                      ExponentialLR)\n",
        "import sklearn.utils\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgiZx1tgjOSw"
      },
      "source": [
        "In CeDAR (Center for Data Analytics Research, ADA University) option, the notebook shall connevt to the local machine (where the whole dataset is supposed to be). To connect to the CeDAR's environment run the following to start Jupyter with access:\n",
        "\n",
        "\n",
        "```\n",
        "jupyter notebook \\\n",
        ">   --NotebookApp.allow_origin='https://colab.research.google.com' \\\n",
        ">   --port=8888 \\\n",
        ">   --NotebookApp.port_retries=0\n",
        "```\n",
        "Then select \"Connect to a local runtime\" and put the link of notebook environment (from the console)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPdLm5R70BBM",
        "outputId": "80a7baa6-dc08-4efc-9522-7089d1d2cb91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on cuda:0\n"
          ]
        }
      ],
      "source": [
        "class Config:\n",
        "    debug = False\n",
        "    env = 'CeDAR' # Dev (Jamal's GoogleDrive), Prod (SLR GDrive) or CeDAR (local)\n",
        "    csv_path = ''\n",
        "    seed = 44\n",
        "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    video_processing_tool = 'OpenCV' # OpenCV, VidGear or TorchVision\n",
        "    max_frames = 25\n",
        "    max_words_in_sentence = 25\n",
        "\n",
        "\n",
        "    drive_folder = '/home/toghrul/SLR/data/dataset_jamal/' # path for the local (CeDAR)\n",
        "    if (env == 'Dev'):\n",
        "      drive_folder = 'drive/MyDrive/SLR_test'\n",
        "    elif (env == 'Prod'):\n",
        "      drive_folder = 'drive/MyDrive/SLR/Data'\n",
        "    \n",
        "    video_folder = drive_folder+'/Video'\n",
        "\n",
        "    train_csv_path = drive_folder+'/sentences.csv'\n",
        "    camera_source = 'Cam2' # Cam1 - side-top, Cam2 - front\n",
        "    BATCH_SIZE = 1 #updated before the training\n",
        "\n",
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "#    torch.manual_seed(seed)\n",
        "    # if torch.cuda.is_available():\n",
        "    #     torch.cuda.manual_seed(seed)\n",
        "\n",
        "config = Config()\n",
        "seed_everything(config.seed)\n",
        "print('Running on',config.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbEkMcdhlL26",
        "outputId": "29faa0bd-1f76-432e-bc0d-50d4ea546c2b"
      },
      "outputs": [],
      "source": [
        "if (config.env != 'CeDAR'):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7PFKq2YWcT4P"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def pip_install(package):\n",
        "  subprocess.check_call([sys.executable, '-m', 'pip', 'install',package])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4439XWPCpzN4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mediapipe in /home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages (0.8.11)\n",
            "Requirement already satisfied: opencv-contrib-python in /home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages (from mediapipe) (4.6.0.66)\n",
            "Requirement already satisfied: protobuf<4,>=3.11 in /home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages (from mediapipe) (3.20.1)\n",
            "Requirement already satisfied: matplotlib in /home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages (from mediapipe) (3.5.1)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages (from mediapipe) (21.4.0)\n",
            "Requirement already satisfied: absl-py in /home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages (from mediapipe) (1.3.0)\n",
            "Requirement already satisfied: numpy in /home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages (from mediapipe) (1.22.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages (from matplotlib->mediapipe) (21.3)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages (from matplotlib->mediapipe) (4.25.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages (from matplotlib->mediapipe) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages (from matplotlib->mediapipe) (0.11.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages (from matplotlib->mediapipe) (9.2.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages (from matplotlib->mediapipe) (1.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
            "Requirement already satisfied: opencv_transforms in /home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages (0.0.6)\n"
          ]
        }
      ],
      "source": [
        "pip_install('mediapipe')\n",
        "\n",
        "# https://github.com/jbohnslav/opencv_transforms\n",
        "pip_install('opencv_transforms')\n",
        "\n",
        "if config.video_processing_tool == 'VidGear':\n",
        "  pip_install('vidgear[core]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qr8VkJpS2Blo",
        "outputId": "37b164e0-7961-4499-9459-c2ae49dc2b29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique words ['1', '14', '1979', '2', '23', '28', '3', '43', '6', '65', 'EOS', 'SOS', 'ada', 'adam', 'adası', 'ana', 'asif', 'ata', 'avtobus', 'avtomobil', 'axtarmaq', 'axşam', 'ayaq', 'ayaqyolu', 'az', 'azərbaycan', 'ağrımaq', 'aşağı', 'bakı', 'balıq', 'bax', 'bağlı', 'baş', 'başa', 'bel', 'biz', 'bizim', 'boğaz', 'bu', 'bulvar', 'burda', 'burdan', 'dammaq', 'dayanmaq', 'demək', 'doğulmaq', 'dünən', 'düşmək', 'dəfinələr', 'dəli', 'dəniz', 'etmək', 'ev', 'eynək', 'getmək', 'görmək', 'gözləmək', 'gün', 'günorta', 'gəlmək', 'gəzmək', 'hansı', 'harda', 'heç', 'həftə', 'hər', 'idman', 'iki', 'indi', 'it', 'iş', 'işləmək', 'i̇çəri', 'keçmək', 'kim', 'kitab', 'kovid', 'kür', 'kənar', 'lazım', 'may', 'metro', 'mın', 'məhərrəmov', 'mən', 'mənim', 'mərkəz', 'məşğul', 'necə', 'olmaq', 'ora', 'orda', 'oxumaq', 'oğul', 'paytaxt', 'pis', 'pişik', 'qapı', 'qaz', 'qaçmaq', 'qeydiyyat', 'qonşu', 'qoxu', 'qız', 'qələm', 'rayon', 'saat', 'sabah', 'sahil', 'saylı', 'siz', 'son', 'sonra', 'su', 'subay', 'səhər', 'sənəd', 'sətəlcəm', 'taksi', 'uca', 'universitet', 'vaksin', 'var', 'vaxt', 'vermək', 'xoşlamaq', 'yanmaq', 'yaxın', 'yazı', 'yaş', 'yaşamaq', 'yox', 'zökəm', 'çağır', 'çox', 'çıxmaq', 'ünvan', 'üst', 'şəhər', 'şənbə', 'əvvəl']\n",
            "Word encodings {'1': 0, '14': 1, '1979': 2, '2': 3, '23': 4, '28': 5, '3': 6, '43': 7, '6': 8, '65': 9, 'EOS': 10, 'SOS': 11, 'ada': 12, 'adam': 13, 'adası': 14, 'ana': 15, 'asif': 16, 'ata': 17, 'avtobus': 18, 'avtomobil': 19, 'axtarmaq': 20, 'axşam': 21, 'ayaq': 22, 'ayaqyolu': 23, 'az': 24, 'azərbaycan': 25, 'ağrımaq': 26, 'aşağı': 27, 'bakı': 28, 'balıq': 29, 'bax': 30, 'bağlı': 31, 'baş': 32, 'başa': 33, 'bel': 34, 'biz': 35, 'bizim': 36, 'boğaz': 37, 'bu': 38, 'bulvar': 39, 'burda': 40, 'burdan': 41, 'dammaq': 42, 'dayanmaq': 43, 'demək': 44, 'doğulmaq': 45, 'dünən': 46, 'düşmək': 47, 'dəfinələr': 48, 'dəli': 49, 'dəniz': 50, 'etmək': 51, 'ev': 52, 'eynək': 53, 'getmək': 54, 'görmək': 55, 'gözləmək': 56, 'gün': 57, 'günorta': 58, 'gəlmək': 59, 'gəzmək': 60, 'hansı': 61, 'harda': 62, 'heç': 63, 'həftə': 64, 'hər': 65, 'idman': 66, 'iki': 67, 'indi': 68, 'it': 69, 'iş': 70, 'işləmək': 71, 'i̇çəri': 72, 'keçmək': 73, 'kim': 74, 'kitab': 75, 'kovid': 76, 'kür': 77, 'kənar': 78, 'lazım': 79, 'may': 80, 'metro': 81, 'mın': 82, 'məhərrəmov': 83, 'mən': 84, 'mənim': 85, 'mərkəz': 86, 'məşğul': 87, 'necə': 88, 'olmaq': 89, 'ora': 90, 'orda': 91, 'oxumaq': 92, 'oğul': 93, 'paytaxt': 94, 'pis': 95, 'pişik': 96, 'qapı': 97, 'qaz': 98, 'qaçmaq': 99, 'qeydiyyat': 100, 'qonşu': 101, 'qoxu': 102, 'qız': 103, 'qələm': 104, 'rayon': 105, 'saat': 106, 'sabah': 107, 'sahil': 108, 'saylı': 109, 'siz': 110, 'son': 111, 'sonra': 112, 'su': 113, 'subay': 114, 'səhər': 115, 'sənəd': 116, 'sətəlcəm': 117, 'taksi': 118, 'uca': 119, 'universitet': 120, 'vaksin': 121, 'var': 122, 'vaxt': 123, 'vermək': 124, 'xoşlamaq': 125, 'yanmaq': 126, 'yaxın': 127, 'yazı': 128, 'yaş': 129, 'yaşamaq': 130, 'yox': 131, 'zökəm': 132, 'çağır': 133, 'çox': 134, 'çıxmaq': 135, 'ünvan': 136, 'üst': 137, 'şəhər': 138, 'şənbə': 139, 'əvvəl': 140}\n",
            "Words by index {0: '1', 1: '14', 2: '1979', 3: '2', 4: '23', 5: '28', 6: '3', 7: '43', 8: '6', 9: '65', 10: 'EOS', 11: 'SOS', 12: 'ada', 13: 'adam', 14: 'adası', 15: 'ana', 16: 'asif', 17: 'ata', 18: 'avtobus', 19: 'avtomobil', 20: 'axtarmaq', 21: 'axşam', 22: 'ayaq', 23: 'ayaqyolu', 24: 'az', 25: 'azərbaycan', 26: 'ağrımaq', 27: 'aşağı', 28: 'bakı', 29: 'balıq', 30: 'bax', 31: 'bağlı', 32: 'baş', 33: 'başa', 34: 'bel', 35: 'biz', 36: 'bizim', 37: 'boğaz', 38: 'bu', 39: 'bulvar', 40: 'burda', 41: 'burdan', 42: 'dammaq', 43: 'dayanmaq', 44: 'demək', 45: 'doğulmaq', 46: 'dünən', 47: 'düşmək', 48: 'dəfinələr', 49: 'dəli', 50: 'dəniz', 51: 'etmək', 52: 'ev', 53: 'eynək', 54: 'getmək', 55: 'görmək', 56: 'gözləmək', 57: 'gün', 58: 'günorta', 59: 'gəlmək', 60: 'gəzmək', 61: 'hansı', 62: 'harda', 63: 'heç', 64: 'həftə', 65: 'hər', 66: 'idman', 67: 'iki', 68: 'indi', 69: 'it', 70: 'iş', 71: 'işləmək', 72: 'i̇çəri', 73: 'keçmək', 74: 'kim', 75: 'kitab', 76: 'kovid', 77: 'kür', 78: 'kənar', 79: 'lazım', 80: 'may', 81: 'metro', 82: 'mın', 83: 'məhərrəmov', 84: 'mən', 85: 'mənim', 86: 'mərkəz', 87: 'məşğul', 88: 'necə', 89: 'olmaq', 90: 'ora', 91: 'orda', 92: 'oxumaq', 93: 'oğul', 94: 'paytaxt', 95: 'pis', 96: 'pişik', 97: 'qapı', 98: 'qaz', 99: 'qaçmaq', 100: 'qeydiyyat', 101: 'qonşu', 102: 'qoxu', 103: 'qız', 104: 'qələm', 105: 'rayon', 106: 'saat', 107: 'sabah', 108: 'sahil', 109: 'saylı', 110: 'siz', 111: 'son', 112: 'sonra', 113: 'su', 114: 'subay', 115: 'səhər', 116: 'sənəd', 117: 'sətəlcəm', 118: 'taksi', 119: 'uca', 120: 'universitet', 121: 'vaksin', 122: 'var', 123: 'vaxt', 124: 'vermək', 125: 'xoşlamaq', 126: 'yanmaq', 127: 'yaxın', 128: 'yazı', 129: 'yaş', 130: 'yaşamaq', 131: 'yox', 132: 'zökəm', 133: 'çağır', 134: 'çox', 135: 'çıxmaq', 136: 'ünvan', 137: 'üst', 138: 'şəhər', 139: 'şənbə', 140: 'əvvəl'}\n",
            "[11, 84, 61, 116, 124, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[11, 84, 28, 130, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "      id                                         video_file  \\\n",
            "0      2  /home/toghrul/SLR/data/dataset_jamal//Video/Ca...   \n",
            "1      2  /home/toghrul/SLR/data/dataset_jamal//Video/Ca...   \n",
            "2      2  /home/toghrul/SLR/data/dataset_jamal//Video/Ca...   \n",
            "3      2  /home/toghrul/SLR/data/dataset_jamal//Video/Ca...   \n",
            "4      2  /home/toghrul/SLR/data/dataset_jamal//Video/Ca...   \n",
            "...   ..                                                ...   \n",
            "2002  72  /home/toghrul/SLR/data/dataset_jamal//Video/Ca...   \n",
            "2003  72  /home/toghrul/SLR/data/dataset_jamal//Video/Ca...   \n",
            "2004  72  /home/toghrul/SLR/data/dataset_jamal//Video/Ca...   \n",
            "2005  72  /home/toghrul/SLR/data/dataset_jamal//Video/Ca...   \n",
            "2006  72  /home/toghrul/SLR/data/dataset_jamal//Video/Ca...   \n",
            "\n",
            "                                               encoding  \n",
            "0     [11, 84, 28, 130, 10, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
            "1     [11, 84, 28, 130, 10, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
            "2     [11, 84, 28, 130, 10, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
            "3     [11, 84, 28, 130, 10, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
            "4     [11, 84, 28, 130, 10, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
            "...                                                 ...  \n",
            "2002  [11, 36, 52, 63, 74, 122, 10, 0, 0, 0, 0, 0, 0...  \n",
            "2003  [11, 36, 52, 63, 74, 122, 10, 0, 0, 0, 0, 0, 0...  \n",
            "2004  [11, 36, 52, 63, 74, 122, 10, 0, 0, 0, 0, 0, 0...  \n",
            "2005  [11, 36, 52, 63, 74, 122, 10, 0, 0, 0, 0, 0, 0...  \n",
            "2006  [11, 36, 52, 63, 74, 122, 10, 0, 0, 0, 0, 0, 0...  \n",
            "\n",
            "[2007 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "# read cvs file\n",
        "sentences = pd.read_csv(config.train_csv_path)\n",
        "\n",
        "# unique words\n",
        "word_set = set(['SOS','EOS'])\n",
        "sentences.iloc[:,2].str.lower().str.split().apply(word_set.update)\n",
        "sorted_word_set = sorted(word_set)\n",
        "print('Unique words',sorted_word_set)\n",
        "\n",
        "# create word encoding\n",
        "encodings = { k:v for v,k in enumerate(sorted_word_set)}\n",
        "word_idx  = { v:k for k,v in encodings.items()}\n",
        "print('Word encodings',encodings)\n",
        "print('Words by index',word_idx)\n",
        "torch.save(encodings,config.drive_folder+'/jamal/encodings.dict')\n",
        "torch.save(word_idx,config.drive_folder+'/jamal/word_idx.dict')\n",
        "\n",
        "# converts a sentence with zero padded encoding list\n",
        "def get_sentence_encoded(sentence):\n",
        "    encoded = [encodings[key] for key in ('SOS '+sentence+' EOS').split()]\n",
        "    return  encoded + list([0]) * (config.max_words_in_sentence - len(encoded))\n",
        "\n",
        "if config.debug:  \n",
        "  print(get_sentence_encoded('mən hansı sənəd vermək'))\n",
        "  print(get_sentence_encoded('mən bakı yaşamaq'))\n",
        "\n",
        "# generate (video file name, encoding list)\n",
        "# Good recommendation on not to iterate over DFs like this:\n",
        "# https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas\n",
        "# but it's not my case - I have fewer rows and one to many with videos.\n",
        "df = pd.DataFrame(columns=[\"id\", \"video_file\",\"encoding\"])\n",
        "\n",
        "for index, row in sentences.iterrows():\n",
        "    id = row[0]\n",
        "    phrase = row[2].lower()\n",
        "    encoded = get_sentence_encoded(phrase)\n",
        "    \n",
        "    # there is a grouping of videos in production.\n",
        "    pre_folder = '/1-250/' if (config.env == 'Prod' and id < 251) else '/'\n",
        "    \n",
        "    dir = config.video_folder+'/'+config.camera_source +pre_folder+str(id)\n",
        "    # iterate over video folders\n",
        "    for filename in os.listdir(dir):\n",
        "        f = os.path.join(dir, filename)\n",
        "        # checking if it is a file\n",
        "        if os.path.isfile(f):\n",
        "            entry = pd.DataFrame.from_dict({\"id\": id, \"video_file\": f, \"encoding\": [encoded]})\n",
        "            df = pd.concat([df, entry], ignore_index = True)\n",
        "\n",
        "if config.debug:\n",
        "    print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBV3Lm5d3tbk"
      },
      "source": [
        "This small piece of code is to test the outputs of the pre-trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Y-M6SmaQosmQ"
      },
      "outputs": [],
      "source": [
        "# from torchvision.models.feature_extraction import create_feature_extractor\n",
        "\n",
        "# x = torch.rand(1, 3, 224, 224).to(config.device)\n",
        "\n",
        "# return_nodes = {\n",
        "#     'features.12.cat': 'layer12'\n",
        "# }\n",
        "# model_new = create_feature_extractor(model, return_nodes=return_nodes).to(config.device)\n",
        "\n",
        "# result = model_new(x)\n",
        "# n,out_filters,out_width,out_height = result['layer12'].shape\n",
        "# print(n,out_filters,out_width,out_height)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WugbJbtaeYJb"
      },
      "outputs": [],
      "source": [
        "rows = 5\n",
        "cols = 5\n",
        "\n",
        "def visualize_frames(frames):\n",
        "  fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(15,15))\n",
        "  \n",
        "  idx = 0\n",
        "  for i in range(rows):\n",
        "      for j in range(cols):\n",
        "        axes[i, j].imshow(frames[0,idx,:,:,:].permute(1,2,0).cpu()*255.0)\n",
        "        idx += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2UWiRi5w5q8"
      },
      "source": [
        "TODO: Implement augmentation to the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in 0.14. Please use the 'torchvision.transforms.functional' module instead.\n",
            "  warnings.warn(\n",
            "/home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages/torchvision/transforms/_transforms_video.py:25: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in 0.14. Please use the 'torchvision.transforms' module instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "from pytorchvideo.transforms import (\n",
        "    ApplyTransformToKey,\n",
        "    Normalize,\n",
        "    RandomShortSideScale,\n",
        "    UniformTemporalSubsample,\n",
        "    Permute,   \n",
        ")\n",
        "\n",
        "from torchvision.transforms import (\n",
        "    Compose,\n",
        "    Lambda,\n",
        "    RandomCrop,\n",
        "    RandomAdjustSharpness,\n",
        "    Resize,\n",
        "    ColorJitter,\n",
        "    RandomHorizontalFlip\n",
        ")\n",
        "\n",
        "from torchvision.transforms._transforms_video import (\n",
        "    CenterCropVideo,\n",
        "    NormalizeVideo\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Tj7JX4ZRTSf7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "from opencv_transforms import transforms\n",
        "\n",
        "# New mediapipe modules\n",
        "# from mediapipe.tasks import python\n",
        "# from mediapipe.tasks.python import vision\n",
        "\n",
        "# Old Mediapipe code\n",
        "mpHands = mp.solutions.hands\n",
        "hands = mpHands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.2, min_tracking_confidence=0.2)\n",
        "\n",
        "# New Mediapipe code\n",
        "# base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
        "# options = vision.HandLandmarkerOptions(base_options = base_options,num_hands=2)\n",
        "# detector = vision.HandLandmarker.create_from_options(options)\n",
        "\n",
        "# keeps only informative frames\n",
        "\n",
        "def keep_frames_with_hands(video_data, crop_size: int = 224,\n",
        "                           video_transform = None):\n",
        "  \"\"\"\n",
        "\n",
        "  Args:\n",
        "      video_data (_type_): _description_\n",
        "      crop_size (int, optional): crop_size for image transform. Defaults to 224.\n",
        "      video_transfrom (torchvision.transforms.Compose, optional): transformations to be applied to the video. Defaults to None.\n",
        "\n",
        "  Returns:\n",
        "      torch.Tensor(): processed video tensor\n",
        "  \"\"\"\n",
        "  \n",
        "  video_arr = torch.zeros((0, 3, crop_size, crop_size)).to(config.device)\n",
        "  \n",
        "    # Note: keeping this old code to know how to get frame count from different libs\n",
        "    #  \n",
        "    # frame_count = 0\n",
        "    # if config.video_processing_tool == 'OpenCV':\n",
        "    #   frame_count = int(video_data.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    # elif config.video_processing_tool == 'VidGear':\n",
        "    #   frame_count = int(video_data.stream.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    # elif config.video_processing_tool == 'TorchVision':\n",
        "    #   metadata = video_data.get_metadata()\n",
        "    #   frame_count = metadata['video']['duration'][0] * metadata['video']['fps'][0]\n",
        "    #   video_data.set_current_stream(\"video\")\n",
        "    \n",
        "  # This uses torchvision's Compose since it has been imported\n",
        "  \n",
        "  # TO-DO: check if the nd.array returned by openCV has np.unit8 as dtype\n",
        "  transform = Compose([\n",
        "      transforms.Resize(size=(crop_size,crop_size)),\n",
        "      transforms.ToTensor(), # Converts to tensor from nd.array (numpy) and shoul've normalized to [0,1].\n",
        "      ])\n",
        "  \n",
        "  ret = True\n",
        "  frame = None\n",
        "  while(True):\n",
        "    if config.video_processing_tool == 'OpenCV':\n",
        "      ret, frame = video_data.read()\n",
        "    elif config.video_processing_tool == 'VidGear':\n",
        "      frame = video_data.read()\n",
        "    elif config.video_processing_tool == 'TorchVision':\n",
        "      frame = next(video_data)['data']\n",
        "\n",
        "    if (ret is False) or (frame is None):\n",
        "        break\n",
        "\n",
        "    # Old Mediapipe code\n",
        "    hand_results = hands.process(frame)\n",
        "\n",
        "    # New Mediapipe code\n",
        "    # hand_results = detector.detect(frame)\n",
        "\n",
        "    if hand_results.multi_hand_landmarks != None:\n",
        "      frame_ext = torch.unsqueeze(transform(frame), dim=0).to(config.device)\n",
        "      video_arr = torch.cat((video_arr, frame_ext/255.0),0)\n",
        "      \n",
        "    # if config.debug:\n",
        "    #   print(f\"Video compiled with shape: {video_arr.shape}\")\n",
        "      \n",
        "    # if video_transform:\n",
        "    #   video_arr = video_transform(video_arr)\n",
        "      \n",
        "    # video_arr = def_video_transform(video_arr)\n",
        "    \n",
        "  return video_arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks638UYxxIR3"
      },
      "source": [
        "TODO: It seems the data is not read randomly. It's read sequentially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_video_transforms(img_size: int = 224):\n",
        "    video_transform=Compose([\n",
        "        # Resize(size=(img_size, img_size)),\n",
        "        # UniformTemporalSubsample(25),\n",
        "        ColorJitter(hue=0.5, contrast=0.5),\n",
        "        # Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
        "        # RandomShortSideScale(min_size=256, max_size=512),\n",
        "        # CenterCropVideo(224),\n",
        "        RandomHorizontalFlip(p=0.5),\n",
        "    ])\n",
        "    \n",
        "    return video_transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "id": "dCKhH_5Zzt3D",
        "outputId": "3c74d208-b67f-4f6b-b430-0fca1243669c"
      },
      "outputs": [],
      "source": [
        "if config.video_processing_tool == 'VidGear':\n",
        "  from vidgear.gears import CamGear\n",
        "import torchvision\n",
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "\n",
        "class SLDataset(Dataset):\n",
        "\n",
        "    def __init__(self, df, video_transform):\n",
        "        # shuffle and save\n",
        "        self.df = sklearn.utils.shuffle(df)\n",
        "        self.video_transform = video_transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        print(f\"Got item at index: {idx}\")\n",
        "        video_path = df.iloc[idx,1]\n",
        "\n",
        "        encoding = torch.tensor(df.iloc[idx,2]).to(config.device)\n",
        "        enc_shape = encoding.shape[0]\n",
        "\n",
        "        reader = None\n",
        "        #--- keep frames with hands\n",
        "        if config.video_processing_tool == 'OpenCV':\n",
        "          reader = cv2.VideoCapture(video_path)\n",
        "        elif config.video_processing_tool == 'VidGear':\n",
        "          reader = CamGear(video_path).start()\n",
        "        elif config.video_processing_tool == 'TorchVision':\n",
        "          reader = torchvision.io.VideoReader(video_path, \"video\")\n",
        "\n",
        "        hands_only = keep_frames_with_hands(reader, video_transform=self.video_transform).to(config.device)\n",
        "        hands_only = video_transform(hands_only) # applies video transformations\n",
        "        \n",
        "        if config.debug:\n",
        "          print(f\"Hands only shape: {hands_only.shape}\")\n",
        "        if config.video_processing_tool == 'OpenCV':\n",
        "          reader.release()\n",
        "        elif config.video_processing_tool == 'VidGear':\n",
        "          reader.stop()\n",
        "        elif config.video_processing_tool == 'TorchVision':\n",
        "          pass\n",
        "\n",
        "        if config.debug:\n",
        "          print(idx,video_path,encoding)\n",
        "\n",
        "        n,l,w,h = hands_only.shape\n",
        "\n",
        "        # When frames are more than we need but not that much (just trim it from the start and end)\n",
        "        if (n > config.max_frames) and (n < 1.5*config.max_frames):\n",
        "          left = (n-config.max_frames)//2\n",
        "          right = n-left-1\n",
        "          hands_only = hands_only[left:right,:,:,:]\n",
        "          n = hands_only.shape[0]\n",
        "        # If we have much more frames than we need\n",
        "        elif (n > config.max_frames):\n",
        "          slice_step = max(2,(n//config.max_frames)+1)\n",
        "          hands_only = hands_only[::slice_step,:,:,:]\n",
        "          n = hands_only.shape[0]\n",
        "\n",
        "        # If we have less frames than we need\n",
        "        # This should be a separate IF: the result of the previous IFs may also deliver fewer frame count\n",
        "        if (n < config.max_frames):\n",
        "          compliment_arr = torch.zeros(config.max_frames-n,l,w,h).to(config.device)\n",
        "          hands_only = torch.cat((hands_only,compliment_arr),0)\n",
        "\n",
        "        return hands_only, torch.reshape(encoding,(enc_shape,1))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "def get_dataloader(df, phase: str, batch_size: int = 96, video_transform = None) -> DataLoader:\n",
        "    train_df, val_df = train_test_split(df, test_size=0.1, random_state=config.seed, stratify=df['id'])\n",
        "    train_df, val_df = train_df.reset_index(drop=True), val_df.reset_index(drop=True)\n",
        "    df = train_df if phase == 'train' else val_df\n",
        "    dataset = SLDataset(df, video_transform)\n",
        "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, num_workers=0, shuffle=True)\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "video_transform = get_video_transforms()\n",
        "\n",
        "# dl = get_dataloader(df,'train',1, video_transform=None)\n",
        "# dl = get_dataloader(df,'train',1, video_transform=video_transform)\n",
        "# dl_next = next(iter(dl))\n",
        "# a,b = dl_next\n",
        "\n",
        "# if config.debug:\n",
        "#   print(a.shape,b.shape)\n",
        "#   visualize_frames(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "14Gs0c9S3Smb"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.models import squeezenet1_1\n",
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        \n",
        "        model = squeezenet1_1(pretrained=True).to(config.device)\n",
        "        return_nodes = {\n",
        "            'features.12.cat': 'layer12'\n",
        "        }\n",
        "        self.pretrained_model = create_feature_extractor(model, return_nodes=return_nodes).to(config.device)\n",
        "        self.pretrained_model.eval()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.rnn = nn.LSTM(\n",
        "                input_size = self.input_size, #  multiply by 2 if bidirectional\n",
        "                hidden_size = self.hidden_size,\n",
        "                num_layers = 1,\n",
        "                dropout = 0,\n",
        "                bidirectional = False,\n",
        "                batch_first = True)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        features = self.pretrained_model(input.squeeze())['layer12'].to(device=config.device)\n",
        "        feat_shape = features.shape\n",
        "\n",
        "        feat_flat =  torch.reshape(features,(1,feat_shape[0],feat_shape[1]*feat_shape[2]*feat_shape[3])).to(device=config.device)\n",
        "\n",
        "        output, hidden = self.rnn(feat_flat, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return (torch.zeros(1, config.BATCH_SIZE, self.hidden_size, device=config.device),torch.zeros(1, config.BATCH_SIZE, self.hidden_size, device=config.device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "MuhkdQ073o2u"
      },
      "outputs": [],
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=config.max_words_in_sentence):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        if config.debug:\n",
        "          print('Attn.init() hidden_size',hidden_size)\n",
        "          print('Attn.init() output_size',output_size)\n",
        "          print('Attn.init() max_length',max_length)\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.rnn = nn.LSTM(\n",
        "                input_size = self.hidden_size, #hid_size * 2 if bidirectional else hid_size,\n",
        "                hidden_size = self.hidden_size,\n",
        "                num_layers = 1,\n",
        "                dropout = 0,\n",
        "                bidirectional = False,\n",
        "                batch_first = True)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(input.shape[0],input.shape[1],self.hidden_size)\n",
        "        embedded = self.dropout(embedded)\n",
        "        if config.debug:\n",
        "          print('Attn.forward() input',input.shape)\n",
        "          print('Attn.forward() hidden',type(hidden),len(hidden),hidden[0].shape)\n",
        "          print('Attn.forward() encoder_outputs',encoder_outputs.shape)\n",
        "          print('embedded: ',embedded.shape)\n",
        "          print('embedded: ',embedded.shape)\n",
        "\n",
        "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1).to(device=config.device)\n",
        "\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),encoder_outputs).to(device=config.device)\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1).to(device=config.device)\n",
        "        output = self.attn_combine(output).unsqueeze(0).to(device=config.device)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.rnn(output, (hidden[0].unsqueeze(0),hidden[0].unsqueeze(0)))\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1).to(device=config.device)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return (torch.zeros(1, 1, self.hidden_size, device=config.device),torch.zeros(1, 1, self.hidden_size, device=config.device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ogrv5Z_94yP_"
      },
      "outputs": [],
      "source": [
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=config.max_words_in_sentence):\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    if config.debug:\n",
        "      print('Input len',input_tensor.shape,'Target len',target_tensor.shape)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "    encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    decoder_input  = target_tensor[:,:(max_length-2),:]   # words from 1 to n-1\n",
        "    decoder_target = target_tensor[:,1:(max_length-1),:]  # words from 2 to n (the target to the input word is the next word)\n",
        "    tar_1hot = torch.nn.functional.one_hot(decoder_target, num_classes = len(encodings))\n",
        "\n",
        "    if config.debug:\n",
        "      print('Encoder hidden_0',len(encoder_hidden),'shape',encoder_hidden[0].shape)\n",
        "      print('enc_out',encoder_output.shape)\n",
        "      print('dec_in',decoder_input.shape)\n",
        "      print('dec_target',decoder_target.shape)\n",
        "\n",
        "    target_length = decoder_target.size(1)\n",
        "\n",
        "    for di in range(target_length):\n",
        "        if config.debug:\n",
        "          print('dec hidden', decoder_hidden[0].shape,decoder_hidden[1].shape)\n",
        "\n",
        "        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input[:,di,:], decoder_hidden[0], encoder_output)\n",
        "\n",
        "        if config.debug:\n",
        "          print('decoder_output',decoder_output.shape)\n",
        "\n",
        "        loss += criterion(decoder_output.squeeze(0), tar_1hot[0,di,:].squeeze(0).double())\n",
        "\n",
        "        if (decoder_target[:,di,:] == torch.tensor(encodings['EOS'], device=config.device)):\n",
        "          break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "  \n",
        "    return loss.item() / (config.BATCH_SIZE*target_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "YcGbNQyP5X8v"
      },
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "\n",
        "def trainIters(encoder, decoder, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    video_transform = get_video_transforms()\n",
        "\n",
        "    trainloader = get_dataloader(df,'train',config.BATCH_SIZE, video_transform=video_transform)\n",
        "\n",
        "    max_epochs = 1\n",
        "\n",
        "    iter = 1\n",
        "    for epoch in range(max_epochs):\n",
        "      print('Starting epoch', epoch)\n",
        "      for inputs, labels in trainloader:\n",
        "          input_tensor = inputs.to(config.device)\n",
        "          target_tensor = labels.to(config.device)\n",
        "\n",
        "          loss = train(input_tensor, target_tensor, encoder,\n",
        "                      decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "          print_loss_total += loss\n",
        "          plot_loss_total += loss\n",
        "\n",
        "          if iter % print_every == 0:\n",
        "              print_loss_avg = print_loss_total / print_every\n",
        "              print_loss_total = 0\n",
        "              print('%.4f' % (print_loss_avg))\n",
        "              torch.save(encoder,config.drive_folder+'/jamal/encoder.model')\n",
        "              torch.save(decoder,config.drive_folder+'/jamal/decoder.model')\n",
        "              gc.collect()\n",
        "              torch.cuda.empty_cache()\n",
        "\n",
        "          if iter % plot_every == 0:\n",
        "              plot_loss_avg = plot_loss_total / plot_every\n",
        "              plot_losses.append(plot_loss_avg)\n",
        "              plot_loss_total = 0\n",
        "\n",
        "          iter += 1\n",
        "                 \n",
        "    #showPlot(plot_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "0foTbyJ85eD-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=SqueezeNet1_1_Weights.IMAGENET1K_V1`. You can also use `weights=SqueezeNet1_1_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting epoch 0\n",
            "Got item at index: 629\n",
            "Got item at index: 1474\n",
            "Got item at index: 564\n",
            "Got item at index: 102\n",
            "Got item at index: 1202\n",
            "Got item at index: 761\n",
            "Got item at index: 174\n",
            "Got item at index: 148\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/toghrul/SLR/sign-lang/toghrul/encoder_decoder/CNN_LSTM_Attention_TrLeInEnc.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e33332e323230222c2275736572223a22746f676872756c227d/home/toghrul/SLR/sign-lang/toghrul/encoder_decoder/CNN_LSTM_Attention_TrLeInEnc.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m attn_decoder \u001b[39m=\u001b[39m AttnDecoderRNN(hidden_size, \u001b[39mlen\u001b[39m(encodings), dropout_p\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\u001b[39m.\u001b[39mto(config\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e33332e323230222c2275736572223a22746f676872756c227d/home/toghrul/SLR/sign-lang/toghrul/encoder_decoder/CNN_LSTM_Attention_TrLeInEnc.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m config\u001b[39m.\u001b[39mBATCH_SZIE\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e33332e323230222c2275736572223a22746f676872756c227d/home/toghrul/SLR/sign-lang/toghrul/encoder_decoder/CNN_LSTM_Attention_TrLeInEnc.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m trainIters(encoder, attn_decoder, print_every\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\n",
            "\u001b[1;32m/home/toghrul/SLR/sign-lang/toghrul/encoder_decoder/CNN_LSTM_Attention_TrLeInEnc.ipynb Cell 23\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e33332e323230222c2275736572223a22746f676872756c227d/home/toghrul/SLR/sign-lang/toghrul/encoder_decoder/CNN_LSTM_Attention_TrLeInEnc.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_epochs):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e33332e323230222c2275736572223a22746f676872756c227d/home/toghrul/SLR/sign-lang/toghrul/encoder_decoder/CNN_LSTM_Attention_TrLeInEnc.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mStarting epoch\u001b[39m\u001b[39m'\u001b[39m, epoch)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e33332e323230222c2275736572223a22746f676872756c227d/home/toghrul/SLR/sign-lang/toghrul/encoder_decoder/CNN_LSTM_Attention_TrLeInEnc.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m   \u001b[39mfor\u001b[39;00m inputs, labels \u001b[39min\u001b[39;00m trainloader:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e33332e323230222c2275736572223a22746f676872756c227d/home/toghrul/SLR/sign-lang/toghrul/encoder_decoder/CNN_LSTM_Attention_TrLeInEnc.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m       input_tensor \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(config\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e33332e323230222c2275736572223a22746f676872756c227d/home/toghrul/SLR/sign-lang/toghrul/encoder_decoder/CNN_LSTM_Attention_TrLeInEnc.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m       target_tensor \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(config\u001b[39m.\u001b[39mdevice)\n",
            "File \u001b[0;32m~/anaconda3/envs/sign-lang/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/anaconda3/envs/sign-lang/lib/python3.9/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m~/anaconda3/envs/sign-lang/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/anaconda3/envs/sign-lang/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "\u001b[1;32m/home/toghrul/SLR/sign-lang/toghrul/encoder_decoder/CNN_LSTM_Attention_TrLeInEnc.ipynb Cell 23\u001b[0m in \u001b[0;36mSLDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e33332e323230222c2275736572223a22746f676872756c227d/home/toghrul/SLR/sign-lang/toghrul/encoder_decoder/CNN_LSTM_Attention_TrLeInEnc.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39melif\u001b[39;00m config\u001b[39m.\u001b[39mvideo_processing_tool \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mTorchVision\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e33332e323230222c2275736572223a22746f676872756c227d/home/toghrul/SLR/sign-lang/toghrul/encoder_decoder/CNN_LSTM_Attention_TrLeInEnc.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m   reader \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mVideoReader(video_path, \u001b[39m\"\u001b[39m\u001b[39mvideo\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e33332e323230222c2275736572223a22746f676872756c227d/home/toghrul/SLR/sign-lang/toghrul/encoder_decoder/CNN_LSTM_Attention_TrLeInEnc.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m hands_only \u001b[39m=\u001b[39m keep_frames_with_hands(reader, video_transform\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvideo_transform)\u001b[39m.\u001b[39mto(config\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e33332e323230222c2275736572223a22746f676872756c227d/home/toghrul/SLR/sign-lang/toghrul/encoder_decoder/CNN_LSTM_Attention_TrLeInEnc.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m hands_only \u001b[39m=\u001b[39m video_transform(hands_only) \u001b[39m# applies video transformations\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e33332e323230222c2275736572223a22746f676872756c227d/home/toghrul/SLR/sign-lang/toghrul/encoder_decoder/CNN_LSTM_Attention_TrLeInEnc.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mdebug:\n",
            "\u001b[1;32m/home/toghrul/SLR/sign-lang/toghrul/encoder_decoder/CNN_LSTM_Attention_TrLeInEnc.ipynb Cell 23\u001b[0m in \u001b[0;36mkeep_frames_with_hands\u001b[0;34m(video_data, crop_size, video_transform)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e33332e323230222c2275736572223a22746f676872756c227d/home/toghrul/SLR/sign-lang/toghrul/encoder_decoder/CNN_LSTM_Attention_TrLeInEnc.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e33332e323230222c2275736572223a22746f676872756c227d/home/toghrul/SLR/sign-lang/toghrul/encoder_decoder/CNN_LSTM_Attention_TrLeInEnc.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39m# Old Mediapipe code\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e33332e323230222c2275736572223a22746f676872756c227d/home/toghrul/SLR/sign-lang/toghrul/encoder_decoder/CNN_LSTM_Attention_TrLeInEnc.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m hand_results \u001b[39m=\u001b[39m hands\u001b[39m.\u001b[39;49mprocess(frame)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e33332e323230222c2275736572223a22746f676872756c227d/home/toghrul/SLR/sign-lang/toghrul/encoder_decoder/CNN_LSTM_Attention_TrLeInEnc.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39m# New Mediapipe code\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e33332e323230222c2275736572223a22746f676872756c227d/home/toghrul/SLR/sign-lang/toghrul/encoder_decoder/CNN_LSTM_Attention_TrLeInEnc.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39m# hand_results = detector.detect(frame)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223139322e3136382e33332e323230222c2275736572223a22746f676872756c227d/home/toghrul/SLR/sign-lang/toghrul/encoder_decoder/CNN_LSTM_Attention_TrLeInEnc.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39mif\u001b[39;00m hand_results\u001b[39m.\u001b[39mmulti_hand_landmarks \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/anaconda3/envs/sign-lang/lib/python3.9/site-packages/mediapipe/python/solutions/hands.py:153\u001b[0m, in \u001b[0;36mHands.process\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess\u001b[39m(\u001b[39mself\u001b[39m, image: np\u001b[39m.\u001b[39mndarray) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NamedTuple:\n\u001b[1;32m    133\u001b[0m   \u001b[39m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \n\u001b[1;32m    135\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39m         right hand) of the detected hand.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mprocess(input_data\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m'\u001b[39;49m: image})\n",
            "File \u001b[0;32m~/anaconda3/envs/sign-lang/lib/python3.9/site-packages/mediapipe/python/solution_base.py:366\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    360\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39madd_packet_to_input_stream(\n\u001b[1;32m    362\u001b[0m         stream\u001b[39m=\u001b[39mstream_name,\n\u001b[1;32m    363\u001b[0m         packet\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_packet(input_stream_type,\n\u001b[1;32m    364\u001b[0m                                  data)\u001b[39m.\u001b[39mat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_simulated_timestamp))\n\u001b[0;32m--> 366\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph\u001b[39m.\u001b[39;49mwait_until_idle()\n\u001b[1;32m    367\u001b[0m \u001b[39m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[39m# output stream names.\u001b[39;00m\n\u001b[1;32m    369\u001b[0m solution_outputs \u001b[39m=\u001b[39m collections\u001b[39m.\u001b[39mnamedtuple(\n\u001b[1;32m    370\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mSolutionOutputs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_stream_type_info\u001b[39m.\u001b[39mkeys())\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "input_size = 86528\n",
        "hidden_size = 256\n",
        "encoder = EncoderRNN(input_size, hidden_size).to(config.device)\n",
        "attn_decoder = AttnDecoderRNN(hidden_size, len(encodings), dropout_p=0.1).to(config.device)\n",
        "\n",
        "config.BATCH_SZIE=256\n",
        "trainIters(encoder, attn_decoder, print_every=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKYsPougTWpU"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, frames, max_length = config.max_words_in_sentence):\n",
        "    with torch.no_grad():\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_output, encoder_hidden = encoder(frames, encoder_hidden)\n",
        "\n",
        "        decoder_input = torch.tensor([[encodings['SOS']]], device=config.device)  # Start of sentence\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = ''\n",
        "        decoder_attentions = torch.zeros(max_length, max_length, device=config.device)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            #print('Input:',decoder_input)\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden[0], encoder_output)\n",
        "\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "\n",
        "            if topi.item() == encodings['EOS']:\n",
        "                decoded_words += '.'\n",
        "                break\n",
        "            else:\n",
        "                decoded_words += word_idx[topi.item()] + ' '\n",
        "\n",
        "            decoder_input = topi.detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfRqprkZU2AM"
      },
      "outputs": [],
      "source": [
        "#config.debug = True\n",
        "config.env = 'Prod'\n",
        "\n",
        "dl = get_dataloader(df,'test',1)\n",
        "\n",
        "encodings = torch.load(config.drive_folder+'/jamal/encodings.dict')\n",
        "word_idx = torch.load(config.drive_folder+'/jamal/word_idx.dict')\n",
        "\n",
        "encoder = torch.load(config.drive_folder+'/jamal/encoder.model',map_location=torch.device(config.device))\n",
        "decoder = torch.load(config.drive_folder+'/jamal/decoder.model',map_location=torch.device(config.device))\n",
        "\n",
        "def print_words(tensor_encoding):\n",
        "  sentence = ''\n",
        "  idx = 1\n",
        "  while tensor_encoding[0,idx] != encodings['EOS']:\n",
        "    sentence += word_idx[tensor_encoding[0,idx,0].item()] + ' '\n",
        "    idx += 1\n",
        "\n",
        "t = 1\n",
        "for a,b in iter(dl):\n",
        "  # print('Testing for: ',b)\n",
        "  output_words, attentions = evaluate(encoder, decoder, a)\n",
        "  print(output_words)\n",
        "  print(torch.max(a[0,0,:,:]))\n",
        "  visualize_frames(a)\n",
        "  if t>0:\n",
        "    break\n",
        "  t += 1\n",
        "\n",
        "#config.debug = False\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "sign-lang",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "96a4e4d8fc4dcb6ce321df308d690f3398dc6d289b3efb6c91f90112c618c739"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
