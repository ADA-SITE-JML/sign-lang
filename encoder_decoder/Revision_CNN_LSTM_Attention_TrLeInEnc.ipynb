{"cells":[{"cell_type":"markdown","metadata":{"id":"PaMOmJAf2Xn4"},"source":["In this notebook, I took out the feature extractor (a pre-trained model) from the Data Loader and added it into the Encoder. The name of the notebook also holds this: *_TRansferLEarningInENCoder"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"StQdMEpvC6Vr"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in 0.14. Please use the 'torchvision.transforms.functional' module instead.\n","  warnings.warn(\n","/home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages/torchvision/transforms/_transforms_video.py:25: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in 0.14. Please use the 'torchvision.transforms' module instead.\n","  warnings.warn(\n"]}],"source":["import os\n","import random\n","import numpy as np \n","import pandas as pd \n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim import AdamW\n","from torch.optim.lr_scheduler import (CosineAnnealingLR,\n","                                      CosineAnnealingWarmRestarts,\n","                                      StepLR,\n","                                      ExponentialLR)\n","\n","from pytorchvideo.transforms import (\n","    ApplyTransformToKey,\n","    Normalize,\n","    RandomShortSideScale,\n","    UniformTemporalSubsample,\n","    Permute,   \n",")\n","\n","from torchvision.transforms import (\n","    Compose,\n","    Lambda,\n","    RandomCrop,\n","    RandomAdjustSharpness,\n","    Resize,\n","    RandomHorizontalFlip\n",")\n","\n","from torchvision.transforms._transforms_video import (\n","    CenterCropVideo,\n","    NormalizeVideo\n",")\n","\n","\n","import sklearn.utils\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","metadata":{"id":"EgiZx1tgjOSw"},"source":["In CeDAR (Center for Data Analytics Research, ADA University) option, the notebook shall connevt to the local machine (where the whole dataset is supposed to be). To connect to the CeDAR's environment run the following to start Jupyter with access:\n","\n","\n","```\n","jupyter notebook \\\n",">   --NotebookApp.allow_origin='https://colab.research.google.com' \\\n",">   --port=8888 \\\n",">   --NotebookApp.port_retries=0\n","```\n","Then select \"Connect to a local runtime\" and put the link of notebook environment (from the console)\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":510,"status":"ok","timestamp":1673096290451,"user":{"displayName":"SLR ADA","userId":"00755505650554998833"},"user_tz":-240},"id":"VPdLm5R70BBM","outputId":"e0294aa0-17f9-46b8-86f5-bb5ac699c4ce"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on cuda:0\n"]}],"source":["class Config:\n","    debug = True\n","    env = 'CeDAR' # Dev (Jamal's GoogleDrive), Prod (SLR GDrive) or CeDAR (local)\n","    csv_path = ''\n","    seed = 44\n","    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","\n","    video_processing_tool = 'OpenCV' # OpenCV, VidGear or TorchVision\n","    max_frames = 25\n","    max_words_in_sentence = 25\n","\n","\n","    drive_folder = '/home/toghrul/SLR/data/dataset_jamal' # path for the local (CeDAR)\n","    if (env == 'Dev'):\n","      drive_folder = 'drive/MyDrive/SLR_test'\n","    elif (env == 'Prod'):\n","      drive_folder = 'drive/MyDrive/SLR/Data'\n","    \n","    video_folder = drive_folder+'/Video'\n","\n","    train_csv_path = drive_folder+'/sentences.csv'\n","    camera_source = 'Cam2' # Cam1 - side-top, Cam2 - front\n","    BATCH_SIZE = 1 #updated before the training\n","\n","def seed_everything(seed: int):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","#    torch.manual_seed(seed)\n","    # if torch.cuda.is_available():\n","    #     torch.cuda.manual_seed(seed)\n","\n","config = Config()\n","seed_everything(config.seed)\n","print('Running on',config.device)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28074,"status":"ok","timestamp":1673096323460,"user":{"displayName":"SLR ADA","userId":"00755505650554998833"},"user_tz":-240},"id":"fbEkMcdhlL26","outputId":"f8fb0ce3-a7bc-427a-9ea6-5d368fc2e7d8"},"outputs":[],"source":["if (config.env != 'CeDAR'):\n","  from google.colab import drive\n","  drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"7PFKq2YWcT4P"},"outputs":[],"source":["import sys\n","import subprocess\n","\n","def pip_install(package):\n","  subprocess.check_call([sys.executable, '-m', 'pip', 'install',package])"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"4439XWPCpzN4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: mediapipe in /home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages (0.8.11)\n","Requirement already satisfied: attrs>=19.1.0 in /home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages (from mediapipe) (21.4.0)\n","Requirement already satisfied: absl-py in /home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages (from mediapipe) (1.3.0)\n","Requirement already satisfied: opencv-contrib-python in /home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages (from mediapipe) (4.6.0.66)\n","Requirement already satisfied: protobuf<4,>=3.11 in /home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages (from mediapipe) (3.20.1)\n","Requirement already satisfied: numpy in /home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages (from mediapipe) (1.22.3)\n","Requirement already satisfied: matplotlib in /home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages (from mediapipe) (3.5.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages (from matplotlib->mediapipe) (4.25.0)\n","Requirement already satisfied: pyparsing>=2.2.1 in /home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages (from matplotlib->mediapipe) (3.0.9)\n","Requirement already satisfied: cycler>=0.10 in /home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages (from matplotlib->mediapipe) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages (from matplotlib->mediapipe) (2.8.2)\n","Requirement already satisfied: packaging>=20.0 in /home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages (from matplotlib->mediapipe) (21.3)\n","Requirement already satisfied: pillow>=6.2.0 in /home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages (from matplotlib->mediapipe) (9.2.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages (from matplotlib->mediapipe) (1.4.2)\n","Requirement already satisfied: six>=1.5 in /home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n","Collecting opencv_transforms\n","  Downloading opencv_transforms-0.0.6-py3-none-any.whl (18 kB)\n","Installing collected packages: opencv_transforms\n","Successfully installed opencv_transforms-0.0.6\n"]}],"source":["pip_install('mediapipe')\n","\n","# https://github.com/jbohnslav/opencv_transforms\n","pip_install('opencv_transforms')\n","\n","if config.video_processing_tool == 'VidGear':\n","  pip_install('vidgear[core]')"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":54243,"status":"ok","timestamp":1673096415575,"user":{"displayName":"SLR ADA","userId":"00755505650554998833"},"user_tz":-240},"id":"Qr8VkJpS2Blo","outputId":"3ef53e61-7223-4d2e-c77f-b87ee47f3716"},"outputs":[{"name":"stdout","output_type":"stream","text":["Unique words ['1', '14', '1979', '2', '23', '28', '3', '43', '6', '65', 'EOS', 'SOS', 'ada', 'adam', 'adası', 'ana', 'asif', 'ata', 'avtobus', 'avtomobil', 'axtarmaq', 'axşam', 'ayaq', 'ayaqyolu', 'az', 'azərbaycan', 'ağrımaq', 'aşağı', 'bakı', 'balıq', 'bax', 'bağlı', 'baş', 'başa', 'bel', 'biz', 'bizim', 'boğaz', 'bu', 'bulvar', 'burda', 'burdan', 'dammaq', 'dayanmaq', 'demək', 'doğulmaq', 'dünən', 'düşmək', 'dəfinələr', 'dəli', 'dəniz', 'etmək', 'ev', 'eynək', 'getmək', 'görmək', 'gözləmək', 'gün', 'günorta', 'gəlmək', 'gəzmək', 'hansı', 'harda', 'heç', 'həftə', 'hər', 'idman', 'iki', 'indi', 'it', 'iş', 'işləmək', 'i̇çəri', 'keçmək', 'kim', 'kitab', 'kovid', 'kür', 'kənar', 'lazım', 'may', 'metro', 'mın', 'məhərrəmov', 'mən', 'mənim', 'mərkəz', 'məşğul', 'necə', 'olmaq', 'ora', 'orda', 'oxumaq', 'oğul', 'paytaxt', 'pis', 'pişik', 'qapı', 'qaz', 'qaçmaq', 'qeydiyyat', 'qonşu', 'qoxu', 'qız', 'qələm', 'rayon', 'saat', 'sabah', 'sahil', 'saylı', 'siz', 'son', 'sonra', 'su', 'subay', 'səhər', 'sənəd', 'sətəlcəm', 'taksi', 'uca', 'universitet', 'vaksin', 'var', 'vaxt', 'vermək', 'xoşlamaq', 'yanmaq', 'yaxın', 'yazı', 'yaş', 'yaşamaq', 'yox', 'zökəm', 'çağır', 'çox', 'çıxmaq', 'ünvan', 'üst', 'şəhər', 'şənbə', 'əvvəl']\n","Word encodings {'1': 0, '14': 1, '1979': 2, '2': 3, '23': 4, '28': 5, '3': 6, '43': 7, '6': 8, '65': 9, 'EOS': 10, 'SOS': 11, 'ada': 12, 'adam': 13, 'adası': 14, 'ana': 15, 'asif': 16, 'ata': 17, 'avtobus': 18, 'avtomobil': 19, 'axtarmaq': 20, 'axşam': 21, 'ayaq': 22, 'ayaqyolu': 23, 'az': 24, 'azərbaycan': 25, 'ağrımaq': 26, 'aşağı': 27, 'bakı': 28, 'balıq': 29, 'bax': 30, 'bağlı': 31, 'baş': 32, 'başa': 33, 'bel': 34, 'biz': 35, 'bizim': 36, 'boğaz': 37, 'bu': 38, 'bulvar': 39, 'burda': 40, 'burdan': 41, 'dammaq': 42, 'dayanmaq': 43, 'demək': 44, 'doğulmaq': 45, 'dünən': 46, 'düşmək': 47, 'dəfinələr': 48, 'dəli': 49, 'dəniz': 50, 'etmək': 51, 'ev': 52, 'eynək': 53, 'getmək': 54, 'görmək': 55, 'gözləmək': 56, 'gün': 57, 'günorta': 58, 'gəlmək': 59, 'gəzmək': 60, 'hansı': 61, 'harda': 62, 'heç': 63, 'həftə': 64, 'hər': 65, 'idman': 66, 'iki': 67, 'indi': 68, 'it': 69, 'iş': 70, 'işləmək': 71, 'i̇çəri': 72, 'keçmək': 73, 'kim': 74, 'kitab': 75, 'kovid': 76, 'kür': 77, 'kənar': 78, 'lazım': 79, 'may': 80, 'metro': 81, 'mın': 82, 'məhərrəmov': 83, 'mən': 84, 'mənim': 85, 'mərkəz': 86, 'məşğul': 87, 'necə': 88, 'olmaq': 89, 'ora': 90, 'orda': 91, 'oxumaq': 92, 'oğul': 93, 'paytaxt': 94, 'pis': 95, 'pişik': 96, 'qapı': 97, 'qaz': 98, 'qaçmaq': 99, 'qeydiyyat': 100, 'qonşu': 101, 'qoxu': 102, 'qız': 103, 'qələm': 104, 'rayon': 105, 'saat': 106, 'sabah': 107, 'sahil': 108, 'saylı': 109, 'siz': 110, 'son': 111, 'sonra': 112, 'su': 113, 'subay': 114, 'səhər': 115, 'sənəd': 116, 'sətəlcəm': 117, 'taksi': 118, 'uca': 119, 'universitet': 120, 'vaksin': 121, 'var': 122, 'vaxt': 123, 'vermək': 124, 'xoşlamaq': 125, 'yanmaq': 126, 'yaxın': 127, 'yazı': 128, 'yaş': 129, 'yaşamaq': 130, 'yox': 131, 'zökəm': 132, 'çağır': 133, 'çox': 134, 'çıxmaq': 135, 'ünvan': 136, 'üst': 137, 'şəhər': 138, 'şənbə': 139, 'əvvəl': 140}\n","Words by index {0: '1', 1: '14', 2: '1979', 3: '2', 4: '23', 5: '28', 6: '3', 7: '43', 8: '6', 9: '65', 10: 'EOS', 11: 'SOS', 12: 'ada', 13: 'adam', 14: 'adası', 15: 'ana', 16: 'asif', 17: 'ata', 18: 'avtobus', 19: 'avtomobil', 20: 'axtarmaq', 21: 'axşam', 22: 'ayaq', 23: 'ayaqyolu', 24: 'az', 25: 'azərbaycan', 26: 'ağrımaq', 27: 'aşağı', 28: 'bakı', 29: 'balıq', 30: 'bax', 31: 'bağlı', 32: 'baş', 33: 'başa', 34: 'bel', 35: 'biz', 36: 'bizim', 37: 'boğaz', 38: 'bu', 39: 'bulvar', 40: 'burda', 41: 'burdan', 42: 'dammaq', 43: 'dayanmaq', 44: 'demək', 45: 'doğulmaq', 46: 'dünən', 47: 'düşmək', 48: 'dəfinələr', 49: 'dəli', 50: 'dəniz', 51: 'etmək', 52: 'ev', 53: 'eynək', 54: 'getmək', 55: 'görmək', 56: 'gözləmək', 57: 'gün', 58: 'günorta', 59: 'gəlmək', 60: 'gəzmək', 61: 'hansı', 62: 'harda', 63: 'heç', 64: 'həftə', 65: 'hər', 66: 'idman', 67: 'iki', 68: 'indi', 69: 'it', 70: 'iş', 71: 'işləmək', 72: 'i̇çəri', 73: 'keçmək', 74: 'kim', 75: 'kitab', 76: 'kovid', 77: 'kür', 78: 'kənar', 79: 'lazım', 80: 'may', 81: 'metro', 82: 'mın', 83: 'məhərrəmov', 84: 'mən', 85: 'mənim', 86: 'mərkəz', 87: 'məşğul', 88: 'necə', 89: 'olmaq', 90: 'ora', 91: 'orda', 92: 'oxumaq', 93: 'oğul', 94: 'paytaxt', 95: 'pis', 96: 'pişik', 97: 'qapı', 98: 'qaz', 99: 'qaçmaq', 100: 'qeydiyyat', 101: 'qonşu', 102: 'qoxu', 103: 'qız', 104: 'qələm', 105: 'rayon', 106: 'saat', 107: 'sabah', 108: 'sahil', 109: 'saylı', 110: 'siz', 111: 'son', 112: 'sonra', 113: 'su', 114: 'subay', 115: 'səhər', 116: 'sənəd', 117: 'sətəlcəm', 118: 'taksi', 119: 'uca', 120: 'universitet', 121: 'vaksin', 122: 'var', 123: 'vaxt', 124: 'vermək', 125: 'xoşlamaq', 126: 'yanmaq', 127: 'yaxın', 128: 'yazı', 129: 'yaş', 130: 'yaşamaq', 131: 'yox', 132: 'zökəm', 133: 'çağır', 134: 'çox', 135: 'çıxmaq', 136: 'ünvan', 137: 'üst', 138: 'şəhər', 139: 'şənbə', 140: 'əvvəl'}\n","[11, 84, 61, 116, 124, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[11, 84, 28, 130, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","      id                                         video_file  \\\n","0      2  /home/toghrul/SLR/data/dataset_jamal/Video/Cam...   \n","1      2  /home/toghrul/SLR/data/dataset_jamal/Video/Cam...   \n","2      2  /home/toghrul/SLR/data/dataset_jamal/Video/Cam...   \n","3      2  /home/toghrul/SLR/data/dataset_jamal/Video/Cam...   \n","4      2  /home/toghrul/SLR/data/dataset_jamal/Video/Cam...   \n","...   ..                                                ...   \n","2002  72  /home/toghrul/SLR/data/dataset_jamal/Video/Cam...   \n","2003  72  /home/toghrul/SLR/data/dataset_jamal/Video/Cam...   \n","2004  72  /home/toghrul/SLR/data/dataset_jamal/Video/Cam...   \n","2005  72  /home/toghrul/SLR/data/dataset_jamal/Video/Cam...   \n","2006  72  /home/toghrul/SLR/data/dataset_jamal/Video/Cam...   \n","\n","                                               encoding  \n","0     [11, 84, 28, 130, 10, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","1     [11, 84, 28, 130, 10, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","2     [11, 84, 28, 130, 10, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","3     [11, 84, 28, 130, 10, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","4     [11, 84, 28, 130, 10, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","...                                                 ...  \n","2002  [11, 36, 52, 63, 74, 122, 10, 0, 0, 0, 0, 0, 0...  \n","2003  [11, 36, 52, 63, 74, 122, 10, 0, 0, 0, 0, 0, 0...  \n","2004  [11, 36, 52, 63, 74, 122, 10, 0, 0, 0, 0, 0, 0...  \n","2005  [11, 36, 52, 63, 74, 122, 10, 0, 0, 0, 0, 0, 0...  \n","2006  [11, 36, 52, 63, 74, 122, 10, 0, 0, 0, 0, 0, 0...  \n","\n","[2007 rows x 3 columns]\n"]}],"source":["# read cvs file\n","sentences = pd.read_csv(config.train_csv_path)\n","\n","# unique words\n","word_set = set(['SOS','EOS'])\n","sentences.iloc[:,2].str.lower().str.split().apply(word_set.update)\n","sorted_word_set = sorted(word_set)\n","print('Unique words',sorted_word_set)\n","\n","# create word encoding\n","encodings = { k:v for v,k in enumerate(sorted_word_set)}\n","word_idx  = { v:k for k,v in encodings.items()}\n","print('Word encodings',encodings)\n","print('Words by index',word_idx)\n","torch.save(encodings,config.drive_folder+'/jamal/encodings.dict')\n","torch.save(word_idx,config.drive_folder+'/jamal/word_idx.dict')\n","\n","# converts a sentence with zero padded encoding list\n","def get_sentence_encoded(sentence):\n","    encoded = [encodings[key] for key in ('SOS '+sentence+' EOS').split()]\n","    return  encoded + list([0]) * (config.max_words_in_sentence - len(encoded))\n","\n","if config.debug:  \n","  print(get_sentence_encoded('mən hansı sənəd vermək'))\n","  print(get_sentence_encoded('mən bakı yaşamaq'))\n","\n","# generate (video file name, encoding list)\n","# Good recommendation on not to iterate over DFs like this:\n","# https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas\n","# but it's not my case - I have fewer rows and one to many with videos.\n","df = pd.DataFrame(columns=[\"id\", \"video_file\",\"encoding\"])\n","\n","for index, row in sentences.iterrows():\n","    id = row[0]\n","    phrase = row[2].lower()\n","    encoded = get_sentence_encoded(phrase)\n","    \n","    # there is a grouping of videos in production.\n","    pre_folder = '/1-250/' if (config.env == 'Prod' and id < 251) else '/'\n","    \n","    dir = config.video_folder+'/'+config.camera_source +pre_folder+str(id)\n","    # iterate over video folders\n","    for filename in os.listdir(dir):\n","        f = os.path.join(dir, filename)\n","        # checking if it is a file\n","        if os.path.isfile(f):\n","            entry = pd.DataFrame.from_dict({\"id\": id, \"video_file\": f, \"encoding\": [encoded]})\n","            df = pd.concat([df, entry], ignore_index = True)\n","\n","if config.debug:\n","    print(df)"]},{"cell_type":"markdown","metadata":{"id":"UBV3Lm5d3tbk"},"source":["This small piece of code is to test the outputs of the pre-trained model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y-M6SmaQosmQ"},"outputs":[],"source":["# from torchvision.models.feature_extraction import create_feature_extractor\n","\n","# x = torch.rand(1, 3, 224, 224).to(config.device)\n","\n","# return_nodes = {\n","#     'features.12.cat': 'layer12'\n","# }\n","# model_new = create_feature_extractor(model, return_nodes=return_nodes).to(config.device)\n","\n","# result = model_new(x)\n","# n,out_filters,out_width,out_height = result['layer12'].shape\n","# print(n,out_filters,out_width,out_height)"]},{"cell_type":"markdown","metadata":{"id":"e2UWiRi5w5q8"},"source":["TODO: Implement augmentation to the data"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["video_transform = Compose([\n","        Lambda(lambda x: x/255),\n","        Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n","        # RandomShortSideScale(min_size=256, max_size=512),\n","        CenterCropVideo(224),\n","        RandomHorizontalFlip(p=0.5),\n","])"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"Tj7JX4ZRTSf7"},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"]}],"source":["import cv2\n","import mediapipe as mp\n","from opencv_transforms import transforms\n","\n","# Do not put these two lines into the processing cycle - they are not removed!\n","# it will lead to a memory leak!\n","mpHands = mp.solutions.hands\n","hands = mpHands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n","\n","# keeps only informative frames\n","def keep_frames_with_hands(video_data):\n","    video_arr = torch.zeros((0,3,224,224)).to(config.device)\n","        \n","    frame_count = 0\n","    if config.video_processing_tool == 'OpenCV':\n","      frame_count = int(video_data.get(cv2.CAP_PROP_FRAME_COUNT))\n","    elif config.video_processing_tool == 'VidGear':\n","      frame_count = int(video_data.stream.get(cv2.CAP_PROP_FRAME_COUNT))\n","    elif config.video_processing_tool == 'TorchVision':\n","      metadata = video_data.get_metadata()\n","      frame_count = metadata['video']['duration'][0] * metadata['video']['fps'][0]\n","      video_data.set_current_stream(\"video\")\n","\n","    skip_frames = max(1,frame_count/config.max_frames) # do not analyze every frame, just every skip_frames\n","\n","    cnt = 0\n","    \n","    transform = transforms.Compose([\n","        transforms.Resize(size=(224,224)),\n","        transforms.ToTensor(),])\n","    \n","    ret = True\n","    frame = None\n","    while(True):\n","      if config.video_processing_tool == 'OpenCV':\n","        ret, frame = video_data.read()\n","      elif config.video_processing_tool == 'VidGear':\n","        frame = video_data.read()\n","      elif config.video_processing_tool == 'TorchVision':\n","        frame = next(video_data)['data']\n","\n","      if (ret is False) or (frame is None):\n","\t        break\n","\n","      if cnt > 0:\n","        cnt -= 1\n","        continue\n","      cnt = skip_frames\n","      \n","      hand_results = hands.process(frame)\n","      if hand_results.multi_hand_landmarks:\n","        \n","        frame_ext = torch.unsqueeze(transform(frame), dim=0).to(config.device)\n","        video_arr = torch.cat((video_arr,frame_ext),0)\n","      else:\n","        break\n","    return video_arr"]},{"cell_type":"markdown","metadata":{"id":"Ks638UYxxIR3"},"source":["TODO: It seems the data is not read randomly. It's read sequentially."]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7198,"status":"ok","timestamp":1673096425986,"user":{"displayName":"SLR ADA","userId":"00755505650554998833"},"user_tz":-240},"id":"dCKhH_5Zzt3D","outputId":"2610f04e-ef52-4d16-8b2f-4bf9b939b243"},"outputs":[{"name":"stdout","output_type":"stream","text":["Hands only shape: torch.Size([0, 3, 224, 224])\n","0 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-07-16 12-19-27.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","1 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-05-21 16-24-38.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","torch.Size([2, 25, 3, 224, 224]) torch.Size([2, 25, 1])\n","torch.Size([25, 3, 224, 224])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","2 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-04-22 12-11-21.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","3 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-08-01 13-51-48.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","torch.Size([2, 25, 3, 224, 224]) torch.Size([2, 25, 1])\n","torch.Size([25, 3, 224, 224])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","4 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-04-19 15-45-12.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","5 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-07-22 10-53-44.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","torch.Size([2, 25, 3, 224, 224]) torch.Size([2, 25, 1])\n","torch.Size([25, 3, 224, 224])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","6 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-05-24 16-24-21.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","7 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-07-02 14-38-04.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","torch.Size([2, 25, 3, 224, 224]) torch.Size([2, 25, 1])\n","torch.Size([25, 3, 224, 224])\n","Hands only shape: torch.Size([9, 3, 224, 224])\n","8 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-07-18 16-18-08.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","9 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-06-16 15-44-37.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","torch.Size([2, 25, 3, 224, 224]) torch.Size([2, 25, 1])\n","torch.Size([25, 3, 224, 224])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","10 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-05-25 13-20-29.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Hands only shape: torch.Size([22, 3, 224, 224])\n","11 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-07-29 18-42-56.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","torch.Size([2, 25, 3, 224, 224]) torch.Size([2, 25, 1])\n","torch.Size([25, 3, 224, 224])\n"]}],"source":["if config.video_processing_tool == 'VidGear':\n","  from vidgear.gears import CamGear\n","import torchvision\n","from torchvision.models.feature_extraction import create_feature_extractor\n","\n","class SLDataset(Dataset):\n","\n","    def __init__(self, df, video_transform=video_transform):\n","        # shuffle and save\n","        self.df = sklearn.utils.shuffle(df)\n","\n","    def __getitem__(self, idx):\n","        video_path = df.iloc[idx,1]\n","\n","        encoding = torch.tensor(df.iloc[idx,2]).to(config.device)\n","        enc_shape = encoding.shape[0]\n","\n","        reader = None\n","        #--- keep frames with hands\n","        if config.video_processing_tool == 'OpenCV':\n","          reader = cv2.VideoCapture(video_path)\n","        elif config.video_processing_tool == 'VidGear':\n","          reader = CamGear(video_path).start()\n","        elif config.video_processing_tool == 'TorchVision':\n","          reader = torchvision.io.VideoReader(video_path, \"video\")\n","\n","        hands_only = keep_frames_with_hands(reader).to(config.device)\n","        print(\"Hands only shape:\", hands_only.shape)\n","        \n","        if config.video_processing_tool == 'OpenCV':\n","          reader.release()\n","        elif config.video_processing_tool == 'VidGear':\n","          reader.stop()\n","        elif config.video_processing_tool == 'TorchVision':\n","          pass\n","\n","        if config.debug:\n","          print(idx,video_path,encoding)\n","\n","        n,l,w,h = hands_only.shape\n","        compliment_arr = torch.zeros(config.max_frames-n,l,w,h).to(config.device)\n","        hands_only = torch.cat((hands_only,compliment_arr),0)\n","\n","        return hands_only, torch.reshape(encoding,(enc_shape,1))\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","def get_dataloader(df, phase: str, batch_size: int = 96) -> DataLoader:\n","    train_df, val_df = train_test_split(df, test_size=0.1, random_state=config.seed, stratify=df['id'])\n","    train_df, val_df = train_df.reset_index(drop=True), val_df.reset_index(drop=True)\n","    df = train_df if phase == 'train' else val_df\n","    dataset = SLDataset(df)\n","    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, num_workers=0)\n","    return dataloader\n","\n","dl = get_dataloader(df,'train',2)\n","# dl_next = next(iter(dl))\n","# a,b = dl_next\n","\n","for i, (a,b) in enumerate(dl):\n","  print(a.shape,b.shape)\n","  INPUT_SHAPE = (a.shape[1:])\n","  print(INPUT_SHAPE)\n","  if i == 5:\n","    break\n","  \n","  \n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"14Gs0c9S3Smb"},"outputs":[],"source":["import torchvision\n","from torchvision.models import squeezenet1_1\n","from torchvision.models.feature_extraction import create_feature_extractor\n","\n","class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(EncoderRNN, self).__init__()\n","        \n","        model = squeezenet1_1(pretrained=True).to(config.device)\n","        return_nodes = {\n","            'features.12.cat': 'layer12'\n","        }\n","        self.pretrained_model = create_feature_extractor(model, return_nodes=return_nodes).to(config.device)\n","        self.pretrained_model.eval()\n","\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","\n","        self.rnn = nn.LSTM(\n","                input_size = self.input_size, #  multiply by 2 if bidirectional\n","                hidden_size = self.hidden_size,\n","                num_layers = 1,\n","                dropout = 0,\n","                bidirectional = False,\n","                batch_first = True)\n","\n","    def forward(self, input, hidden):\n","        features = self.pretrained_model(input.squeeze())['layer12'].to(device=config.device)\n","        feat_shape = features.shape\n","\n","        feat_flat =  torch.reshape(features,(1,feat_shape[0],feat_shape[1]*feat_shape[2]*feat_shape[3])).to(device=config.device)\n","\n","        output, hidden = self.rnn(feat_flat, hidden)\n","        return output, hidden\n","\n","    def initHidden(self):\n","        return (torch.zeros(1, config.BATCH_SIZE, self.hidden_size, device=config.device),torch.zeros(1, config.BATCH_SIZE, self.hidden_size, device=config.device))"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"MuhkdQ073o2u"},"outputs":[],"source":["class AttnDecoderRNN(nn.Module):\n","    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=config.max_words_in_sentence):\n","        super(AttnDecoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.dropout_p = dropout_p\n","        self.max_length = max_length\n","\n","        if config.debug:\n","          print('Attn.init() hidden_size',hidden_size)\n","          print('Attn.init() output_size',output_size)\n","          print('Attn.init() max_length',max_length)\n","\n","        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n","        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n","        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n","        self.dropout = nn.Dropout(self.dropout_p)\n","        self.rnn = nn.LSTM(        print(\"hands\"hands_only.shape)\n","\n","                input_size = self.hidden_size, #hid_size * 2 if bidirectional else hid_size,\n","                hidden_size = self.hidden_size,\n","                num_layers = 1,\n","                dropout = 0,\n","                bidirectional = False,\n","                batch_first = True)\n","        self.out = nn.Linear(self.hidden_size, self.output_size)\n","\n","    def forward(self, input, hidden, encoder_outputs):\n","        embedded = self.embedding(input).view(input.shape[0],input.shape[1],self.hidden_size)\n","        embedded = self.dropout(embedded)\n","        if config.debug:\n","          print('Attn.forward() input',input.shape)\n","          print('Attn.forward() hidden',type(hidden),len(hidden),hidden[0].shape)\n","          print('Attn.forward() encoder_outputs',encoder_outputs.shape)\n","          print('embedded: ',embedded.shape)\n","          print('embedded: ',embedded.shape)\n","\n","        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1).to(device=config.device)\n","\n","        attn_applied = torch.bmm(attn_weights.unsqueeze(0),encoder_outputs).to(device=config.device)\n","\n","        output = torch.cat((embedded[0], attn_applied[0]), 1).to(device=config.device)\n","        output = self.attn_combine(output).unsqueeze(0).to(device=config.device)\n","\n","        output = F.relu(output)\n","        output, hidden = self.rnn(output, (hidden[0].unsqueeze(0),hidden[0].unsqueeze(0)))\n","\n","        output = F.log_softmax(self.out(output[0]), dim=1).to(device=config.device)\n","        return output, hidden, attn_weights\n","\n","    def initHidden(self):\n","        return (torch.zeros(1, 1, self.hidden_size, device=config.device),torch.zeros(1, 1, self.hidden_size, device=config.device))"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"ogrv5Z_94yP_"},"outputs":[],"source":["def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=config.max_words_in_sentence):\n","    encoder_optimizer.zero_grad()\n","    decoder_optimizer.zero_grad()\n","\n","    if config.debug:\n","      print('Input len',input_tensor.shape,'Target len',target_tensor.shape)\n","\n","    loss = 0\n","\n","    encoder_hidden = encoder.initHidden()\n","    encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n","\n","    decoder_hidden = encoder_hidden\n","\n","    decoder_input  = target_tensor[:,:(max_length-2),:]   # words from 1 to n-1\n","    decoder_target = target_tensor[:,1:(max_length-1),:]  # words from 2 to n (the target to the input word is the next word)\n","    tar_1hot = torch.nn.functional.one_hot(decoder_target, num_classes = len(encodings))\n","\n","    if config.debug:\n","      print('Encoder hidden_0',len(encoder_hidden),'shape',encoder_hidden[0].shape)\n","      print('enc_out',encoder_output.shape)\n","      print('dec_in',decoder_input.shape)\n","      print('dec_target',decoder_target.shape)\n","\n","    target_length = decoder_target.size(1)\n","\n","    for di in range(target_length):\n","        if config.debug:\n","          print('dec hidden', decoder_hidden[0].shape,decoder_hidden[1].shape)\n","\n","        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input[:,di,:], decoder_hidden[0], encoder_output)\n","\n","        if config.debug:\n","          print('decoder_output',decoder_output.shape)\n","\n","        loss += criterion(decoder_output.squeeze(0), tar_1hot[0,di,:].squeeze(0).double())\n","\n","        if (decoder_target[:,di,:] == torch.tensor(encodings['EOS'], device=config.device)):\n","          break\n","\n","    loss.backward()\n","\n","    encoder_optimizer.step()\n","    decoder_optimizer.step()\n","  \n","    return loss.item() / (config.BATCH_SIZE*target_length)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"YcGbNQyP5X8v"},"outputs":[],"source":["from torch import optim\n","import torch.nn.functional as F\n","import gc\n","\n","def trainIters(encoder, decoder, print_every=1000, plot_every=100, learning_rate=0.01):\n","    plot_losses = []\n","    print_loss_total = 0  # Reset every print_every\n","    plot_loss_total = 0  # Reset every plot_every\n","\n","    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n","    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n","\n","    criterion = nn.CrossEntropyLoss()\n","\n","    trainloader = get_dataloader(df,'train',config.BATCH_SIZE)\n","\n","    max_epochs = 1\n","\n","    iter = 1\n","    for epoch in range(max_epochs):\n","      print('Starting epoch', epoch)\n","      for inputs, labels in trainloader:\n","          input_tensor = inputs.to(config.device)\n","          print(\"Input tensor shape:\", input_tensor.shape)\n","          target_tensor = labels.to(config.device)\n","\n","          loss = train(input_tensor, target_tensor, encoder,\n","                      decoder, encoder_optimizer, decoder_optimizer, criterion)\n","          print_loss_total += loss\n","          plot_loss_total += loss\n","\n","          if iter % print_every == 0:\n","              print_loss_avg = print_loss_total / print_every\n","              print_loss_total = 0\n","              print('%.4f' % (print_loss_avg))\n","              torch.save(encoder,config.drive_folder+'/jamal/encoder.model')\n","              torch.save(decoder,config.drive_folder+'/jamal/decoder.model')\n","              gc.collect()\n","              torch.cuda.empty_cache()\n","\n","          if iter % plot_every == 0:\n","              plot_loss_avg = plot_loss_total / plot_every\n","              plot_losses.append(plot_loss_avg)\n","              plot_loss_total = 0\n","\n","          iter += 1\n","                 \n","    #showPlot(plot_losses)"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":798,"referenced_widgets":["028ed6deaa6d48ac9d3798276c8679ab","d9c7597e4e5341609b2635d0924d8e35","49d9083162714b87aaa80760e5a9c837","8c65cb64f1494205ab78608a63c73afb","0384158f2a4840b4b1ff6ee051ba1814","4b0b707588c54b3a8cd479613d57bd82","472e5948f7094592a261f34cfffe2b2c","a32b805a8e7f42879b4cae5c10cf030d","91775115124b47d29fb630a55c12f516","82d21cb41de74493a7cc3e11add2f2a4","1dad9cfb630c4f848cd15fcb70e07087"]},"executionInfo":{"elapsed":3077135,"status":"ok","timestamp":1673003375759,"user":{"displayName":"SLR ADA","userId":"00755505650554998833"},"user_tz":-240},"id":"0foTbyJ85eD-","outputId":"ce8d0201-2ee9-449b-b459-b5737854f783"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","/home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=SqueezeNet1_1_Weights.IMAGENET1K_V1`. You can also use `weights=SqueezeNet1_1_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["Attn.init() hidden_size 256\n","Attn.init() output_size 141\n","Attn.init() max_length 25\n","Starting epoch 0\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","0 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-07-16 12-19-27.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","1 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-05-21 16-24-38.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","2 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-04-22 12-11-21.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","3 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-08-01 13-51-48.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","4 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-04-19 15-45-12.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","5 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-07-22 10-53-44.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","6 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-05-24 16-24-21.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","7 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-07-02 14-38-04.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([9, 3, 224, 224])\n","8 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-07-18 16-18-08.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","9 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-06-16 15-44-37.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","10 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-05-25 13-20-29.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([22, 3, 224, 224])\n","11 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-07-29 18-42-56.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","12 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-04-25 13-04-30.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","13 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-05-19 14-59-37.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","14 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-05-19 14-59-23.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([2, 3, 224, 224])\n","15 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-04-25 13-30-11.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","16 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-07-18 12-22-41.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","17 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-04-26 12-58-07.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","18 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-07-20 12-46-45.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","19 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-07-13 14-27-44.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","20 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-05-21 15-42-10.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([4, 3, 224, 224])\n","21 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-04-21 17-25-09.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","22 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-05-21 15-41-59.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","23 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-05-24 15-46-49.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","24 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-06-11 11-35-37.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","25 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-07-15 14-49-52.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","26 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-07-20 16-22-47.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","27 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-05-19 14-57-23.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","28 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-05-19 15-18-01.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","29 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-05-26 11-17-25.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","30 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-05-21 16-23-49.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","31 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-07-16 11-23-40.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","32 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-07-02 17-35-50.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","33 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-08-01 12-43-38.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","34 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-07-19 11-41-45.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","35 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-05-19 15-18-09.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","36 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-07-22 13-29-14.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","37 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-06-13 12-50-24.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","38 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-05-31 16-00-49.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","39 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-05-19 14-58-55.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","40 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-04-19 15-17-13.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","41 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-05-24 15-46-57.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","42 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-06-07 11-29-46.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([5, 3, 224, 224])\n","43 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/2/2022-04-23 11-56-14.mp4 tensor([ 11,  84,  28, 130,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","       device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","44 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/3/2022-05-21 15-43-04.mp4 tensor([11, 28, 25, 94, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0,  0,  0,  0,  0,  0,  0], device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([1, 3, 224, 224])\n","45 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/3/2022-04-26 12-58-32.mp4 tensor([11, 28, 25, 94, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0,  0,  0,  0,  0,  0,  0], device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([4, 3, 224, 224])\n","46 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/3/2022-04-23 11-53-45.mp4 tensor([11, 28, 25, 94, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0,  0,  0,  0,  0,  0,  0], device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","47 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/3/2022-05-21 16-26-05.mp4 tensor([11, 28, 25, 94, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0,  0,  0,  0,  0,  0,  0], device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","48 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/3/2022-05-24 15-47-27.mp4 tensor([11, 28, 25, 94, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0,  0,  0,  0,  0,  0,  0], device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","49 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/3/2022-05-21 15-43-14.mp4 tensor([11, 28, 25, 94, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0,  0,  0,  0,  0,  0,  0], device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","0.2103\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","50 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/3/2022-07-18 16-19-41.mp4 tensor([11, 28, 25, 94, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0,  0,  0,  0,  0,  0,  0], device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","51 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/3/2022-07-20 12-47-03.mp4 tensor([11, 28, 25, 94, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0,  0,  0,  0,  0,  0,  0], device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","52 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/3/2022-05-19 15-08-12.mp4 tensor([11, 28, 25, 94, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0,  0,  0,  0,  0,  0,  0], device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","Hands only shape: torch.Size([0, 3, 224, 224])\n","53 /home/toghrul/SLR/data/dataset_jamal/Video/Cam2/3/2022-04-19 15-17-29.mp4 tensor([11, 28, 25, 94, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0,  0,  0,  0,  0,  0,  0], device='cuda:0')\n","Input tensor shape: torch.Size([1, 25, 3, 224, 224])\n","Input len torch.Size([1, 25, 3, 224, 224]) Target len torch.Size([1, 25, 1])\n","Encoder hidden_0 2 shape torch.Size([1, 1, 256])\n","enc_out torch.Size([1, 25, 256])\n","dec_in torch.Size([1, 23, 1])\n","dec_target torch.Size([1, 23, 1])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n","dec hidden torch.Size([1, 1, 256]) torch.Size([1, 1, 256])\n","Attn.forward() input torch.Size([1, 1])\n","Attn.forward() hidden <class 'torch.Tensor'> 1 torch.Size([1, 256])\n","Attn.forward() encoder_outputs torch.Size([1, 25, 256])\n","embedded:  torch.Size([1, 1, 256])\n","embedded:  torch.Size([1, 1, 256])\n","decoder_output torch.Size([1, 141])\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/home/toghrul/SLR/sign-lang/encoder_decoder/Revision_CNN_LSTM_Attention_TrLeInEnc.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/encoder_decoder/Revision_CNN_LSTM_Attention_TrLeInEnc.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m attn_decoder \u001b[39m=\u001b[39m AttnDecoderRNN(hidden_size, \u001b[39mlen\u001b[39m(encodings), dropout_p\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\u001b[39m.\u001b[39mto(config\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/encoder_decoder/Revision_CNN_LSTM_Attention_TrLeInEnc.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m config\u001b[39m.\u001b[39mBATCH_SZIE\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/encoder_decoder/Revision_CNN_LSTM_Attention_TrLeInEnc.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m trainIters(encoder, attn_decoder, print_every\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\n","\u001b[1;32m/home/toghrul/SLR/sign-lang/encoder_decoder/Revision_CNN_LSTM_Attention_TrLeInEnc.ipynb Cell 20\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/encoder_decoder/Revision_CNN_LSTM_Attention_TrLeInEnc.ipynb#X24sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_epochs):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/encoder_decoder/Revision_CNN_LSTM_Attention_TrLeInEnc.ipynb#X24sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mStarting epoch\u001b[39m\u001b[39m'\u001b[39m, epoch)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/encoder_decoder/Revision_CNN_LSTM_Attention_TrLeInEnc.ipynb#X24sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m   \u001b[39mfor\u001b[39;00m inputs, labels \u001b[39min\u001b[39;00m trainloader:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/encoder_decoder/Revision_CNN_LSTM_Attention_TrLeInEnc.ipynb#X24sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m       input_tensor \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(config\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/encoder_decoder/Revision_CNN_LSTM_Attention_TrLeInEnc.ipynb#X24sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m       \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInput tensor shape:\u001b[39m\u001b[39m\"\u001b[39m, input_tensor\u001b[39m.\u001b[39mshape)\n","File \u001b[0;32m~/anaconda3/envs/sign-lang/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m~/anaconda3/envs/sign-lang/lib/python3.9/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m~/anaconda3/envs/sign-lang/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m~/anaconda3/envs/sign-lang/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n","\u001b[1;32m/home/toghrul/SLR/sign-lang/encoder_decoder/Revision_CNN_LSTM_Attention_TrLeInEnc.ipynb Cell 20\u001b[0m in \u001b[0;36mSLDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/encoder_decoder/Revision_CNN_LSTM_Attention_TrLeInEnc.ipynb#X24sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39melif\u001b[39;00m config\u001b[39m.\u001b[39mvideo_processing_tool \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mTorchVision\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/encoder_decoder/Revision_CNN_LSTM_Attention_TrLeInEnc.ipynb#X24sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m   reader \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mVideoReader(video_path, \u001b[39m\"\u001b[39m\u001b[39mvideo\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/encoder_decoder/Revision_CNN_LSTM_Attention_TrLeInEnc.ipynb#X24sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m hands_only \u001b[39m=\u001b[39m keep_frames_with_hands(reader)\u001b[39m.\u001b[39mto(config\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/encoder_decoder/Revision_CNN_LSTM_Attention_TrLeInEnc.ipynb#X24sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mHands only shape:\u001b[39m\u001b[39m\"\u001b[39m, hands_only\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/encoder_decoder/Revision_CNN_LSTM_Attention_TrLeInEnc.ipynb#X24sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mvideo_processing_tool \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mOpenCV\u001b[39m\u001b[39m'\u001b[39m:\n","\u001b[1;32m/home/toghrul/SLR/sign-lang/encoder_decoder/Revision_CNN_LSTM_Attention_TrLeInEnc.ipynb Cell 20\u001b[0m in \u001b[0;36mkeep_frames_with_hands\u001b[0;34m(video_data)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/encoder_decoder/Revision_CNN_LSTM_Attention_TrLeInEnc.ipynb#X24sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mwhile\u001b[39;00m(\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/encoder_decoder/Revision_CNN_LSTM_Attention_TrLeInEnc.ipynb#X24sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m   \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mvideo_processing_tool \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mOpenCV\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/encoder_decoder/Revision_CNN_LSTM_Attention_TrLeInEnc.ipynb#X24sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     ret, frame \u001b[39m=\u001b[39m video_data\u001b[39m.\u001b[39;49mread()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/encoder_decoder/Revision_CNN_LSTM_Attention_TrLeInEnc.ipynb#X24sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m   \u001b[39melif\u001b[39;00m config\u001b[39m.\u001b[39mvideo_processing_tool \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mVidGear\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/encoder_decoder/Revision_CNN_LSTM_Attention_TrLeInEnc.ipynb#X24sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     frame \u001b[39m=\u001b[39m video_data\u001b[39m.\u001b[39mread()\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["input_size = 86528\n","hidden_size = 256\n","encoder = EncoderRNN(input_size, hidden_size).to(config.device)\n","attn_decoder = AttnDecoderRNN(hidden_size, len(encodings), dropout_p=0.1).to(config.device)\n","\n","config.BATCH_SZIE=256\n","trainIters(encoder, attn_decoder, print_every=50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fKYsPougTWpU"},"outputs":[],"source":["def evaluate(encoder, decoder, frames, max_length = config.max_words_in_sentence):\n","    with torch.no_grad():\n","        encoder_hidden = encoder.initHidden()\n","\n","        encoder_output, encoder_hidden = encoder(frames, encoder_hidden)\n","\n","        decoder_input = torch.tensor([[encodings['SOS']]], device=config.device)  # Start of sentence\n","\n","        decoder_hidden = encoder_hidden\n","\n","        decoded_words = ''\n","        decoder_attentions = torch.zeros(max_length, max_length, device=config.device)\n","\n","        for di in range(max_length):\n","            #print('Input:',decoder_input)\n","            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden[0], encoder_output)\n","\n","            decoder_attentions[di] = decoder_attention.data\n","\n","            topv, topi = decoder_output.data.topk(1)\n","\n","            if topi.item() == encodings['EOS']:\n","                decoded_words += '.'\n","                break\n","            else:\n","                decoded_words += word_idx[topi.item()] + ' '\n","\n","            decoder_input = topi.detach()\n","\n","        return decoded_words, decoder_attentions[:di + 1]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1552,"status":"error","timestamp":1673003377307,"user":{"displayName":"SLR ADA","userId":"00755505650554998833"},"user_tz":-240},"id":"KfRqprkZU2AM","outputId":"f63a4e91-d467-41c6-86c0-bf2a083330bd"},"outputs":[{"ename":"IndexError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-3fba3f46eca1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Testing for: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprint_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m   \u001b[0moutput_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattentions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-3fba3f46eca1>\u001b[0m in \u001b[0;36mprint_words\u001b[0;34m(tensor_encoding)\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0mtensor_encoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mencodings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EOS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0msentence\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mword_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor_encoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 2 with size 1"]}],"source":["#config.debug = True\n","config.env = 'Prod'\n","\n","dl = get_dataloader(df,'test',1)\n","\n","encodings = torch.load(config.drive_folder+'/jamal/encodings.dict')\n","word_idx = torch.load(config.drive_folder+'/jamal/word_idx.dict')\n","\n","encoder = torch.load(config.drive_folder+'/jamal/encoder.model',map_location=torch.device(config.device))\n","decoder = torch.load(config.drive_folder+'/jamal/decoder.model',map_location=torch.device(config.device))\n","\n","def print_words(tensor_encoding):\n","  sentence = ''\n","  idx = 1\n","  while tensor_encoding[0,idx] != encodings['EOS']:\n","    sentence += word_idx[tensor_encoding[0,0,idx]] + ' '\n","    idx += 1\n","\n","t = 1\n","for a,b in iter(dl):\n","  print('Testing for: ',print_words(b))\n","  output_words, attentions = evaluate(encoder, decoder, a)\n","  print(output_words)\n","  if t>1:\n","    break\n","  t += 1\n","\n","#config.debug = False\n"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"sign-lang","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"96a4e4d8fc4dcb6ce321df308d690f3398dc6d289b3efb6c91f90112c618c739"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"028ed6deaa6d48ac9d3798276c8679ab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d9c7597e4e5341609b2635d0924d8e35","IPY_MODEL_49d9083162714b87aaa80760e5a9c837","IPY_MODEL_8c65cb64f1494205ab78608a63c73afb"],"layout":"IPY_MODEL_0384158f2a4840b4b1ff6ee051ba1814"}},"0384158f2a4840b4b1ff6ee051ba1814":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1dad9cfb630c4f848cd15fcb70e07087":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"472e5948f7094592a261f34cfffe2b2c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"49d9083162714b87aaa80760e5a9c837":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a32b805a8e7f42879b4cae5c10cf030d","max":4958839,"min":0,"orientation":"horizontal","style":"IPY_MODEL_91775115124b47d29fb630a55c12f516","value":4958839}},"4b0b707588c54b3a8cd479613d57bd82":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82d21cb41de74493a7cc3e11add2f2a4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c65cb64f1494205ab78608a63c73afb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_82d21cb41de74493a7cc3e11add2f2a4","placeholder":"​","style":"IPY_MODEL_1dad9cfb630c4f848cd15fcb70e07087","value":" 4.73M/4.73M [00:01&lt;00:00, 3.09MB/s]"}},"91775115124b47d29fb630a55c12f516":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a32b805a8e7f42879b4cae5c10cf030d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9c7597e4e5341609b2635d0924d8e35":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b0b707588c54b3a8cd479613d57bd82","placeholder":"​","style":"IPY_MODEL_472e5948f7094592a261f34cfffe2b2c","value":"100%"}}}}},"nbformat":4,"nbformat_minor":0}
