{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Source: https://www.kaggle.com/code/polomarco/ecg-classification-cnn-lstm-attention-mechanism"
      ],
      "metadata": {
        "id": "3SzSjVQ3waV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StQdMEpvC6Vr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "#import itertools\n",
        "#import time\n",
        "import random\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "#import matplotlib.pyplot as plt\n",
        "#import matplotlib.colors as mcolors\n",
        "#import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import (CosineAnnealingLR,\n",
        "                                      CosineAnnealingWarmRestarts,\n",
        "                                      StepLR,\n",
        "                                      ExponentialLR)\n",
        "import sklearn.utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from sklearn.metrics import precision_recall_curve\n",
        "#from sklearn.metrics import classification_report, confusion_matrix\n",
        "#from sklearn.metrics import accuracy_score, auc, f1_score, precision_score, recall_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    csv_path = ''\n",
        "    seed = 69\n",
        "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    max_words_in_sentence = 10\n",
        "    video_folder = 'video'\n",
        "\n",
        "    # attn_state_path = 'attn.pth'\n",
        "    # attn_logs = 'attn.csv'\n",
        "    \n",
        "    train_csv_path = 'sentences.csv'\n",
        "    # test_csv_path = '.csv'\n",
        "\n",
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "\n",
        "config = Config()\n",
        "seed_everything(config.seed)"
      ],
      "metadata": {
        "id": "VPdLm5R70BBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read cvs file\n",
        "sentences = pd.read_csv('sentences.csv')\n",
        "\n",
        "# unique words\n",
        "word_set = set()\n",
        "sentences.iloc[:,2].str.lower().str.split().apply(word_set.update)\n",
        "sorted_word_set = sorted(word_set)\n",
        "print('Unique words',sorted_word_set)\n",
        "\n",
        "# create word encoding\n",
        "encodings = { k:v for v,k in enumerate(sorted_word_set)}\n",
        "print('Word encodings',encodings)\n",
        "\n",
        "# converts a sentence with zero padded encoding list\n",
        "def get_sentence_encoded(sentence):\n",
        "    encoded = [encodings[key] for key in sentence.split()]\n",
        "    return  encoded + list([0]) * (config.max_words_in_sentence - len(encoded))\n",
        "\n",
        "# print(get_sentence_encoded('mən hansı sənəd vermək'))\n",
        "# print(get_sentence_encoded('mən bakı yaşamaq'))\n",
        "\n",
        "# generate (video file name, encoding list)\n",
        "# Good recommendation on not to iterate over DFs like this:\n",
        "# https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas\n",
        "# but it's not my case - I have fewer rows and one to many with videos.\n",
        "df = pd.DataFrame(columns=[\"id\", \"video_file\",\"encoding\"])\n",
        "\n",
        "for index, row in sentences.head(2).iterrows():\n",
        "    id = row[0]\n",
        "    phrase = row[2].lower()\n",
        "    encoded = get_sentence_encoded(phrase)\n",
        "    # iterate over video folders\n",
        "    dir = config.video_folder+'/'+str(id)\n",
        "    for filename in os.listdir(dir):\n",
        "        f = os.path.join(dir, filename)\n",
        "        # checking if it is a file\n",
        "        if os.path.isfile(f):\n",
        "            entry = pd.DataFrame.from_dict({\"id\": id, \"video_file\": f, \"encoding\": [encoded]})\n",
        "            df = pd.concat([df, entry], ignore_index = True)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qr8VkJpS2Blo",
        "outputId": "b07b3444-b368-4740-9a0e-d23a06362dc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique words ['1', '2', 'ad', 'ana', 'ata', 'azərbaycan', 'bakı', 'bu', 'dünən', 'getmək', 'hansı', 'iş', 'mən', 'necə', 'nə', 'olmaq', 'ora', 'orda', 'oğul', 'paytaxt', 'qız', 'siz', 'subay', 'sənəd', 'var', 'vermək', 'yaşamaq', 'yox']\n",
            "Word encodings {'1': 0, '2': 1, 'ad': 2, 'ana': 3, 'ata': 4, 'azərbaycan': 5, 'bakı': 6, 'bu': 7, 'dünən': 8, 'getmək': 9, 'hansı': 10, 'iş': 11, 'mən': 12, 'necə': 13, 'nə': 14, 'olmaq': 15, 'ora': 16, 'orda': 17, 'oğul': 18, 'paytaxt': 19, 'qız': 20, 'siz': 21, 'subay': 22, 'sənəd': 23, 'var': 24, 'vermək': 25, 'yaşamaq': 26, 'yox': 27}\n",
            "  id                       video_file                          encoding\n",
            "0  1  video/1/2022-04-19 15-44-38.mp4  [21, 2, 14, 0, 0, 0, 0, 0, 0, 0]\n",
            "1  1  video/1/2022-04-22 11-15-55.mp4  [21, 2, 14, 0, 0, 0, 0, 0, 0, 0]\n",
            "2  1  video/1/2022-04-21 17-23-55.mp4  [21, 2, 14, 0, 0, 0, 0, 0, 0, 0]\n",
            "3  2  video/2/2022-04-19 15-17-13.mp4  [12, 6, 26, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SLDataset(Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "        # shuffle and save\n",
        "        self.df = sklearn.utils.shuffle(df) \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video = df.iloc[idx,1]\n",
        "        encoding = df.iloc[idx,2]\n",
        "        # keep frames with hands\n",
        "\n",
        "        # get last convolutional layer for each\n",
        "        video_features = 0\n",
        "\n",
        "        return video, encoding\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "def get_dataloader(df, phase: str, batch_size: int = 96) -> DataLoader:\n",
        "    '''\n",
        "    Dataset and DataLoader.\n",
        "    Parameters:\n",
        "        phase: training or validation phase.\n",
        "        batch_size: data per iteration.\n",
        "    Returns:\n",
        "        data generator\n",
        "    '''\n",
        "    train_df, val_df = train_test_split(df, test_size=0.15, random_state=config.seed)#, stratify=df['id'])\n",
        "    train_df, val_df = train_df.reset_index(drop=True), val_df.reset_index(drop=True)\n",
        "    df = train_df if phase == 'train' else val_df\n",
        "    dataset = SLDataset(df)\n",
        "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, num_workers=4)\n",
        "    return dataloader\n",
        "\n",
        "dl = get_dataloader(df,'train',4)\n",
        "next(iter(dl))"
      ],
      "metadata": {
        "id": "dCKhH_5Zzt3D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12290dac-3ee0-46d2-d787-82ca7746826c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('video/1/2022-04-19 15-44-38.mp4',\n",
              "  'video/1/2022-04-22 11-15-55.mp4',\n",
              "  'video/1/2022-04-21 17-23-55.mp4'),\n",
              " [tensor([21, 21, 21]), tensor([2, 2, 2]), tensor([14, 14, 14])]]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNAttentionModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        hid_size,\n",
        "        rnn_type,\n",
        "        bidirectional,\n",
        "        n_classes=5,\n",
        "        kernel_size=5,\n",
        "    ):\n",
        "        super().__init__()\n",
        " \n",
        "        self.rnn_layer = nn.LSTM(\n",
        "                input_size = 46, #hid_size * 2 if bidirectional else hid_size,\n",
        "                hidden_size = hid_size,\n",
        "                num_layers = 1,\n",
        "                dropout = 0,\n",
        "                bidirectional = bidirectional,\n",
        "                batch_first = True)\n",
        "        \n",
        "        self.conv1 = ConvNormPool(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hid_size,\n",
        "            kernel_size=kernel_size,\n",
        "        )\n",
        "        self.conv2 = ConvNormPool(\n",
        "            input_size=hid_size,\n",
        "            hidden_size=hid_size,\n",
        "            kernel_size=kernel_size,\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveMaxPool1d((1))\n",
        "        self.attn = nn.Linear(hid_size, hid_size, bias=False)\n",
        "        self.fc = nn.Linear(in_features=hid_size, out_features=n_classes)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        x = self.conv1(input)\n",
        "        x = self.conv2(x)\n",
        "        x_out, hid_states = self.rnn_layer(x)\n",
        "        x = torch.cat([hid_states[0], hid_states[1]], dim=0).transpose(0, 1)\n",
        "        x_attn = torch.tanh(self.attn(x))\n",
        "        x = x_attn.bmm(x_out)\n",
        "        x = x.transpose(2, 1)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(-1, x.size(1) * x.size(2))\n",
        "        x = F.softmax(self.fc(x), dim=-1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "lR2eq9ewwDmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Meter:\n",
        "    def __init__(self, n_classes=5):\n",
        "        self.metrics = {}\n",
        "        self.confusion = torch.zeros((n_classes, n_classes))\n",
        "    \n",
        "    def update(self, x, y, loss):\n",
        "        x = np.argmax(x.detach().cpu().numpy(), axis=1)\n",
        "        y = y.detach().cpu().numpy()\n",
        "        self.metrics['loss'] += loss\n",
        "        self.metrics['accuracy'] += accuracy_score(x,y)\n",
        "        self.metrics['f1'] += f1_score(x,y,average='macro')\n",
        "        self.metrics['precision'] += precision_score(x, y, average='macro', zero_division=1)\n",
        "        self.metrics['recall'] += recall_score(x,y, average='macro', zero_division=1)\n",
        "        \n",
        "        self._compute_cm(x, y)\n",
        "        \n",
        "    def _compute_cm(self, x, y):\n",
        "        for prob, target in zip(x, y):\n",
        "            if prob == target:\n",
        "                self.confusion[target][target] += 1\n",
        "            else:\n",
        "                self.confusion[target][prob] += 1\n",
        "    \n",
        "    def init_metrics(self):\n",
        "        self.metrics['loss'] = 0\n",
        "        self.metrics['accuracy'] = 0\n",
        "        self.metrics['f1'] = 0\n",
        "        self.metrics['precision'] = 0\n",
        "        self.metrics['recall'] = 0\n",
        "        \n",
        "    def get_metrics(self):\n",
        "        return self.metrics\n",
        "    \n",
        "    def get_confusion_matrix(self):\n",
        "        return self.confusion"
      ],
      "metadata": {
        "id": "IK9Z8zH3wSeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(self, net, lr, batch_size, num_epochs):\n",
        "        self.net = net.to(config.device)\n",
        "        self.num_epochs = num_epochs\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = AdamW(self.net.parameters(), lr=lr)\n",
        "        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=num_epochs, eta_min=5e-6)\n",
        "        self.best_loss = float('inf')\n",
        "        self.phases = ['train', 'val']\n",
        "        self.dataloaders = {\n",
        "            phase: get_dataloader(phase, batch_size) for phase in self.phases\n",
        "        }\n",
        "        self.train_df_logs = pd.DataFrame()\n",
        "        self.val_df_logs = pd.DataFrame()\n",
        "    \n",
        "    def _train_epoch(self, phase):\n",
        "        print(f\"{phase} mode | time: {time.strftime('%H:%M:%S')}\")\n",
        "        \n",
        "        self.net.train() if phase == 'train' else self.net.eval()\n",
        "        meter = Meter()\n",
        "        meter.init_metrics()\n",
        "        \n",
        "        for i, (data, target) in enumerate(self.dataloaders[phase]):\n",
        "            data = data.to(config.device)\n",
        "            target = target.to(config.device)\n",
        "            \n",
        "            output = self.net(data)\n",
        "            loss = self.criterion(output, target)\n",
        "                        \n",
        "            if phase == 'train':\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "            \n",
        "            meter.update(output, target, loss.item())\n",
        "        \n",
        "        metrics = meter.get_metrics()\n",
        "        metrics = {k:v / i for k, v in metrics.items()}\n",
        "        df_logs = pd.DataFrame([metrics])\n",
        "        confusion_matrix = meter.get_confusion_matrix()\n",
        "        \n",
        "        if phase == 'train':\n",
        "            self.train_df_logs = pd.concat([self.train_df_logs, df_logs], axis=0)\n",
        "        else:\n",
        "            self.val_df_logs = pd.concat([self.val_df_logs, df_logs], axis=0)\n",
        "        \n",
        "        # show logs\n",
        "        print('{}: {}, {}: {}, {}: {}, {}: {}, {}: {}'\n",
        "              .format(*(x for kv in metrics.items() for x in kv))\n",
        "             )\n",
        "        fig, ax = plt.subplots(figsize=(5, 5))\n",
        "        cm_ = ax.imshow(confusion_matrix, cmap='hot')\n",
        "        ax.set_title('Confusion matrix', fontsize=15)\n",
        "        ax.set_xlabel('Actual', fontsize=13)\n",
        "        ax.set_ylabel('Predicted', fontsize=13)\n",
        "        plt.colorbar(cm_)\n",
        "        plt.show()\n",
        "        \n",
        "        return loss\n",
        "    \n",
        "    def run(self):\n",
        "        for epoch in range(self.num_epochs):\n",
        "            self._train_epoch(phase='train')\n",
        "            with torch.no_grad():\n",
        "                val_loss = self._train_epoch(phase='val')\n",
        "                self.scheduler.step()\n",
        "            \n",
        "            if val_loss < self.best_loss:\n",
        "                self.best_loss = val_loss\n",
        "                print('\\nNew checkpoint\\n')\n",
        "                self.best_loss = val_loss\n",
        "                torch.save(self.net.state_dict(), f\"best_model_epoc{epoch}.pth\")\n",
        "            #clear_output()"
      ],
      "metadata": {
        "id": "N9f92q4DwUkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_model = RNNAttentionModel(1, 64, 'lstm', False).to(config.device)\n",
        "attn_model.load_state_dict(\n",
        "    torch.load(config.attn_state_path,\n",
        "               map_location=config.device)\n",
        ");\n",
        "attn_model.eval();\n",
        "logs = pd.read_csv(config.attn_logs)"
      ],
      "metadata": {
        "id": "1Y9ZzEbtwq46"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}