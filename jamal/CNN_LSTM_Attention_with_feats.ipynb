{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ADA-SITE-JML/sign-lang/blob/main/jamal/CNN_LSTM_Attention_with_feats.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaMOmJAf2Xn4"
      },
      "source": [
        "In this notebook, all the features from the video frames are extracted and saved separately. The model in this notebook uses the ready features, instead of real-time video processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "StQdMEpvC6Vr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import (CosineAnnealingLR,\n",
        "                                      CosineAnnealingWarmRestarts,\n",
        "                                      StepLR,\n",
        "                                      ExponentialLR)\n",
        "import sklearn.utils\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgiZx1tgjOSw"
      },
      "source": [
        "In CeDAR (Center for Data Analytics Research, ADA University) option, the notebook shall connevt to the local machine (where the whole dataset is supposed to be). To connect to the CeDAR's environment run the following to start Jupyter with access:\n",
        "\n",
        "\n",
        "```\n",
        "jupyter notebook \\\n",
        ">   --NotebookApp.allow_origin='https://colab.research.google.com' \\\n",
        ">   --port=8888 \\\n",
        ">   --NotebookApp.port_retries=0\n",
        "```\n",
        "Then select \"Connect to a local runtime\" and put the link of notebook environment (from the console)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPdLm5R70BBM",
        "outputId": "e34316c5-d233-4378-b75b-cc410e6d6b5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on cuda:0\n",
            "GPU number: 1\n"
          ]
        }
      ],
      "source": [
        "class Config:\n",
        "    debug = False\n",
        "    env = 'Prod' # Dev (Jamal's GoogleDrive), Prod (SLR GDrive) or CeDAR (local)\n",
        "    csv_path = ''\n",
        "    seed = 44\n",
        "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "    #device = 'tpu' # uncomment to switch to TPU usage\n",
        "\n",
        "    video_processing_tool = 'TorchVision' # OpenCV, VidGear or TorchVision\n",
        "    max_frames = 64\n",
        "    max_words_in_sentence = 10\n",
        "\n",
        "\n",
        "    drive_folder = '/home/sign-lang/jamal/Dataset' # path for the local (CeDAR)\n",
        "    if (env == 'Dev'):\n",
        "      drive_folder = 'drive/MyDrive/SLR_test'\n",
        "    elif (env == 'Prod'):\n",
        "      drive_folder = 'drive/MyDrive/SLR/Data'\n",
        "    \n",
        "    feat_folder = drive_folder + '/jamal/Video_features'\n",
        "\n",
        "    train_csv_path = drive_folder+'/sentences_all.csv'\n",
        "    BATCH_SIZE = 1 #updated before the training\n",
        "\n",
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "#    torch.manual_seed(seed)\n",
        "    # if torch.cuda.is_available():\n",
        "    #     torch.cuda.manual_seed(seed)\n",
        "\n",
        "config = Config()\n",
        "seed_everything(config.seed)\n",
        "print('Running on',config.device)\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU number:',torch.cuda.device_count())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if (config.device == 'tpu'):\n",
        "  !pip install cloud-tpu-client==0.10 torch==2.0.0 torchvision==0.15.1 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl\n",
        "\n",
        "  import torch_xla\n",
        "  import torch_xla.core.xla_model as xm\n",
        "  \n",
        "  \n",
        "  dev = xm.xla_device()\n",
        "  t1 = torch.ones(3, 3, device = dev)\n",
        "  print(t1)\n",
        "  config.device = dev"
      ],
      "metadata": {
        "id": "44jGu0Aboz6b"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbEkMcdhlL26",
        "outputId": "fbb64ef4-884a-45ca-d804-dd52e8584301"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "if (config.env != 'CeDAR'):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7PFKq2YWcT4P"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def pip_install(package):\n",
        "  subprocess.check_call([sys.executable, '-m', 'pip', 'install',package])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qr8VkJpS2Blo"
      },
      "outputs": [],
      "source": [
        "train_set_size = 15\n",
        "\n",
        "# read cvs file\n",
        "sentences = pd.read_csv(config.train_csv_path)\n",
        "sentences = sentences.iloc[:,:train_set_size]\n",
        "\n",
        "# unique words\n",
        "word_set = set(['SOS','EOS'])\n",
        "sentences.iloc[:,2].str.lower().str.split().apply(word_set.update)\n",
        "sorted_word_set = sorted(word_set)\n",
        "print('Unique words',sorted_word_set)\n",
        "\n",
        "# create word encoding\n",
        "encodings = { k:v for v,k in enumerate(sorted_word_set)}\n",
        "word_idx  = { v:k for k,v in encodings.items()}\n",
        "print('Word encodings',encodings)\n",
        "print('Words by index',word_idx)\n",
        "torch.save(encodings,config.drive_folder+'/jamal/encodings.dict')\n",
        "torch.save(word_idx,config.drive_folder+'/jamal/word_idx.dict')\n",
        "\n",
        "# converts a sentence with zero padded encoding list\n",
        "def get_sentence_encoded(sentence):\n",
        "    encoded = [encodings[key] for key in ('SOS '+sentence+' EOS').split()]\n",
        "    return  encoded + list([0]) * (config.max_words_in_sentence - len(encoded))\n",
        "\n",
        "if config.debug:  \n",
        "  print(get_sentence_encoded('mən hansı sənəd vermək'))\n",
        "  print(get_sentence_encoded('mən bakı yaşamaq'))\n",
        "\n",
        "# generate (video file name, encoding list)\n",
        "# Good recommendation on not to iterate over DFs like this:\n",
        "# https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas\n",
        "# but it's not my case - I have fewer rows and one to many with videos.\n",
        "df = pd.DataFrame(columns=[\"id\", \"feat_file\",\"encoding\"])\n",
        "\n",
        "#for index, row in sentences.iterrows():\n",
        "for id in range(2,train_set_size):\n",
        "    phrase = sentences.iloc[id,2].lower()\n",
        "    encoded = get_sentence_encoded(phrase)\n",
        "    \n",
        "    dir = config.feat_folder + '/' + str(id)\n",
        "    # iterate over video folders\n",
        "    for filename in os.listdir(dir):\n",
        "        f = os.path.join(dir, filename)\n",
        "        # checking if it is a file\n",
        "        if os.path.isfile(f):\n",
        "            entry = pd.DataFrame.from_dict({\"id\": id, \"feat_file\": f, \"encoding\": [encoded]})\n",
        "            df = pd.concat([df, entry], ignore_index = True)\n",
        "\n",
        "if config.debug:\n",
        "    print(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "yNMIA1a-GdK5",
        "outputId": "5b74b410-abcc-4815-d053-bb470a6ddb6f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  id                                          feat_file  \\\n",
              "0  2  drive/MyDrive/SLR/Data/jamal/Video_features/2/...   \n",
              "1  2  drive/MyDrive/SLR/Data/jamal/Video_features/2/...   \n",
              "2  2  drive/MyDrive/SLR/Data/jamal/Video_features/2/...   \n",
              "3  2  drive/MyDrive/SLR/Data/jamal/Video_features/2/...   \n",
              "4  2  drive/MyDrive/SLR/Data/jamal/Video_features/2/...   \n",
              "\n",
              "                                    encoding  \n",
              "0  [35, 560, 367, 840, 922, 33, 34, 0, 0, 0]  \n",
              "1  [35, 560, 367, 840, 922, 33, 34, 0, 0, 0]  \n",
              "2  [35, 560, 367, 840, 922, 33, 34, 0, 0, 0]  \n",
              "3  [35, 560, 367, 840, 922, 33, 34, 0, 0, 0]  \n",
              "4  [35, 560, 367, 840, 922, 33, 34, 0, 0, 0]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-90345a58-85e9-4db8-8b85-73f8c0d4a133\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>feat_file</th>\n",
              "      <th>encoding</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>drive/MyDrive/SLR/Data/jamal/Video_features/2/...</td>\n",
              "      <td>[35, 560, 367, 840, 922, 33, 34, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>drive/MyDrive/SLR/Data/jamal/Video_features/2/...</td>\n",
              "      <td>[35, 560, 367, 840, 922, 33, 34, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>drive/MyDrive/SLR/Data/jamal/Video_features/2/...</td>\n",
              "      <td>[35, 560, 367, 840, 922, 33, 34, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>drive/MyDrive/SLR/Data/jamal/Video_features/2/...</td>\n",
              "      <td>[35, 560, 367, 840, 922, 33, 34, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>drive/MyDrive/SLR/Data/jamal/Video_features/2/...</td>\n",
              "      <td>[35, 560, 367, 840, 922, 33, 34, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-90345a58-85e9-4db8-8b85-73f8c0d4a133')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-90345a58-85e9-4db8-8b85-73f8c0d4a133 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-90345a58-85e9-4db8-8b85-73f8c0d4a133');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dCKhH_5Zzt3D"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "\n",
        "class SLDataset(Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "        # shuffle and save\n",
        "        self.df = sklearn.utils.shuffle(df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if config.debug:\n",
        "          print(f\"Got item at index: {idx}\")\n",
        "          \n",
        "        feat_path = df.iloc[idx,1]\n",
        "        # loading to CPU is to avoid problems with TPU - it's impossible to convert from GPU to TPU\n",
        "        # feats = torch.load(feat_path, map_location=torch.device('cpu')).to(config.device)\n",
        "        feats = torch.load(feat_path)\n",
        "\n",
        "        encoding = torch.tensor(df.iloc[idx,2]).to(config.device)\n",
        "        enc_shape = encoding.shape[0]\n",
        "\n",
        "        return feats, torch.reshape(encoding,(enc_shape,1)),feat_path\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "def get_dataloader(df, phase: str, batch_size: int = 96) -> DataLoader:\n",
        "    train_df, val_df = train_test_split(df, test_size=0.1, random_state=config.seed, stratify=df['id'])\n",
        "    train_df, val_df = train_df.reset_index(drop=True), val_df.reset_index(drop=True)\n",
        "    df = train_df if phase == 'train' else val_df\n",
        "    dataset = SLDataset(df)\n",
        "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, num_workers=0, shuffle=True)\n",
        "    return dataloader\n",
        "\n",
        "dl = get_dataloader(df,'train',1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgOLGv7gYYi5",
        "outputId": "ccc0cd61-bc5e-4591-ecde-02b388125603"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data fetching: 6.059112071990967 sec\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "measure = time.time()\n",
        "dl_next = next(iter(dl))\n",
        "print('Data fetching:',time.time() - measure,'sec')\n",
        "\n",
        "a,b,fname = dl_next\n",
        "\n",
        "if config.debug:\n",
        "  print(a.shape,b.shape,fname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "14Gs0c9S3Smb"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.models import squeezenet1_1\n",
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, device, biDirectional = False):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.device = device\n",
        "        self.D = 2 if biDirectional else 1\n",
        "\n",
        "        self.rnn = nn.LSTM(\n",
        "                input_size = self.input_size,\n",
        "                hidden_size = self.hidden_size*self.D,\n",
        "                num_layers = 1,\n",
        "                dropout = 0,\n",
        "                bidirectional = biDirectional,\n",
        "                batch_first = True).to(config.device)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output, hidden = self.rnn(input, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return (torch.zeros(self.D, config.BATCH_SIZE, self.hidden_size*self.D, device=self.device),\n",
        "                torch.zeros(self.D, config.BATCH_SIZE, self.hidden_size*self.D, device=self.device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MuhkdQ073o2u"
      },
      "outputs": [],
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, device, dropout_p=0.1, max_length=config.max_frames, biDirectional = False, debug=False): #max_length=config.max_words_in_sentence\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "        self.debug = debug\n",
        "        self.device = device\n",
        "\n",
        "        self.D = 2 if biDirectional else 1\n",
        "\n",
        "        if self.debug:\n",
        "          print('Attn.init() hidden_size',hidden_size)\n",
        "          print('Attn.init() output_size',output_size)\n",
        "          print('Attn.init() max_length',max_length)\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 3, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.rnn = nn.LSTM(\n",
        "                input_size = self.hidden_size,\n",
        "                hidden_size = self.hidden_size*self.D,\n",
        "                num_layers = 1,\n",
        "                dropout = 0,\n",
        "                bidirectional = biDirectional,\n",
        "                batch_first = True)\n",
        "        self.out = nn.Linear(self.hidden_size*self.D, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(input.shape[0],input.shape[1], self.hidden_size)\n",
        "        embedded = self.dropout(embedded)\n",
        "        if self.debug:\n",
        "          print('Attn.forward() input',input.shape)\n",
        "          print('Attn.forward() hidden',type(hidden),len(hidden),hidden[0].shape)\n",
        "          print('Attn.forward() encoder_outputs',encoder_outputs.shape)\n",
        "          print('embedded: ',embedded.shape)\n",
        "\n",
        "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1).to(device=self.device)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),encoder_outputs).to(device=self.device)\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1).to(device=self.device)\n",
        "        output = self.attn_combine(output).unsqueeze(0).to(device=self.device)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.rnn(output, (hidden[0].unsqueeze(0),hidden[0].unsqueeze(0)))\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1).to(device=self.device)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return (torch.zeros(self.D, 1, self.hidden_size*self.D, device=self.device),\n",
        "                torch.zeros(self.D, 1, self.hidden_size*self.D, device=self.device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ogrv5Z_94yP_"
      },
      "outputs": [],
      "source": [
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=config.max_words_in_sentence):\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    if config.debug:\n",
        "      print('Input len',input_tensor.shape,'Target len',target_tensor.shape)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "    encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    decoder_input  = target_tensor[:,:(max_length-2),:]   # words from 1 to n-1\n",
        "    decoder_target = target_tensor[:,1:(max_length-1),:]  # words from 2 to n (the target to the input word is the next word)\n",
        "    tar_1hot = torch.nn.functional.one_hot(decoder_target, num_classes = len(encodings))\n",
        "\n",
        "    if config.debug:\n",
        "      print('Encoder hidden_0',len(encoder_hidden),'shape',encoder_hidden[0].shape)\n",
        "      print('enc_out',encoder_output.shape)\n",
        "      print('dec_in',decoder_input.shape)\n",
        "      print('dec_target',decoder_target.shape)\n",
        "\n",
        "    target_length = decoder_target.size(1)\n",
        "\n",
        "    for di in range(target_length):\n",
        "        if config.debug:\n",
        "          print('dec hidden', decoder_hidden[0].shape,decoder_hidden[1].shape)\n",
        "        \n",
        "        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input[:,di,:], decoder_hidden[0], encoder_output)\n",
        "\n",
        "        if config.debug:\n",
        "          print('decoder_output & attn',decoder_output.shape, decoder_attention.shape)\n",
        "\n",
        "        loss += criterion(decoder_output.squeeze(0), tar_1hot[0,di,:].squeeze(0).double())\n",
        "\n",
        "        if (decoder_target[:,di,:] == torch.tensor(encodings['EOS'], device=config.device)):\n",
        "          break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    if str(config.device).startswith('xla'):\n",
        "      xm.mark_step()\n",
        "  \n",
        "    return loss.item() / (config.BATCH_SIZE*target_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YcGbNQyP5X8v"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "\n",
        "def trainIters(encoder, decoder, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    trainloader = get_dataloader(df,'train',config.BATCH_SIZE)\n",
        "\n",
        "    max_epochs = 10\n",
        "\n",
        "    iter = 1\n",
        "    start = time.time()\n",
        "    for epoch in range(max_epochs):\n",
        "      print('Starting epoch', epoch)\n",
        "      for inputs, labels,fname in trainloader:\n",
        "          if (iter%10 == 0):\n",
        "            print('|', end = '')\n",
        "          else:\n",
        "            print('.', end = '')\n",
        "          input_tensor = inputs.to(config.device)\n",
        "          target_tensor = labels.to(config.device)\n",
        "\n",
        "          try:\n",
        "            loss = train(input_tensor, target_tensor, encoder,\n",
        "                      decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "          except Exception as exp:\n",
        "            print('There was an error: ',fname,exp)\n",
        "            continue\n",
        "\n",
        "          print_loss_total += loss\n",
        "          plot_loss_total += loss\n",
        "\n",
        "          if iter % print_every == 0:\n",
        "              print_loss_avg = print_loss_total / print_every\n",
        "              print_loss_total = 0\n",
        "              print('%.4f' % (print_loss_avg))\n",
        "\n",
        "              # model_scripted = torch.jit.script(encoder) # Export to TorchScript\n",
        "              # model_scripted.save('/jamal/encoder.model') # Save\n",
        "              # model_scripted = torch.jit.script(decoder) # Export to TorchScript\n",
        "              # model_scripted.save('/jamal/decoder.model') # Save\n",
        "              print('Time spent in seconds:',time.time() - start)\n",
        "              start = time.time()\n",
        "              \n",
        "              torch.save(encoder.state_dict(),config.drive_folder+'/jamal/encoder_' + str(config.device) + '.model')\n",
        "              torch.save(decoder.state_dict(),config.drive_folder+'/jamal/decoder_' + str(config.device) + '.model')\n",
        "\n",
        "              gc.collect()\n",
        "\n",
        "              if str(config.device).startswith('cuda'):\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "          if iter % plot_every == 0:\n",
        "              plot_loss_avg = plot_loss_total / plot_every\n",
        "              plot_losses.append(plot_loss_avg)\n",
        "              plot_loss_total = 0\n",
        "\n",
        "          iter += 1\n",
        "    print()             \n",
        "    #showPlot(plot_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0foTbyJ85eD-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "216ec6d3-492c-4753-d1a6-acb61f085c3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 0\n",
            ".........|.........|.........|.........|.........|1.4643\n",
            "Time spent in seconds: 60.02301859855652\n",
            ".........|.........|.........|.........|.........|0.4584\n",
            "Time spent in seconds: 59.401318311691284\n",
            ".........|.........|.........|...Starting epoch 1\n",
            "......|.........|0.3159\n",
            "Time spent in seconds: 40.058908224105835\n",
            ".........|.........|.........|.........|.........|0.2966\n",
            "Time spent in seconds: 3.556232452392578\n",
            ".........|.........|.........|.........|.........|0.2508\n",
            "Time spent in seconds: 3.548758029937744\n",
            ".........|......Starting epoch 2\n",
            "...|.........|.........|.........|0.2518\n",
            "Time spent in seconds: 4.349709987640381\n",
            ".........|.........|.........|.........|.........|0.2475\n",
            "Time spent in seconds: 3.542429208755493\n",
            ".........|.........|.........|.........|.........Starting epoch 3\n",
            "|0.2213\n",
            "Time spent in seconds: 3.529724359512329\n",
            ".........|.........|.........|.........|.........|0.2340\n",
            "Time spent in seconds: 4.484477281570435\n",
            ".........|.........|.........|.........|.........|0.2341\n",
            "Time spent in seconds: 3.539832830429077\n",
            ".........|.........|.........|..Starting epoch 4\n",
            ".......|.........|0.2463\n",
            "Time spent in seconds: 3.5394480228424072\n",
            ".........|.........|.........|.........|.........|0.2322\n",
            "Time spent in seconds: 3.473947525024414\n",
            ".........|.........|.........|.........|.........|0.2456\n",
            "Time spent in seconds: 4.776981592178345\n",
            ".........|.....Starting epoch 5\n",
            "....|.........|.........|.........|0.2705\n",
            "Time spent in seconds: 3.5558807849884033\n",
            ".........|.........|.........|.........|.........|0.2458\n",
            "Time spent in seconds: 4.496368885040283\n",
            ".........|.........|.........|.........|........Starting epoch 6\n",
            ".|0.2486\n",
            "Time spent in seconds: 3.5482337474823\n",
            ".........|.........|.........|.........|.........|0.2396\n",
            "Time spent in seconds: 3.555776357650757\n",
            ".........|.........|.........|"
          ]
        }
      ],
      "source": [
        "input_size = 86528\n",
        "hidden_size = 64\n",
        "config.debug = False\n",
        "\n",
        "encoder = EncoderRNN(input_size, hidden_size, device=config.device, biDirectional = True).to(config.device)\n",
        "attn_decoder = AttnDecoderRNN(hidden_size*2, len(encodings), device=config.device, dropout_p=0.1, biDirectional = False, debug=config.debug).to(config.device)\n",
        "\n",
        "# use the previous weights\n",
        "#encoder.load_state_dict(torch.load(config.drive_folder+'/jamal/encoder_' + str(config.device) + '.model', map_location=torch.device(config.device)))\n",
        "#attn_decoder.load_state_dict(torch.load(config.drive_folder+'/jamal/decoder_' + str(config.device) + '.model', map_location=torch.device(config.device)))\n",
        "\n",
        "config.BATCH_SZIE=64\n",
        "trainIters(encoder, attn_decoder, print_every=50)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}