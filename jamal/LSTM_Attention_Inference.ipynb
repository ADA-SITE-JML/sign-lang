{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import squeezenet1_1\n",
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "Zv7sVM9P-pHW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Yj0FrfJe8yA7"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.models import squeezenet1_1\n",
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, device, biDirectional = False):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        \n",
        "        model = squeezenet1_1(pretrained=True).to(config.device)\n",
        "        return_nodes = {\n",
        "            'features.12.cat': 'layer12'\n",
        "        }\n",
        "        self.pretrained_model = create_feature_extractor(model, return_nodes=return_nodes).to(config.device)\n",
        "        self.pretrained_model.eval()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.device = device\n",
        "        self.D = 2 if biDirectional else 1\n",
        "\n",
        "        self.rnn = nn.LSTM(\n",
        "                input_size = self.input_size,\n",
        "                hidden_size = self.hidden_size*self.D,\n",
        "                num_layers = 1,\n",
        "                dropout = 0,\n",
        "                bidirectional = biDirectional,\n",
        "                batch_first = True).to(config.device)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        features = self.pretrained_model(input.squeeze())['layer12'].to(device=self.device)\n",
        "        feat_shape = features.shape\n",
        "\n",
        "        feat_flat =  torch.reshape(features,(1,feat_shape[0],feat_shape[1]*feat_shape[2]*feat_shape[3])).to(device=self.device)\n",
        "\n",
        "        output, hidden = self.rnn(feat_flat, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return (torch.zeros(self.D, 1, self.hidden_size*self.D, device=self.device),\n",
        "                torch.zeros(self.D, 1, self.hidden_size*self.D, device=self.device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MuhkdQ073o2u"
      },
      "outputs": [],
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, device, dropout_p=0.1, max_length=64, biDirectional = False, debug=False): #max_length=config.max_words_in_sentence\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "        self.debug = debug\n",
        "        self.device = device\n",
        "\n",
        "        self.D = 2 if biDirectional else 1\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 3, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.rnn = nn.LSTM(\n",
        "                input_size = self.hidden_size,\n",
        "                hidden_size = self.hidden_size*self.D,\n",
        "                num_layers = 1,\n",
        "                dropout = 0,\n",
        "                bidirectional = biDirectional,\n",
        "                batch_first = True)\n",
        "        self.out = nn.Linear(self.hidden_size*self.D, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(input.shape[0],input.shape[1], self.hidden_size)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1).to(device=self.device)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),encoder_outputs).to(device=self.device)\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1).to(device=self.device)\n",
        "        output = self.attn_combine(output).unsqueeze(0).to(device=self.device)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.rnn(output, (hidden[0].unsqueeze(0),hidden[0].unsqueeze(0)))\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1).to(device=self.device)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return (torch.zeros(self.D, 1, self.hidden_size*self.D, device=self.device),\n",
        "                torch.zeros(self.D, 1, self.hidden_size*self.D, device=self.device))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, frames, max_length = 64):\n",
        "    with torch.no_grad():\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_output, encoder_hidden = encoder(frames, encoder_hidden)\n",
        "\n",
        "        decoder_input = torch.tensor([[encodings['SOS']]], device=device)  # Start of sentence\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = ''\n",
        "        decoder_attentions = torch.zeros(max_length, max_length, device=device)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden[0], encoder_output)\n",
        "\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "\n",
        "            if topi.item() == encodings['EOS']:\n",
        "                decoded_words += '.'\n",
        "                break\n",
        "            else:\n",
        "                decoded_words += word_idx[topi.item()] + ' '\n",
        "\n",
        "            decoder_input = topi.detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "metadata": {
        "id": "AY8P_4Cz9Sv3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_folder = 'drive/MyDrive/SLR/Data/'"
      ],
      "metadata": {
        "id": "Uvaei1IA_x48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b83ab15-7e30-457c-8a25-b9b7ed80b013"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#data_folder = '/home/sign-lang/jamal/Dataset'\n",
        "\n",
        "encodings = torch.load(data_folder+'/jamal/encodings.dict')\n",
        "word_idx = torch.load(data_folder+'/jamal/word_idx.dict')\n",
        "\n",
        "encoder = torch.load(data_folder+'/jamal/encoder_' + device + '.model', map_location=torch.device(device))\n",
        "decoder = torch.load(data_folder+'/jamal/decoder_' + device + '.model', map_location=torch.device(device))\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "# 25 frames in (64,3,224,224) shape\n",
        "video_frames = torch.zeros((64,3,224,224)).to(device)\n",
        "\n",
        "output_words, attentions = evaluate(encoder, decoder, video_frames)\n",
        "print(output_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D2Yj3Vg9ZT1",
        "outputId": "a32bc97f-2edf-472e-cec7-ed6f7efa9a14"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "m…ôn bu .\n"
          ]
        }
      ]
    }
  ]
}