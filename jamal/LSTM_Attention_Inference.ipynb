{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwCv5VOTN4lPGWLYCWtdwI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ADA-SITE-JML/sign-lang/blob/main/jamal/LSTM_Attention_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import squeezenet1_1\n",
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "import sklearn.utils\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "rnn_type = 'GRU'\n",
        "max_frames = 5\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "print('Running on ' + device + ' with ' + rnn_type)"
      ],
      "metadata": {
        "id": "Zv7sVM9P-pHW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "439d821b-3859-4597-f873-493757acaa39"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on cuda:0 with GRU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Yj0FrfJe8yA7"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.models import squeezenet1_1\n",
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, device, biDirectional = False):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.device = device\n",
        "        # this is for LSTM\n",
        "        self.D = 2 if biDirectional else 1\n",
        "\n",
        "        if rnn_type == 'GRU':\n",
        "          self.rnn = nn.GRU(\n",
        "                  input_size = self.input_size,\n",
        "                  hidden_size = self.hidden_size*self.D,\n",
        "                  batch_first = True).to(device)\n",
        "        elif rnn_type == 'LSTM':\n",
        "          self.rnn = nn.LSTM(\n",
        "                  input_size = self.input_size,\n",
        "                  hidden_size = self.hidden_size*self.D,\n",
        "                  num_layers = 1,\n",
        "                  dropout = 0,\n",
        "                  bidirectional = biDirectional,\n",
        "                  batch_first = True).to(device)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output, hidden = self.rnn(input, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        if rnn_type == 'GRU':\n",
        "          return torch.zeros(self.D, BATCH_SIZE, self.hidden_size*self.D, device=self.device)\n",
        "        elif rnn_type == 'LSTM':\n",
        "          return (torch.zeros(self.D, BATCH_SIZE, self.hidden_size*self.D, device=self.device),\n",
        "                  torch.zeros(self.D, BATCH_SIZE, self.hidden_size*self.D, device=self.device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MuhkdQ073o2u"
      },
      "outputs": [],
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, device, dropout_p=0.1, max_length=max_frames, biDirectional = False, debug=False): #max_length=config.max_words_in_sentence\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "        self.debug = debug\n",
        "        self.device = device\n",
        "\n",
        "        self.D = 2 if biDirectional else 1\n",
        "\n",
        "        if self.debug:\n",
        "          print('Attn.init() hidden_size',hidden_size)\n",
        "          print('Attn.init() output_size',output_size)\n",
        "          print('Attn.init() max_length',max_length)\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size*2)\n",
        "        self.attn = nn.Linear(self.hidden_size * 3, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 3, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "\n",
        "        if rnn_type == 'GRU':\n",
        "          self.rnn = nn.GRU(\n",
        "                  input_size = self.hidden_size,\n",
        "                  hidden_size = self.hidden_size*self.D,\n",
        "                  batch_first = True).to(device)\n",
        "        elif rnn_type == 'LSTM':\n",
        "          self.rnn = nn.LSTM(\n",
        "                  input_size = self.hidden_size,\n",
        "                  hidden_size = self.hidden_size*self.D,\n",
        "                  num_layers = 1,\n",
        "                  dropout = 0,\n",
        "                  bidirectional = biDirectional,\n",
        "                  batch_first = True)\n",
        "        self.out = nn.Linear(self.hidden_size*self.D, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        if rnn_type == 'GRU':\n",
        "          hidden = hidden.unsqueeze(0)\n",
        "\n",
        "        embedded = self.embedding(input).view(input.shape[0],input.shape[1], self.hidden_size*2)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        if self.debug:\n",
        "          print('Attn.forward() input',input.shape)\n",
        "          print('Attn.forward() hidden',type(hidden),len(hidden),hidden[0].shape)\n",
        "          print('Attn.forward() encoder_outputs',encoder_outputs.shape)\n",
        "          print('embedded: ',embedded.shape)\n",
        "\n",
        "        tcat = torch.cat((embedded[0], hidden[0]), 1)\n",
        "        attn_weights = F.softmax(self.attn(tcat), dim=1).to(device=self.device)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),encoder_outputs).to(device=self.device)\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1).to(device=self.device)\n",
        "        output = self.attn_combine(output).unsqueeze(0).to(device=self.device)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        if rnn_type == 'GRU':\n",
        "          output, hidden = self.rnn(output, hidden[0].unsqueeze(0))\n",
        "        elif rnn_type == 'LSTM':\n",
        "          output, hidden = self.rnn(output, (hidden[0].unsqueeze(0),hidden[0].unsqueeze(0)))\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1).to(device=self.device)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        if rnn_type == 'GRU':\n",
        "          return torch.zeros(self.D, 1, self.hidden_size*self.D, device=self.device)\n",
        "        elif rnn_type == 'LSTM':\n",
        "          return (torch.zeros(self.D, 1, self.hidden_size*self.D, device=self.device),\n",
        "                  torch.zeros(self.D, 1, self.hidden_size*self.D, device=self.device))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, frames, max_length = 5):\n",
        "    with torch.no_grad():\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_output, encoder_hidden = encoder(frames, encoder_hidden)\n",
        "\n",
        "        decoder_input = torch.tensor([[encodings['SOS']]], device=device)  # Start of sentence\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = ''\n",
        "        decoder_attentions = torch.zeros(max_length, max_length, device=device)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden[0], encoder_output)\n",
        "\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "\n",
        "            if topi.item() == encodings['EOS']:\n",
        "                decoded_words += '.'\n",
        "                break\n",
        "            else:\n",
        "                decoded_words += word_idx[topi.item()] + ' '\n",
        "\n",
        "            decoder_input = topi.detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "metadata": {
        "id": "AY8P_4Cz9Sv3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_folder = 'drive/MyDrive/SLR/Data/'"
      ],
      "metadata": {
        "id": "Uvaei1IA_x48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78f785a0-1362-4bfc-a60c-625e2ccbe459"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encodings = torch.load(data_folder+'/jamal/encodings.dict')\n",
        "word_idx = torch.load(data_folder+'/jamal/word_idx.dict')\n",
        "\n",
        "# encoder = torch.load(data_folder+'/jamal/encoder_' + device + '.model', map_location=torch.device(device))\n",
        "# decoder = torch.load(data_folder+'/jamal/decoder_' + device + '.model', map_location=torch.device(device))\n",
        "\n",
        "input_size = 2048\n",
        "hidden_size = 64\n",
        "encoding_len = len(encodings)\n",
        "DEBUG = False\n",
        "\n",
        "encoder = EncoderRNN(input_size, hidden_size, device = device, biDirectional = False).to(device)\n",
        "decoder = AttnDecoderRNN(hidden_size, len(encodings), device = device, dropout_p=0.1, biDirectional = False, debug = DEBUG).to(device)\n",
        "\n",
        "encoder.load_state_dict(torch.load(data_folder+'/jamal/encoder_' + device + '.model', map_location=torch.device(device)))\n",
        "decoder.load_state_dict(torch.load(data_folder+'/jamal/decoder_' + device + '.model', map_location=torch.device(device)))\n",
        "\n",
        "# encoder.eval()\n",
        "# decoder.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D2Yj3Vg9ZT1",
        "outputId": "9e9ca510-041f-4810-c822-3db4d67d7b00"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For I3D features\n",
        "!git clone https://github.com/v-iashin/video_features.git\n",
        "!pip install omegaconf==2.0.6\n",
        "\n",
        "%cd video_features\n",
        "\n",
        "from models.i3d.extract_i3d import ExtractI3D\n",
        "from models.raft.raft_src.raft import RAFT, InputPadder\n",
        "from utils.utils import build_cfg_path\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "# Load and patch the config\n",
        "args = OmegaConf.load(build_cfg_path('i3d'))\n",
        "# args.show_pred = True\n",
        "# args.stack_size = 24\n",
        "# args.step_size = 24\n",
        "# args.extraction_fps = 30\n",
        "args.flow_type = 'raft' # 'pwc' is not supported on Google Colab (cupy version mismatch)\n",
        "# args.streams = 'flow'\n",
        "\n",
        "# Load the model\n",
        "extractor = ExtractI3D(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXZC_nP9uGyo",
        "outputId": "158305d2-4c9f-4b4f-bf6e-94181d64b2e0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'video_features'...\n",
            "remote: Enumerating objects: 1299, done.\u001b[K\n",
            "remote: Counting objects: 100% (420/420), done.\u001b[K\n",
            "remote: Compressing objects: 100% (189/189), done.\u001b[K\n",
            "remote: Total 1299 (delta 264), reused 322 (delta 215), pack-reused 879\u001b[K\n",
            "Receiving objects: 100% (1299/1299), 288.63 MiB | 18.76 MiB/s, done.\n",
            "Resolving deltas: 100% (671/671), done.\n",
            "Updating files: 100% (177/177), done.\n",
            "Collecting omegaconf==2.0.6\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf==2.0.6) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf==2.0.6) (4.7.1)\n",
            "Installing collected packages: omegaconf\n",
            "Successfully installed omegaconf-2.0.6\n",
            "/content/video_features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read cvs file\n",
        "# %cd $drive_folder\n",
        "drive_folder = '/content/drive/MyDrive/SLR/Data/'\n",
        "video_folder = drive_folder+'/Video'\n",
        "train_csv_path = drive_folder+'sentences_all.csv'\n",
        "camera_source = 'Cam2' # Cam1 - side-top, Cam2 - front\n",
        "\n",
        "sentences = pd.read_csv(train_csv_path)\n",
        "sentences = sklearn.utils.shuffle(sentences)\n",
        "\n",
        "for index, row in sentences.iterrows():\n",
        "    id = int(row[0])\n",
        "\n",
        "    phrase = row[2].lower()\n",
        "\n",
        "    # there is a grouping of videos in production.\n",
        "    pre_folder = '/1-250/' if (id < 251) else '/'\n",
        "\n",
        "    dir = video_folder+'/' + camera_source + pre_folder + str(id)\n",
        "    # iterate over video folders\n",
        "    fidx = 1\n",
        "\n",
        "    if str(device).startswith('cuda'):\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "    for filename in os.listdir(dir):\n",
        "        if fidx > 3:\n",
        "          break\n",
        "        f = os.path.join(dir, filename)\n",
        "        # checking if it is a file\n",
        "        if os.path.isfile(f):\n",
        "            video_id = filename[:filename.rindex('.')]\n",
        "            print(id,f)\n",
        "\n",
        "            feature_dict = extractor.extract(f)\n",
        "\n",
        "            f_num, f_size = feature_dict['rgb'].shape\n",
        "            REQ_FEATS = 5 # required number of features\n",
        "\n",
        "            # Keep only REQ_FEATS features from each and apply zero padding if there are less than REQ_FEATS features\n",
        "            feats_rgb = torch.from_numpy(feature_dict['rgb'])\n",
        "            feats_flow = torch.from_numpy(feature_dict['flow'])\n",
        "\n",
        "            # Trim extra features.\n",
        "            # Trim shall be applied on each, since we need to have equal number of RGB and FLOW features.\n",
        "            # Like for RGB and FLOW, 8 features each will make 16 features if we apply catenation first.\n",
        "            # If we trimming after that to keep 10 features, eight of them will be about RGB, two - FLOW.\n",
        "            if f_num > REQ_FEATS:\n",
        "              feats_rgb  = feats_rgb[-(REQ_FEATS-f_num):,:]\n",
        "              feats_flow = feats_flow[-(REQ_FEATS-f_num):,:]\n",
        "\n",
        "            # Concatenate the features\n",
        "            feats = torch.cat((feats_rgb,feats_flow),1)\n",
        "\n",
        "            # Apply zero padding if needed.\n",
        "            # Zero padding needs to be done after the catenation - zero features shall come at the end, not after each type (RGB and FLOW)\n",
        "            if f_num < REQ_FEATS:\n",
        "              padarr = torch.zeros((REQ_FEATS-f_num,f_size*2))\n",
        "              feats = torch.cat((feats,padarr),0)\n",
        "\n",
        "            print('Original:',phrase)\n",
        "            output_words, attentions = evaluate(encoder, decoder, feats.float().unsqueeze(0).to(device))\n",
        "            print('Prediction:',output_words)\n",
        "\n",
        "            fidx += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XN4k8jevuXfU",
        "outputId": "0e33e2d4-42c6-4821-e6fc-b273d7971792"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "277 /content/drive/MyDrive/SLR/Data//Video/Cam2/277/2022-10-26 14-04-29.mp4\n",
            "Original: dünən youtube maraqlı video baxmaq olmaq\n",
            "Prediction: mən ev 2 pişik var \n",
            "277 /content/drive/MyDrive/SLR/Data//Video/Cam2/277/2022-10-26 14-56-57.mp4\n",
            "Original: dünən youtube maraqlı video baxmaq olmaq\n",
            "Prediction: mən ev 2 pişik var \n",
            "277 /content/drive/MyDrive/SLR/Data//Video/Cam2/277/2022-11-04 12-32-06.mp4\n",
            "Original: dünən youtube maraqlı video baxmaq olmaq\n",
            "Prediction: mən 2 oğul 1 qız \n",
            "197 /content/drive/MyDrive/SLR/Data//Video/Cam2/1-250/197/2022-06-02 13-01-27.mp4\n",
            "Original: mənim şəkil çəkmək xoşum gəlmək\n",
            "Prediction: mən ev 2 pişik var \n",
            "197 /content/drive/MyDrive/SLR/Data//Video/Cam2/1-250/197/2022-06-02 15-28-48.mp4\n",
            "Original: mənim şəkil çəkmək xoşum gəlmək\n",
            "Prediction: mən 2 oğul 1 qız \n",
            "197 /content/drive/MyDrive/SLR/Data//Video/Cam2/1-250/197/2022-06-02 16-48-05.mp4\n",
            "Original: mənim şəkil çəkmək xoşum gəlmək\n",
            "Prediction: mən ev 2 pişik var \n",
            "282 /content/drive/MyDrive/SLR/Data//Video/Cam2/282/2022-10-26 14-08-07.mp4\n",
            "Original: sənin ləqəb nə ?\n",
            "Prediction: mən ev 2 pişik var \n",
            "282 /content/drive/MyDrive/SLR/Data//Video/Cam2/282/2022-10-26 14-58-53.mp4\n",
            "Original: sənin ləqəb nə ?\n",
            "Prediction: mən 2 oğul 1 qız \n",
            "282 /content/drive/MyDrive/SLR/Data//Video/Cam2/282/2022-11-04 12-42-28.mp4\n",
            "Original: sənin ləqəb nə ?\n",
            "Prediction: mən 2 oğul 1 qız \n",
            "10 /content/drive/MyDrive/SLR/Data//Video/Cam2/1-250/10/2022-04-19 15-19-50.mp4\n",
            "Original: mən orda dünən olmaq\n",
            "Prediction: mən 2 oğul 1 qız \n",
            "10 /content/drive/MyDrive/SLR/Data//Video/Cam2/1-250/10/2022-04-19 15-54-27.mp4\n",
            "Original: mən orda dünən olmaq\n",
            "Prediction: mən 2 oğul 1 qız \n",
            "10 /content/drive/MyDrive/SLR/Data//Video/Cam2/1-250/10/2022-04-21 17-31-00.mp4\n",
            "Original: mən orda dünən olmaq\n",
            "Prediction: mən ev 2 pişik var \n",
            "575 /content/drive/MyDrive/SLR/Data//Video/Cam2/575/2022-12-10 16-10-11.mp4\n",
            "Original: telefon batareyası bitmək zaretqa taxmaq olar ?\n",
            "Prediction: mən 2 oğul 1 qız \n",
            "575 /content/drive/MyDrive/SLR/Data//Video/Cam2/575/2022-12-14 11-26-05.mp4\n",
            "Original: telefon batareyası bitmək zaretqa taxmaq olar ?\n",
            "Prediction: mən ev 2 pişik var \n",
            "575 /content/drive/MyDrive/SLR/Data//Video/Cam2/575/2022-12-14 19-05-35.mp4\n",
            "Original: telefon batareyası bitmək zaretqa taxmaq olar ?\n",
            "Prediction: mən ev 2 pişik var \n",
            "601 /content/drive/MyDrive/SLR/Data//Video/Cam2/601/2022-12-10 16-26-02.mp4\n",
            "Original: vermək kofe çox acı başqa yox ?\n",
            "Prediction: mən ev 2 pişik var \n",
            "601 /content/drive/MyDrive/SLR/Data//Video/Cam2/601/2022-12-14 11-49-39.mp4\n",
            "Original: vermək kofe çox acı başqa yox ?\n",
            "Prediction: mən ev 2 pişik var \n",
            "601 /content/drive/MyDrive/SLR/Data//Video/Cam2/601/2022-12-14 19-14-10.mp4\n",
            "Original: vermək kofe çox acı başqa yox ?\n",
            "Prediction: mən ev 2 pişik var \n",
            "332 /content/drive/MyDrive/SLR/Data//Video/Cam2/332/2022-10-26 16-48-19.mp4\n",
            "Original: siz seçim hansı istiqamət olacaq ?\n",
            "Prediction: mən 2 oğul 1 qız \n",
            "332 /content/drive/MyDrive/SLR/Data//Video/Cam2/332/2022-11-04 14-43-55.mp4\n",
            "Original: siz seçim hansı istiqamət olacaq ?\n",
            "Prediction: mən 2 oğul 1 qız \n",
            "332 /content/drive/MyDrive/SLR/Data//Video/Cam2/332/2022-11-05 18-22-07.mp4\n",
            "Original: siz seçim hansı istiqamət olacaq ?\n",
            "Prediction: mən ev 2 pişik var \n",
            "386 /content/drive/MyDrive/SLR/Data//Video/Cam2/386/2022-10-28 16-18-45.mp4\n",
            "Original: yemək ərzaq baha\n",
            "Prediction: mən 2 oğul 1 qız \n",
            "386 /content/drive/MyDrive/SLR/Data//Video/Cam2/386/2022-10-28 18-03-49.mp4\n",
            "Original: yemək ərzaq baha\n",
            "Prediction: mən 2 oğul 1 qız \n",
            "386 /content/drive/MyDrive/SLR/Data//Video/Cam2/386/2022-11-04 16-01-28.mp4\n",
            "Original: yemək ərzaq baha\n",
            "Prediction: mən 2 oğul 1 qız \n",
            "84 /content/drive/MyDrive/SLR/Data//Video/Cam2/1-250/84/2022-04-26 14-02-22.mp4\n",
            "Original: mənim ana 67 yaş var\n",
            "Prediction: mən ev 2 pişik var \n",
            "84 /content/drive/MyDrive/SLR/Data//Video/Cam2/1-250/84/2022-05-25 17-39-28.mp4\n",
            "Original: mənim ana 67 yaş var\n",
            "Prediction: mən ev 2 pişik var \n",
            "84 /content/drive/MyDrive/SLR/Data//Video/Cam2/1-250/84/2022-05-31 12-32-01.mp4\n",
            "Original: mənim ana 67 yaş var\n",
            "Prediction: mən ev 2 pişik var \n",
            "546 /content/drive/MyDrive/SLR/Data//Video/Cam2/546/2022-12-09 15-06-18.mp4\n",
            "Original: mən hələ uşaq siz fanatınız\n",
            "Prediction: mən ev 2 pişik var \n",
            "546 /content/drive/MyDrive/SLR/Data//Video/Cam2/546/2022-12-09 18-16-31.mp4\n",
            "Original: mən hələ uşaq siz fanatınız\n",
            "Prediction: mən ev 2 pişik var \n",
            "546 /content/drive/MyDrive/SLR/Data//Video/Cam2/546/2022-12-14 18-58-33.mp4\n",
            "Original: mən hələ uşaq siz fanatınız\n",
            "Prediction: mən ev 2 pişik var \n",
            "393 /content/drive/MyDrive/SLR/Data//Video/Cam2/393/2022-10-28 16-28-52.mp4\n",
            "Original: o riyaziyyat ders sevmək\n",
            "Prediction: mən 2 oğul 1 qız \n",
            "393 /content/drive/MyDrive/SLR/Data//Video/Cam2/393/2022-10-28 18-08-19.mp4\n",
            "Original: o riyaziyyat ders sevmək\n",
            "Prediction: mən 2 oğul 1 qız \n",
            "393 /content/drive/MyDrive/SLR/Data//Video/Cam2/393/2022-11-04 16-07-30.mp4\n",
            "Original: o riyaziyyat ders sevmək\n",
            "Prediction: mən 2 oğul 1 qız \n",
            "68 /content/drive/MyDrive/SLR/Data//Video/Cam2/1-250/68/2022-04-26 13-55-23.mp4\n",
            "Original: biz ev qaz qoxu gəlmək\n",
            "Prediction: mən 2 oğul 1 qız \n",
            "68 /content/drive/MyDrive/SLR/Data//Video/Cam2/1-250/68/2022-05-25 17-27-37.mp4\n",
            "Original: biz ev qaz qoxu gəlmək\n",
            "Prediction: mən ev 2 pişik var \n",
            "68 /content/drive/MyDrive/SLR/Data//Video/Cam2/1-250/68/2022-05-31 12-16-06.mp4\n",
            "Original: biz ev qaz qoxu gəlmək\n",
            "Prediction: mən ev 2 pişik var \n",
            "87 /content/drive/MyDrive/SLR/Data//Video/Cam2/1-250/87/2022-04-26 14-04-05.mp4\n",
            "Original: mən yüksək təhsil yox\n",
            "Prediction: mən ev 2 pişik var \n",
            "87 /content/drive/MyDrive/SLR/Data//Video/Cam2/1-250/87/2022-04-26 14-04-17.mp4\n",
            "Original: mən yüksək təhsil yox\n",
            "Prediction: mən ev 2 pişik var \n",
            "87 /content/drive/MyDrive/SLR/Data//Video/Cam2/1-250/87/2022-05-25 17-44-02.mp4\n",
            "Original: mən yüksək təhsil yox\n",
            "Prediction: mən ev 2 pişik var \n",
            "632 /content/drive/MyDrive/SLR/Data//Video/Cam2/632/2022-12-10 16-50-51.mp4\n",
            "Original: dəqiq vaxt mənə de səni gözləmək\n",
            "Prediction: mən ev 2 pişik var \n",
            "632 /content/drive/MyDrive/SLR/Data//Video/Cam2/632/2022-12-14 12-19-27.mp4\n",
            "Original: dəqiq vaxt mənə de səni gözləmək\n",
            "Prediction: mən ev 2 pişik var \n",
            "632 /content/drive/MyDrive/SLR/Data//Video/Cam2/632/2022-12-14 19-21-55.mp4\n",
            "Original: dəqiq vaxt mənə de səni gözləmək\n",
            "Prediction: mən ev 2 pişik var \n",
            "356 /content/drive/MyDrive/SLR/Data//Video/Cam2/356/2022-10-28 15-48-40.mp4\n",
            "Original: mən ət yemir\n",
            "Prediction: mən 2 oğul 1 qız \n",
            "356 /content/drive/MyDrive/SLR/Data//Video/Cam2/356/2022-10-28 17-40-25.mp4\n",
            "Original: mən ət yemir\n",
            "Prediction: mən 2 oğul 1 qız \n",
            "356 /content/drive/MyDrive/SLR/Data//Video/Cam2/356/2022-11-04 15-30-20.mp4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-daff17c3b716>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mfeature_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mf_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rgb'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/video_features/models/i3d/extract_i3d.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(self, video_path)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;31m# preprocess the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mrgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                 \u001b[0mrgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                 \u001b[0mrgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/video_features/models/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_to_smaller_edge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/video_features/models/transforms.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, resize_to_smaller_edge, interpolation)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0moh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2006\u001b[0m                 )\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}