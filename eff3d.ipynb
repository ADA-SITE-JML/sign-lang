{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "import os\n",
    "import cv2\n",
    "import pdb\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torch.nn.functional import InterpolationMode\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import logging\n",
    "import datetime\n",
    "import sys\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gloss_df_path = \"data_validation/processed_gloss.csv\"\n",
    "gloss_df = pd.read_csv(gloss_df_path)\n",
    "gloss_df.dropna(inplace=True)\n",
    "gloss_df.replace(to_replace=\"ASHAG\", value=\"AŞAĞI\", inplace=True)\n",
    "gloss_df['glossRange'] = gloss_df['glossEnd'] - gloss_df['glossStart']\n",
    "# gloss_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(gloss_df.gloss.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchviz\n",
    "from pytorchvideo.data import LabeledVideoDataset, make_clip_sampler\n",
    "from torchvision.models import squeezenet1_1, SqueezeNet1_1_Weights\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in 0.14. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages/torchvision/transforms/_transforms_video.py:25: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in 0.14. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pytorchvideo.data import labeled_video_dataset\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    "    Permute\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomAdjustSharpness,\n",
    "    Resize\n",
    ")\n",
    "\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_transform = Compose([\n",
    "    ApplyTransformToKey(key=\"video\",\n",
    "    transform=Compose([\n",
    "        UniformTemporalSubsample(25),\n",
    "        Lambda(lambda x: x/255),\n",
    "        Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "        RandomShortSideScale(min_size=224, max_size=256),\n",
    "        CenterCropVideo(224),\n",
    "    ]),\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../data/gloss_train\"\n",
    "val_path = \"../data/gloss_val\"\n",
    "test_path = \"../data/gloss_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningModule, seed_everything, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.metrics import classification_report\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/toghrul/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "source": [
    "video_model = torch.hub.load('facebookresearch/pytorchvideo', 'efficient_x3d_xs', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoModel(LightningModule):\n",
    "    def __init__(self, ):\n",
    "        super(VideoModel, self).__init__()\n",
    "\n",
    "        self.video_model = torch.hub.load('facebookresearch/pytorchvideo', 'efficient_x3d_xs', pretrained=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(400, 1)\n",
    "\n",
    "        self.lr = 1e-3\n",
    "        self.batch_size = 8\n",
    "        self.num_worker = 4\n",
    "\n",
    "        # self.metric = torchmetrics.F1Score(task=\"multiclass\", num_classes=3)\n",
    "\n",
    "        #loss\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.video_model(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(params=self.parameters(), lr = self.lr)\n",
    "        scheduler = CosineAnnealingLR(opt, T_max=10, eta_min=1e-6, last_epoch=-1)\n",
    "        return {'optimizer': opt,\n",
    "                'lr_scheduler': scheduler}\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = labeled_video_dataset(train_path, clip_sampler=make_clip_sampler('random', 2),\n",
    "                                         transform=video_transform, decode_audio=False)\n",
    "\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, num_workers=self.num_worker, pin_memory=True)\n",
    "\n",
    "        return loader\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        video, label = batch['video'], batch['label']\n",
    "        out = self(video)\n",
    "        # print(f\"Pred:\\n{out}\")\n",
    "        # print(f\"Pred shape:\\n{out.shape}\")\n",
    "        # print(f\"Label:\\n{label}\")\n",
    "        # print(f\"Label shape:\\n{label.shape}\")\n",
    "        # print(\">>> INFO: Computing Training Loss\")\n",
    "        loss = self.criterion(out, label.float().unsqueeze(1))\n",
    "        print(f\">>>{loss}\")\n",
    "        \n",
    "        # print(\">>> INFO: Training Loss Computed\")\n",
    "        # print(\">>> INFO: Computing Training Metric\")\n",
    "        # metric = self.metric(out, label.to(torch.int64))\n",
    "\n",
    "        # return {\"loss\": loss,\n",
    "        #         \"metric\": metric.detach()}\n",
    "        \n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        loss = torch.stack([x['loss'] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        # metric = torch.stack([x['metric'] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        self.log('training_loss', loss)\n",
    "        print(f\">>>Epoch end loss: {loss}\")\n",
    "        # self.log('training_metric', metric)\n",
    "        \n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataset = labeled_video_dataset(val_path, clip_sampler=make_clip_sampler('random', 2),\n",
    "                                         transform=video_transform, decode_audio=False)\n",
    "\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, num_workers=self.num_worker, pin_memory=True)\n",
    "\n",
    "        return loader\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        video, label = batch['video'], batch['label']\n",
    "        out = self(video)\n",
    "        # print(\">>> INFO: Computing Val Loss\")\n",
    "        loss = self.criterion(out, label.float().unsqueeze(1))\n",
    "        # print(\">>> INFO: Val Loss Computed\")\n",
    "        # print(\">>> INFO: Computing Val Metric\")\n",
    "        # metric = self.metric(out, label.to(torch.int64))\n",
    "\n",
    "        # return {\"loss\": loss,\n",
    "        #         \"metric\": metric.detach()}\n",
    "        \n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        loss = torch.stack([x['loss'] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        # metric = torch.stack([x['metric'] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        self.log('val_loss', loss)\n",
    "        # self.log('val_metric', metric)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        dataset = labeled_video_dataset(test_path, clip_sampler=make_clip_sampler('random', 2),\n",
    "                                         transform=video_transform, decode_audio=False)\n",
    "\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, num_workers=self.num_worker, pin_memory=True)\n",
    "\n",
    "        return loader\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        video, label = batch['video'], batch['label']\n",
    "        out = self.forward(video)\n",
    "\n",
    "        return {\"label\": label,\n",
    "                \"pred\": out.detach()}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        label=torch.cat([x['label'] for x in outputs]).cpu().numpy()\n",
    "        pred = torch.cat([x['pred'] for x in outputs]).cpu().numpy()\n",
    "\n",
    "        print(classification_report(label, pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss', dirpath=\"checkpoints\", \n",
    "                                    verbose=True, save_last=True, save_top_k=2)\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"epoch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/toghrul/.cache/torch/hub/facebookresearch_pytorchvideo_main\n",
      "Global seed set to 0\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model = VideoModel()\n",
    "seed_everything(0)\n",
    "\n",
    "trainer = Trainer(max_epochs=1,\n",
    "                accelerator=\"gpu\", devices=-1,\n",
    "                precision=16,\n",
    "                accumulate_grad_batches=2,\n",
    "                enable_progress_bar=True,\n",
    "                num_sanity_val_steps=0,\n",
    "                callbacks=[lr_monitor, checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | video_model | EfficientX3d     | 3.8 M \n",
      "1 | relu        | ReLU             | 0     \n",
      "2 | fc          | Linear           | 401   \n",
      "3 | criterion   | CrossEntropyLoss | 0     \n",
      "-------------------------------------------------\n",
      "3.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.8 M     Total params\n",
      "7.589     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a3bbfe8ab2405d93907960ffcfffa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>-0.0\n",
      ">>>-0.0\n",
      ">>>-0.0\n",
      ">>>-0.0\n",
      ">>>-0.0\n",
      ">>>-0.0\n",
      ">>>-0.0\n",
      ">>>-0.0\n",
      ">>>-0.0\n",
      ">>>-0.0\n",
      ">>>-0.0\n",
      ">>>-0.0\n",
      ">>>-0.0\n",
      ">>>-0.0\n",
      ">>>-0.0\n",
      ">>>-0.0\n",
      ">>>-0.0\n",
      ">>>-0.0\n",
      ">>>-0.0\n",
      ">>>-0.0\n",
      ">>>-0.0\n",
      ">>>-0.0\n",
      ">>>-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:653: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('sign-lang')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96a4e4d8fc4dcb6ce321df308d690f3398dc6d289b3efb6c91f90112c618c739"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
