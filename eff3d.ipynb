{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "import os\n",
    "import cv2\n",
    "import pdb\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torch.nn.functional import InterpolationMode\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import logging \n",
    "import datetime\n",
    "import sys\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gloss_df_path = \"data_validation/processed_gloss.csv\"\n",
    "gloss_df = pd.read_csv(gloss_df_path)\n",
    "gloss_df.dropna(inplace=True)\n",
    "gloss_df.replace(to_replace=\"ASHAG\", value=\"AŞAĞI\", inplace=True)\n",
    "gloss_df['glossRange'] = gloss_df['glossEnd'] - gloss_df['glossStart']\n",
    "# gloss_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(gloss_df.gloss.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchviz\n",
    "from pytorchvideo.data import LabeledVideoDataset, make_clip_sampler\n",
    "from torchvision.models import squeezenet1_1, SqueezeNet1_1_Weights\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in 0.14. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages/torchvision/transforms/_transforms_video.py:25: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in 0.14. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pytorchvideo.data import labeled_video_dataset\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    "    Permute\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomAdjustSharpness,\n",
    "    Resize\n",
    ")\n",
    "\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_transform = Compose([\n",
    "    ApplyTransformToKey(key=\"video\",\n",
    "    transform=Compose([\n",
    "        UniformTemporalSubsample(25),\n",
    "        Lambda(lambda x: x/255),\n",
    "        Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "        RandomShortSideScale(min_size=224, max_size=256),\n",
    "        CenterCropVideo(224),\n",
    "    ]),\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/home/toghrul/SLR/data/train\"\n",
    "val_path = \"/home/toghrul/SLR/data/val\"\n",
    "test_path = \"/home/toghrul/SLR/data/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningModule, seed_everything, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.metrics import classification_report\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_model = torch.hub.load('facebookresearch/pytorchvideo', 'efficient_x3d_xs', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.randn(size=(8, num_classes)).requires_grad_()\n",
    "# b = torch.randint(5, size=(8, ), dtype=torch.long)\n",
    "# a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = F.one_hot(b, num_classes=num_classes).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# criterion(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = nn.CrossEntropyLoss()\n",
    "# input = torch.randn(3, 5, requires_grad=True)\n",
    "# target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "# output = loss(input, target)\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoModel(LightningModule):\n",
    "    def __init__(self, ):\n",
    "        super(VideoModel, self).__init__()\n",
    "\n",
    "        self.video_model = torch.hub.load('facebookresearch/pytorchvideo', 'efficient_x3d_xs', pretrained=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(400, num_classes)\n",
    "\n",
    "        self.lr = 1e-3\n",
    "        self.batch_size = 8\n",
    "        self.num_worker = 4\n",
    "        self.num_steps_train = 0\n",
    "        self.num_steps_val = 0\n",
    "\n",
    "        self.metric = torchmetrics.classification.MultilabelAccuracy(num_labels=num_classes)\n",
    "\n",
    "        #loss\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.video_model(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(params=self.parameters(), lr = self.lr)\n",
    "        scheduler = CosineAnnealingLR(opt, T_max=10, eta_min=1e-6, last_epoch=-1)\n",
    "        return {'optimizer': opt,\n",
    "                'lr_scheduler': scheduler}\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = labeled_video_dataset(train_path, clip_sampler=make_clip_sampler('random', 2),\n",
    "                                         transform=video_transform, decode_audio=False)\n",
    "\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, num_workers=self.num_worker, pin_memory=True)\n",
    "\n",
    "        return loader\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        video, label = batch['video'], batch['label']\n",
    "        label = F.one_hot(label, num_classes=num_classes).float()\n",
    "        out = self(video)\n",
    "        print(f\">>> Training step No.{self.num_steps_train}:\")\n",
    "        # print(\"Pred:\", out)\n",
    "        # print(\"GT:\", label)\n",
    "        # print(f\"Pred:\\n{out}\")\n",
    "        # print(f\"Pred shape:\\n{out.shape}\")\n",
    "        # print(f\"Label:\\n{label}\")\n",
    "        # print(f\"Label shape:\\n{label.shape}\")\n",
    "        # print(\">>> INFO: Computing Training Loss\")\n",
    "        loss = self.criterion(out, label)\n",
    "        print(f\"Loss: {loss}\")\n",
    "        self.num_steps_train += 1\n",
    "        # print(\">>> INFO: Training Loss Computed\")\n",
    "        # print(\">>> INFO: Computing Training Metric\")\n",
    "        metric = self.metric(out, label)\n",
    "        print(f\"Accuracy: {metric}\")\n",
    "\n",
    "        values = {\"loss\": loss,\n",
    "                \"metric\": metric.detach()}\n",
    "        \n",
    "        self.log_dict({\"step_loss\": loss,\n",
    "                        \"step_metric\": metric.detach()})\n",
    "        \n",
    "        return values\n",
    "        \n",
    "        # return {\"loss\": loss}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        loss = torch.stack([x['loss'] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        metric = torch.stack([x['metric'] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        \n",
    "        self.log('training_loss', loss)\n",
    "        print(f\">>> Epoch end loss: {loss}\")\n",
    "        self.log('training_metric', metric)\n",
    "        \n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataset = labeled_video_dataset(val_path, clip_sampler=make_clip_sampler('random', 2),\n",
    "                                         transform=video_transform, decode_audio=False)\n",
    "\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, num_workers=self.num_worker, pin_memory=True)\n",
    "\n",
    "        return loader\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        video, label = batch['video'], batch['label']\n",
    "        label = F.one_hot(label, num_classes=num_classes).float()\n",
    "        out = self(video)\n",
    "        # print(\">>> INFO: Computing Val Loss\")\n",
    "        print(f\">>> Validation step No.{self.num_steps_val}:\")\n",
    "        loss = self.criterion(out, label)\n",
    "        print(f\"Loss: {loss}\")\n",
    "        self.num_steps_val += 1\n",
    "        # print(\">>> INFO: Val Loss Computed\")\n",
    "        # print(\">>> INFO: Computing Val Metric\")\n",
    "        metric = self.metric(out, label)\n",
    "        print(f\"Accuracy: {metric}\")\n",
    "        \n",
    "\n",
    "        return {\"loss\": loss,\n",
    "                \"metric\": metric.detach()}\n",
    "        \n",
    "        # return {\"loss\": loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        loss = torch.stack([x['loss'] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        metric = torch.stack([x['metric'] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_metric', metric)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        dataset = labeled_video_dataset(test_path, clip_sampler=make_clip_sampler('random', 2),\n",
    "                                         transform=video_transform, decode_audio=False)\n",
    "\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, num_workers=self.num_worker, pin_memory=True)\n",
    "\n",
    "        return loader\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        video, label = batch['video'], batch['label']\n",
    "        label = F.one_hot(label, num_classes=num_classes).float()\n",
    "        out = self.forward(video)\n",
    "        metric = self.metric(out, label)\n",
    "\n",
    "        return {\"label\": label,\n",
    "                \"pred\": out.detach(),}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        label=torch.cat([x['label'] for x in outputs]).cpu().numpy()\n",
    "        pred = torch.cat([x['pred'] for x in outputs]).cpu().numpy()\n",
    "\n",
    "        print(f\">> Label: {label}\\nPred: {pred}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss', dirpath=\"checkpoints\", \n",
    "                                    verbose=True, save_last=True, save_top_k=2)\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"epoch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/toghrul/.cache/torch/hub/facebookresearch_pytorchvideo_main\n",
      "Global seed set to 0\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model = VideoModel()\n",
    "seed_everything(0)\n",
    "\n",
    "trainer = Trainer(max_epochs=10,\n",
    "                accelerator=\"gpu\", devices=-1,\n",
    "                precision=16,\n",
    "                # accumulate_grad_batches=2,\n",
    "                enable_progress_bar=True,\n",
    "                # num_sanity_val_steps=0,\n",
    "                callbacks=[lr_monitor, checkpoint_callback],\n",
    "                log_every_n_steps=5,\n",
    "                limit_train_batches=10,\n",
    "                limit_val_batches=5,\n",
    "                limit_test_batches=5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type               | Params\n",
      "---------------------------------------------------\n",
      "0 | video_model | EfficientX3d       | 3.8 M \n",
      "1 | relu        | ReLU               | 0     \n",
      "2 | fc          | Linear             | 93.8 K\n",
      "3 | metric      | MultilabelAccuracy | 0     \n",
      "4 | criterion   | BCEWithLogitsLoss  | 0     \n",
      "---------------------------------------------------\n",
      "3.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.9 M     Total params\n",
      "7.776     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46b913c1ba3444c280daaaecce64876b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.0:\n",
      "Loss: 0.7293049693107605\n",
      "Accuracy: 0.5438034534454346\n",
      ">>> Validation step No.1:\n",
      "Loss: 0.7303302884101868\n",
      "Accuracy: 0.5507479310035706\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f764fcd4d21478c8a023330b847b9a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training step No.0:\n",
      "Loss: 0.7077091932296753\n",
      "Accuracy: 0.5464743971824646\n",
      ">>> Training step No.1:\n",
      "Loss: 0.5746216177940369\n",
      "Accuracy: 0.7393162846565247\n",
      ">>> Training step No.2:\n",
      "Loss: 0.3870113492012024\n",
      "Accuracy: 0.8616453409194946\n",
      ">>> Training step No.3:\n",
      "Loss: 0.2158113569021225\n",
      "Accuracy: 0.939636766910553\n",
      ">>> Training step No.4:\n",
      "Loss: 0.10342903435230255\n",
      "Accuracy: 0.9780983328819275\n",
      ">>> Training step No.5:\n",
      "Loss: 0.056837841868400574\n",
      "Accuracy: 0.9914530515670776\n",
      ">>> Training step No.6:\n",
      "Loss: 0.03889686241745949\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.7:\n",
      "Loss: 0.040093887597322464\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.8:\n",
      "Loss: 0.04646025225520134\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.9:\n",
      "Loss: 0.04310651496052742\n",
      "Accuracy: 0.9957265853881836\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7faa9796ebb94d91bc0c02f8fcb8f995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.2:\n",
      "Loss: 0.10697975754737854\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Validation step No.3:\n",
      "Loss: 0.11796384304761887\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Validation step No.4:\n",
      "Loss: 0.11326473951339722\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Validation step No.5:\n",
      "Loss: 0.12199532985687256\n",
      "Accuracy: 0.9957265257835388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 10: 'val_loss' reached 0.12000 (best 0.12000), saving model to '/home/toghrul/SLR/sign-lang/checkpoints/epoch=0-step=10.ckpt' as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.6:\n",
      "Loss: 0.12751246988773346\n",
      "Accuracy: 0.995726466178894\n",
      ">>> Epoch end loss: 0.2199999988079071\n",
      ">>> Training step No.10:\n",
      "Loss: 0.04844420775771141\n",
      "Accuracy: 0.995726466178894\n",
      ">>> Training step No.11:\n",
      "Loss: 0.045509763062000275\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Training step No.12:\n",
      "Loss: 0.0638267919421196\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.13:\n",
      "Loss: 0.06256014853715897\n",
      "Accuracy: 0.9962607622146606\n",
      ">>> Training step No.14:\n",
      "Loss: 0.047053053975105286\n",
      "Accuracy: 0.9946581721305847\n",
      ">>> Training step No.15:\n",
      "Loss: 0.04092346131801605\n",
      "Accuracy: 0.9951923489570618\n",
      ">>> Training step No.16:\n",
      "Loss: 0.04732353240251541\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.17:\n",
      "Loss: 0.0523768812417984\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.18:\n",
      "Loss: 0.030742913484573364\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Training step No.19:\n",
      "Loss: 0.04022940993309021\n",
      "Accuracy: 0.995726466178894\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e1d9208e2344faad50b881e0f01b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.7:\n",
      "Loss: 0.06281205266714096\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Validation step No.8:\n",
      "Loss: 0.06427343189716339\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Validation step No.9:\n",
      "Loss: 0.05607280135154724\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Validation step No.10:\n",
      "Loss: 0.06544362753629684\n",
      "Accuracy: 0.9957265257835388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 20: 'val_loss' reached 0.06000 (best 0.06000), saving model to '/home/toghrul/SLR/sign-lang/checkpoints/epoch=1-step=20.ckpt' as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.11:\n",
      "Loss: 0.074832022190094\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Epoch end loss: 0.05000000074505806\n",
      ">>> Training step No.20:\n",
      "Loss: 0.041221294552087784\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.21:\n",
      "Loss: 0.04688849672675133\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.22:\n",
      "Loss: 0.03543812036514282\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.23:\n",
      "Loss: 0.03195163980126381\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.24:\n",
      "Loss: 0.030084848403930664\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.25:\n",
      "Loss: 0.02951720356941223\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.26:\n",
      "Loss: 0.034219879657030106\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.27:\n",
      "Loss: 0.02788941189646721\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.28:\n",
      "Loss: 0.02420848421752453\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Training step No.29:\n",
      "Loss: 0.031377095729112625\n",
      "Accuracy: 0.9957265853881836\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d3346efaef494aa7d3e2aa36316e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.12:\n",
      "Loss: 0.037578243762254715\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Validation step No.13:\n",
      "Loss: 0.03535047173500061\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Validation step No.14:\n",
      "Loss: 0.03592156618833542\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Validation step No.15:\n",
      "Loss: 0.03520483896136284\n",
      "Accuracy: 0.9957265257835388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 30: 'val_loss' reached 0.04000 (best 0.04000), saving model to '/home/toghrul/SLR/sign-lang/checkpoints/epoch=2-step=30.ckpt' as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.16:\n",
      "Loss: 0.03358329087495804\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Epoch end loss: 0.029999999329447746\n",
      ">>> Training step No.30:\n",
      "Loss: 0.03095480240881443\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.31:\n",
      "Loss: 0.031234052032232285\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Training step No.32:\n",
      "Loss: 0.02643049880862236\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.33:\n",
      "Loss: 0.03152305632829666\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Training step No.34:\n",
      "Loss: 0.02625282108783722\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.35:\n",
      "Loss: 0.020671654492616653\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Training step No.36:\n",
      "Loss: 0.021355731412768364\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.37:\n",
      "Loss: 0.028371017426252365\n",
      "Accuracy: 0.995726466178894\n",
      ">>> Training step No.38:\n",
      "Loss: 0.02674509398639202\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.39:\n",
      "Loss: 0.028680317103862762\n",
      "Accuracy: 0.9957265257835388\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1434da581f49faa1285b6c276bf3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.17:\n",
      "Loss: 0.03259549289941788\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Validation step No.18:\n",
      "Loss: 0.03410656377673149\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Validation step No.19:\n",
      "Loss: 0.030004926025867462\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Validation step No.20:\n",
      "Loss: 0.03051418624818325\n",
      "Accuracy: 0.9957265853881836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 40: 'val_loss' reached 0.03000 (best 0.03000), saving model to '/home/toghrul/SLR/sign-lang/checkpoints/epoch=3-step=40.ckpt' as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.21:\n",
      "Loss: 0.031827107071876526\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Epoch end loss: 0.029999999329447746\n",
      ">>> Training step No.40:\n",
      "Loss: 0.02696157805621624\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.41:\n",
      "Loss: 0.018960706889629364\n",
      "Accuracy: 0.995726466178894\n",
      ">>> Training step No.42:\n",
      "Loss: 0.03149951249361038\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.43:\n",
      "Loss: 0.025965692475438118\n",
      "Accuracy: 0.995726466178894\n",
      ">>> Training step No.44:\n",
      "Loss: 0.03104996681213379\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Training step No.45:\n",
      "Loss: 0.028003564104437828\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.46:\n",
      "Loss: 0.02630777098238468\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.47:\n",
      "Loss: 0.02318665012717247\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.48:\n",
      "Loss: 0.033082976937294006\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.49:\n",
      "Loss: 0.022747397422790527\n",
      "Accuracy: 0.9957265257835388\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7078577d6c74bc99060e0b4d56df3cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.22:\n",
      "Loss: 0.03143312782049179\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Validation step No.23:\n",
      "Loss: 0.032869793474674225\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Validation step No.24:\n",
      "Loss: 0.03272216394543648\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Validation step No.25:\n",
      "Loss: 0.031097078695893288\n",
      "Accuracy: 0.9957265257835388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 50: 'val_loss' reached 0.03000 (best 0.03000), saving model to '/home/toghrul/SLR/sign-lang/checkpoints/epoch=4-step=50.ckpt' as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.26:\n",
      "Loss: 0.030635297298431396\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Epoch end loss: 0.029999999329447746\n",
      ">>> Training step No.50:\n",
      "Loss: 0.02659144438803196\n",
      "Accuracy: 0.995726466178894\n",
      ">>> Training step No.51:\n",
      "Loss: 0.0280385073274374\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.52:\n",
      "Loss: 0.02982749231159687\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.53:\n",
      "Loss: 0.02450150065124035\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.54:\n",
      "Loss: 0.026779083535075188\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.55:\n",
      "Loss: 0.02551215887069702\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.56:\n",
      "Loss: 0.02811128832399845\n",
      "Accuracy: 0.995726466178894\n",
      ">>> Training step No.57:\n",
      "Loss: 0.021916117519140244\n",
      "Accuracy: 0.995726466178894\n",
      ">>> Training step No.58:\n",
      "Loss: 0.023967834189534187\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.59:\n",
      "Loss: 0.02954760380089283\n",
      "Accuracy: 0.9957265257835388\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd1d7f267d04ce8a2bf12e9fcf4a8d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.27:\n",
      "Loss: 0.03137540817260742\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Validation step No.28:\n",
      "Loss: 0.030733618885278702\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Validation step No.29:\n",
      "Loss: 0.03213072568178177\n",
      "Accuracy: 0.995726466178894\n",
      ">>> Validation step No.30:\n",
      "Loss: 0.030113110318779945\n",
      "Accuracy: 0.995726466178894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 60: 'val_loss' was not in top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.31:\n",
      "Loss: 0.031316183507442474\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Epoch end loss: 0.029999999329447746\n",
      ">>> Training step No.60:\n",
      "Loss: 0.02793314680457115\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.61:\n",
      "Loss: 0.021278459578752518\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Training step No.62:\n",
      "Loss: 0.021779512986540794\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.63:\n",
      "Loss: 0.02251744642853737\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.64:\n",
      "Loss: 0.0228579118847847\n",
      "Accuracy: 0.9962607026100159\n",
      ">>> Training step No.65:\n",
      "Loss: 0.020932108163833618\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.66:\n",
      "Loss: 0.021492518484592438\n",
      "Accuracy: 0.995726466178894\n",
      ">>> Training step No.67:\n",
      "Loss: 0.023038413375616074\n",
      "Accuracy: 0.995726466178894\n",
      ">>> Training step No.68:\n",
      "Loss: 0.023397106677293777\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.69:\n",
      "Loss: 0.019465647637844086\n",
      "Accuracy: 0.9962607622146606\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7863b00086404d0ba843c6020e120ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.32:\n",
      "Loss: 0.03353780880570412\n",
      "Accuracy: 0.9946581125259399\n",
      ">>> Validation step No.33:\n",
      "Loss: 0.029089398682117462\n",
      "Accuracy: 0.9946581125259399\n",
      ">>> Validation step No.34:\n",
      "Loss: 0.03161735460162163\n",
      "Accuracy: 0.9951923489570618\n",
      ">>> Validation step No.35:\n",
      "Loss: 0.03042745776474476\n",
      "Accuracy: 0.9946581125259399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 70: 'val_loss' was not in top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.36:\n",
      "Loss: 0.03343329578638077\n",
      "Accuracy: 0.9941239356994629\n",
      ">>> Epoch end loss: 0.019999999552965164\n",
      ">>> Training step No.70:\n",
      "Loss: 0.022672448307275772\n",
      "Accuracy: 0.9962607026100159\n",
      ">>> Training step No.71:\n",
      "Loss: 0.02493852749466896\n",
      "Accuracy: 0.995192289352417\n",
      ">>> Training step No.72:\n",
      "Loss: 0.022128595039248466\n",
      "Accuracy: 0.9951923489570618\n",
      ">>> Training step No.73:\n",
      "Loss: 0.02365112490952015\n",
      "Accuracy: 0.9962607622146606\n",
      ">>> Training step No.74:\n",
      "Loss: 0.030309544876217842\n",
      "Accuracy: 0.9951923489570618\n",
      ">>> Training step No.75:\n",
      "Loss: 0.02229502610862255\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.76:\n",
      "Loss: 0.02374919131398201\n",
      "Accuracy: 0.9962607622146606\n",
      ">>> Training step No.77:\n",
      "Loss: 0.027821186929941177\n",
      "Accuracy: 0.9962607026100159\n",
      ">>> Training step No.78:\n",
      "Loss: 0.01947598345577717\n",
      "Accuracy: 0.9967948794364929\n",
      ">>> Training step No.79:\n",
      "Loss: 0.02431837096810341\n",
      "Accuracy: 0.9962607622146606\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52840866eef4a4cad128e3c8b6e1603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.37:\n",
      "Loss: 0.03182431682944298\n",
      "Accuracy: 0.9941239356994629\n",
      ">>> Validation step No.38:\n",
      "Loss: 0.03411296382546425\n",
      "Accuracy: 0.9941239356994629\n",
      ">>> Validation step No.39:\n",
      "Loss: 0.02872391976416111\n",
      "Accuracy: 0.9951923489570618\n",
      ">>> Validation step No.40:\n",
      "Loss: 0.029036955907940865\n",
      "Accuracy: 0.9951923489570618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 80: 'val_loss' was not in top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.41:\n",
      "Loss: 0.03284301981329918\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Epoch end loss: 0.019999999552965164\n",
      ">>> Training step No.80:\n",
      "Loss: 0.022339973598718643\n",
      "Accuracy: 0.9962607026100159\n",
      ">>> Training step No.81:\n",
      "Loss: 0.02498381957411766\n",
      "Accuracy: 0.9962607026100159\n",
      ">>> Training step No.82:\n",
      "Loss: 0.02726437710225582\n",
      "Accuracy: 0.9951924085617065\n",
      ">>> Training step No.83:\n",
      "Loss: 0.02329268679022789\n",
      "Accuracy: 0.9962607622146606\n",
      ">>> Training step No.84:\n",
      "Loss: 0.02727963961660862\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Training step No.85:\n",
      "Loss: 0.02733934484422207\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.86:\n",
      "Loss: 0.025865310803055763\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.87:\n",
      "Loss: 0.02710905857384205\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.88:\n",
      "Loss: 0.023028435185551643\n",
      "Accuracy: 0.9962607026100159\n",
      ">>> Training step No.89:\n",
      "Loss: 0.015965092927217484\n",
      "Accuracy: 0.995726466178894\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ee644f48ee47ee8c3cc09ea3c73f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.42:\n",
      "Loss: 0.030639277771115303\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Validation step No.43:\n",
      "Loss: 0.0322275273501873\n",
      "Accuracy: 0.9946581721305847\n",
      ">>> Validation step No.44:\n",
      "Loss: 0.03255581855773926\n",
      "Accuracy: 0.995192289352417\n",
      ">>> Validation step No.45:\n",
      "Loss: 0.031289465725421906\n",
      "Accuracy: 0.995192289352417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 90: 'val_loss' was not in top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.46:\n",
      "Loss: 0.031584084033966064\n",
      "Accuracy: 0.9946581125259399\n",
      ">>> Epoch end loss: 0.019999999552965164\n",
      ">>> Training step No.90:\n",
      "Loss: 0.026419449597597122\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.91:\n",
      "Loss: 0.02539360150694847\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.92:\n",
      "Loss: 0.021696528419852257\n",
      "Accuracy: 0.995192289352417\n",
      ">>> Training step No.93:\n",
      "Loss: 0.02195451594889164\n",
      "Accuracy: 0.9962607026100159\n",
      ">>> Training step No.94:\n",
      "Loss: 0.021285224705934525\n",
      "Accuracy: 0.9962607026100159\n",
      ">>> Training step No.95:\n",
      "Loss: 0.025742217898368835\n",
      "Accuracy: 0.9951923489570618\n",
      ">>> Training step No.96:\n",
      "Loss: 0.02808966115117073\n",
      "Accuracy: 0.9962607622146606\n",
      ">>> Training step No.97:\n",
      "Loss: 0.025203196331858635\n",
      "Accuracy: 0.995192289352417\n",
      ">>> Training step No.98:\n",
      "Loss: 0.02247701585292816\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.99:\n",
      "Loss: 0.020665157586336136\n",
      "Accuracy: 0.9957265257835388\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36644f7472b4910b3643c5ada320c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.47:\n",
      "Loss: 0.031411007046699524\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Validation step No.48:\n",
      "Loss: 0.031880270689725876\n",
      "Accuracy: 0.995192289352417\n",
      ">>> Validation step No.49:\n",
      "Loss: 0.03245586156845093\n",
      "Accuracy: 0.9946581125259399\n",
      ">>> Validation step No.50:\n",
      "Loss: 0.02989715337753296\n",
      "Accuracy: 0.9957265257835388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 100: 'val_loss' was not in top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.51:\n",
      "Loss: 0.031822048127651215\n",
      "Accuracy: 0.995192289352417\n",
      ">>> Epoch end loss: 0.019999999552965164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/toghrul/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "source": [
    "model = VideoModel.load_from_checkpoint(\n",
    "    checkpoint_path=\"/home/toghrul/SLR/sign-lang/checkpoints/last.ckpt\",\n",
    "    hparams_file=\"/home/toghrul/SLR/sign-lang/lightning_logs/version_38/hparams.yaml\",\n",
    "    map_location=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a869bc32610b4f87a561cf641f4d1d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Label: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Pred: [[-0.4612    0.2253    0.001697 ...  0.693    -0.00444   0.1227  ]\n",
      " [-0.2053    0.342     0.7627   ...  0.6035   -0.3425   -0.04852 ]\n",
      " [ 0.3962    0.5854    0.5273   ...  0.7075   -0.0956    0.3489  ]\n",
      " ...\n",
      " [-0.369     0.71      0.2156   ... -0.07446   0.1896    0.6064  ]\n",
      " [-0.584     0.3564    0.356    ...  0.28      0.14     -0.3337  ]\n",
      " [-0.3782    0.6465    0.548    ...  0.525    -0.1487    0.0789  ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('sign-lang')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96a4e4d8fc4dcb6ce321df308d690f3398dc6d289b3efb6c91f90112c618c739"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
