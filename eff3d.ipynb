{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "import os\n",
    "import cv2\n",
    "import pdb\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torch.nn.functional import InterpolationMode\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import logging \n",
    "import datetime\n",
    "import sys\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gloss_df_path = \"data_validation/processed_gloss.csv\"\n",
    "gloss_df = pd.read_csv(gloss_df_path)\n",
    "gloss_df.dropna(inplace=True)\n",
    "gloss_df.replace(to_replace=\"ASHAG\", value=\"AŞAĞI\", inplace=True)\n",
    "gloss_df['glossRange'] = gloss_df['glossEnd'] - gloss_df['glossStart']\n",
    "# gloss_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(gloss_df.gloss.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchviz\n",
    "from pytorchvideo.data import LabeledVideoDataset, make_clip_sampler\n",
    "from torchvision.models import squeezenet1_1, SqueezeNet1_1_Weights\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in 0.14. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/toghrul/anaconda3/envs/sign-lang/lib/python3.9/site-packages/torchvision/transforms/_transforms_video.py:25: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in 0.14. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pytorchvideo.data import labeled_video_dataset\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    "    Permute\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomAdjustSharpness,\n",
    "    Resize\n",
    ")\n",
    "\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_transform = Compose([\n",
    "    ApplyTransformToKey(key=\"video\",\n",
    "    transform=Compose([\n",
    "        UniformTemporalSubsample(25),\n",
    "        Lambda(lambda x: x/255),\n",
    "        Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "        RandomShortSideScale(min_size=224, max_size=256),\n",
    "        CenterCropVideo(224),\n",
    "    ]),\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/home/toghrul/SLR/data/train\"\n",
    "val_path = \"/home/toghrul/SLR/data/val\"\n",
    "test_path = \"/home/toghrul/SLR/data/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningModule, seed_everything, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.metrics import classification_report\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_model = torch.hub.load('facebookresearch/pytorchvideo', 'efficient_x3d_xs', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.randn(size=(8, num_classes)).requires_grad_()\n",
    "# b = torch.randint(5, size=(8, ), dtype=torch.long)\n",
    "# a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = F.one_hot(b, num_classes=num_classes).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# criterion(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = nn.CrossEntropyLoss()\n",
    "# input = torch.randn(3, 5, requires_grad=True)\n",
    "# target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "# output = loss(input, target)\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoModel(LightningModule):\n",
    "    def __init__(self, ):\n",
    "        super(VideoModel, self).__init__()\n",
    "\n",
    "        self.video_model = torch.hub.load('facebookresearch/pytorchvideo', 'efficient_x3d_xs', pretrained=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(400, num_classes)\n",
    "\n",
    "        self.lr = 1e-3\n",
    "        self.batch_size = 8\n",
    "        self.num_worker = 4\n",
    "        self.num_steps_train = 0\n",
    "        self.num_steps_val = 0\n",
    "\n",
    "        self.metric = torchmetrics.classification.MultilabelAccuracy(num_labels=num_classes)\n",
    "\n",
    "        #loss\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.video_model(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(params=self.parameters(), lr = self.lr)\n",
    "        scheduler = CosineAnnealingLR(opt, T_max=10, eta_min=1e-6, last_epoch=-1)\n",
    "        return {'optimizer': opt,\n",
    "                'lr_scheduler': scheduler}\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = labeled_video_dataset(train_path, clip_sampler=make_clip_sampler('random', 2),\n",
    "                                         transform=video_transform, decode_audio=False)\n",
    "\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, num_workers=self.num_worker, pin_memory=True)\n",
    "\n",
    "        return loader\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        video, label = batch['video'], batch['label']\n",
    "        label = F.one_hot(label, num_classes=num_classes).float()\n",
    "        out = self(video)\n",
    "        print(f\">>> Training step No.{self.num_steps_train}:\")\n",
    "        # print(\"Pred:\", out)\n",
    "        # print(\"GT:\", label)\n",
    "        # print(f\"Pred:\\n{out}\")\n",
    "        # print(f\"Pred shape:\\n{out.shape}\")\n",
    "        # print(f\"Label:\\n{label}\")\n",
    "        # print(f\"Label shape:\\n{label.shape}\")\n",
    "        # print(\">>> INFO: Computing Training Loss\")\n",
    "        loss = self.criterion(out, label)\n",
    "        print(f\"Loss: {loss}\")\n",
    "        self.num_steps_train += 1\n",
    "        # print(\">>> INFO: Training Loss Computed\")\n",
    "        # print(\">>> INFO: Computing Training Metric\")\n",
    "        metric = self.metric(out, label)\n",
    "        print(f\"Accuracy: {metric}\")\n",
    "\n",
    "        values = {\"loss\": loss,\n",
    "                \"metric\": metric.detach()}\n",
    "        \n",
    "        self.log_dict({\"step_loss\": loss,\n",
    "                        \"step_metric\": metric.detach()})\n",
    "        \n",
    "        return values\n",
    "        \n",
    "        # return {\"loss\": loss}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        loss = torch.stack([x['loss'] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        metric = torch.stack([x['metric'] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        \n",
    "        self.log('training_loss', loss)\n",
    "        print(f\">>> Epoch end loss: {loss}\")\n",
    "        self.log('training_metric', metric)\n",
    "        \n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataset = labeled_video_dataset(val_path, clip_sampler=make_clip_sampler('random', 2),\n",
    "                                         transform=video_transform, decode_audio=False)\n",
    "\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, num_workers=self.num_worker, pin_memory=True)\n",
    "\n",
    "        return loader\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        video, label = batch['video'], batch['label']\n",
    "        label = F.one_hot(label, num_classes=num_classes).float()\n",
    "        out = self(video)\n",
    "        # print(\">>> INFO: Computing Val Loss\")\n",
    "        print(f\">>> Validation step No.{self.num_steps_val}:\")\n",
    "        loss = self.criterion(out, label)\n",
    "        print(f\"Loss: {loss}\")\n",
    "        self.num_steps_val += 1\n",
    "        # print(\">>> INFO: Val Loss Computed\")\n",
    "        # print(\">>> INFO: Computing Val Metric\")\n",
    "        metric = self.metric(out, label)\n",
    "        print(f\"Accuracy: {metric}\")\n",
    "        \n",
    "\n",
    "        return {\"loss\": loss,\n",
    "                \"metric\": metric.detach()}\n",
    "        \n",
    "        # return {\"loss\": loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        loss = torch.stack([x['loss'] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        metric = torch.stack([x['metric'] for x in outputs]).mean().cpu().numpy().round(2)\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_metric', metric)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        dataset = labeled_video_dataset(test_path, clip_sampler=make_clip_sampler('random', 2),\n",
    "                                         transform=video_transform, decode_audio=False)\n",
    "\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, num_workers=self.num_worker, pin_memory=True)\n",
    "\n",
    "        return loader\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        video, label = batch['video'], batch['label']\n",
    "        label = F.one_hot(label, num_classes=num_classes).float()\n",
    "        out = self.forward(video)\n",
    "\n",
    "        return {\"label\": label,\n",
    "                \"pred\": out.detach()}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        label=torch.cat([x['label'] for x in outputs]).cpu().numpy()\n",
    "        pred = torch.cat([x['pred'] for x in outputs]).cpu().numpy()\n",
    "\n",
    "        # print(classification_report(label, pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss', dirpath=\"checkpoints\", \n",
    "                                    verbose=True, save_last=True, save_top_k=2)\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"epoch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/toghrul/.cache/torch/hub/facebookresearch_pytorchvideo_main\n",
      "Global seed set to 0\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model = VideoModel()\n",
    "seed_everything(0)\n",
    "\n",
    "trainer = Trainer(max_epochs=10,\n",
    "                accelerator=\"gpu\", devices=-1,\n",
    "                precision=16,\n",
    "                # accumulate_grad_batches=2,\n",
    "                enable_progress_bar=True,\n",
    "                # num_sanity_val_steps=0,\n",
    "                callbacks=[lr_monitor, checkpoint_callback],\n",
    "                log_every_n_steps=5,\n",
    "                limit_train_batches=10,\n",
    "                limit_val_batches=5,\n",
    "                limit_test_batches=5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type               | Params\n",
      "---------------------------------------------------\n",
      "0 | video_model | EfficientX3d       | 3.8 M \n",
      "1 | relu        | ReLU               | 0     \n",
      "2 | fc          | Linear             | 93.8 K\n",
      "3 | metric      | MultilabelAccuracy | 0     \n",
      "4 | criterion   | BCEWithLogitsLoss  | 0     \n",
      "---------------------------------------------------\n",
      "3.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.9 M     Total params\n",
      "7.776     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46b913c1ba3444c280daaaecce64876b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.0:\n",
      "Loss: 0.7293049693107605\n",
      "Accuracy: 0.5438034534454346\n",
      ">>> Validation step No.1:\n",
      "Loss: 0.7303302884101868\n",
      "Accuracy: 0.5507479310035706\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f764fcd4d21478c8a023330b847b9a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training step No.0:\n",
      "Loss: 0.7077091932296753\n",
      "Accuracy: 0.5464743971824646\n",
      ">>> Training step No.1:\n",
      "Loss: 0.5746216177940369\n",
      "Accuracy: 0.7393162846565247\n",
      ">>> Training step No.2:\n",
      "Loss: 0.3870113492012024\n",
      "Accuracy: 0.8616453409194946\n",
      ">>> Training step No.3:\n",
      "Loss: 0.2158113569021225\n",
      "Accuracy: 0.939636766910553\n",
      ">>> Training step No.4:\n",
      "Loss: 0.10342903435230255\n",
      "Accuracy: 0.9780983328819275\n",
      ">>> Training step No.5:\n",
      "Loss: 0.056837841868400574\n",
      "Accuracy: 0.9914530515670776\n",
      ">>> Training step No.6:\n",
      "Loss: 0.03889686241745949\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.7:\n",
      "Loss: 0.040093887597322464\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.8:\n",
      "Loss: 0.04646025225520134\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.9:\n",
      "Loss: 0.04310651496052742\n",
      "Accuracy: 0.9957265853881836\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7faa9796ebb94d91bc0c02f8fcb8f995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.2:\n",
      "Loss: 0.10697975754737854\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Validation step No.3:\n",
      "Loss: 0.11796384304761887\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Validation step No.4:\n",
      "Loss: 0.11326473951339722\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Validation step No.5:\n",
      "Loss: 0.12199532985687256\n",
      "Accuracy: 0.9957265257835388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 10: 'val_loss' reached 0.12000 (best 0.12000), saving model to '/home/toghrul/SLR/sign-lang/checkpoints/epoch=0-step=10.ckpt' as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.6:\n",
      "Loss: 0.12751246988773346\n",
      "Accuracy: 0.995726466178894\n",
      ">>> Epoch end loss: 0.2199999988079071\n",
      ">>> Training step No.10:\n",
      "Loss: 0.04844420775771141\n",
      "Accuracy: 0.995726466178894\n",
      ">>> Training step No.11:\n",
      "Loss: 0.045509763062000275\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Training step No.12:\n",
      "Loss: 0.0638267919421196\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.13:\n",
      "Loss: 0.06256014853715897\n",
      "Accuracy: 0.9962607622146606\n",
      ">>> Training step No.14:\n",
      "Loss: 0.047053053975105286\n",
      "Accuracy: 0.9946581721305847\n",
      ">>> Training step No.15:\n",
      "Loss: 0.04092346131801605\n",
      "Accuracy: 0.9951923489570618\n",
      ">>> Training step No.16:\n",
      "Loss: 0.04732353240251541\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.17:\n",
      "Loss: 0.0523768812417984\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.18:\n",
      "Loss: 0.030742913484573364\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Training step No.19:\n",
      "Loss: 0.04022940993309021\n",
      "Accuracy: 0.995726466178894\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e1d9208e2344faad50b881e0f01b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.7:\n",
      "Loss: 0.06281205266714096\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Validation step No.8:\n",
      "Loss: 0.06427343189716339\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Validation step No.9:\n",
      "Loss: 0.05607280135154724\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Validation step No.10:\n",
      "Loss: 0.06544362753629684\n",
      "Accuracy: 0.9957265257835388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 20: 'val_loss' reached 0.06000 (best 0.06000), saving model to '/home/toghrul/SLR/sign-lang/checkpoints/epoch=1-step=20.ckpt' as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.11:\n",
      "Loss: 0.074832022190094\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Epoch end loss: 0.05000000074505806\n",
      ">>> Training step No.20:\n",
      "Loss: 0.041221294552087784\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.21:\n",
      "Loss: 0.04688849672675133\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.22:\n",
      "Loss: 0.03543812036514282\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.23:\n",
      "Loss: 0.03195163980126381\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.24:\n",
      "Loss: 0.030084848403930664\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.25:\n",
      "Loss: 0.02951720356941223\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.26:\n",
      "Loss: 0.034219879657030106\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.27:\n",
      "Loss: 0.02788941189646721\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.28:\n",
      "Loss: 0.02420848421752453\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Training step No.29:\n",
      "Loss: 0.031377095729112625\n",
      "Accuracy: 0.9957265853881836\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d3346efaef494aa7d3e2aa36316e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.12:\n",
      "Loss: 0.037578243762254715\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Validation step No.13:\n",
      "Loss: 0.03535047173500061\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Validation step No.14:\n",
      "Loss: 0.03592156618833542\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Validation step No.15:\n",
      "Loss: 0.03520483896136284\n",
      "Accuracy: 0.9957265257835388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 30: 'val_loss' reached 0.04000 (best 0.04000), saving model to '/home/toghrul/SLR/sign-lang/checkpoints/epoch=2-step=30.ckpt' as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.16:\n",
      "Loss: 0.03358329087495804\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Epoch end loss: 0.029999999329447746\n",
      ">>> Training step No.30:\n",
      "Loss: 0.03095480240881443\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.31:\n",
      "Loss: 0.031234052032232285\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Training step No.32:\n",
      "Loss: 0.02643049880862236\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.33:\n",
      "Loss: 0.03152305632829666\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Training step No.34:\n",
      "Loss: 0.02625282108783722\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.35:\n",
      "Loss: 0.020671654492616653\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Training step No.36:\n",
      "Loss: 0.021355731412768364\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.37:\n",
      "Loss: 0.028371017426252365\n",
      "Accuracy: 0.995726466178894\n",
      ">>> Training step No.38:\n",
      "Loss: 0.02674509398639202\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.39:\n",
      "Loss: 0.028680317103862762\n",
      "Accuracy: 0.9957265257835388\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1434da581f49faa1285b6c276bf3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.17:\n",
      "Loss: 0.03259549289941788\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Validation step No.18:\n",
      "Loss: 0.03410656377673149\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Validation step No.19:\n",
      "Loss: 0.030004926025867462\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Validation step No.20:\n",
      "Loss: 0.03051418624818325\n",
      "Accuracy: 0.9957265853881836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 40: 'val_loss' reached 0.03000 (best 0.03000), saving model to '/home/toghrul/SLR/sign-lang/checkpoints/epoch=3-step=40.ckpt' as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.21:\n",
      "Loss: 0.031827107071876526\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Epoch end loss: 0.029999999329447746\n",
      ">>> Training step No.40:\n",
      "Loss: 0.02696157805621624\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.41:\n",
      "Loss: 0.018960706889629364\n",
      "Accuracy: 0.995726466178894\n",
      ">>> Training step No.42:\n",
      "Loss: 0.03149951249361038\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.43:\n",
      "Loss: 0.025965692475438118\n",
      "Accuracy: 0.995726466178894\n",
      ">>> Training step No.44:\n",
      "Loss: 0.03104996681213379\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Training step No.45:\n",
      "Loss: 0.028003564104437828\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.46:\n",
      "Loss: 0.02630777098238468\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.47:\n",
      "Loss: 0.02318665012717247\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.48:\n",
      "Loss: 0.033082976937294006\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.49:\n",
      "Loss: 0.022747397422790527\n",
      "Accuracy: 0.9957265257835388\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7078577d6c74bc99060e0b4d56df3cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.22:\n",
      "Loss: 0.03143312782049179\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Validation step No.23:\n",
      "Loss: 0.032869793474674225\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Validation step No.24:\n",
      "Loss: 0.03272216394543648\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Validation step No.25:\n",
      "Loss: 0.031097078695893288\n",
      "Accuracy: 0.9957265257835388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 50: 'val_loss' reached 0.03000 (best 0.03000), saving model to '/home/toghrul/SLR/sign-lang/checkpoints/epoch=4-step=50.ckpt' as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.26:\n",
      "Loss: 0.030635297298431396\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Epoch end loss: 0.029999999329447746\n",
      ">>> Training step No.50:\n",
      "Loss: 0.02659144438803196\n",
      "Accuracy: 0.995726466178894\n",
      ">>> Training step No.51:\n",
      "Loss: 0.0280385073274374\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.52:\n",
      "Loss: 0.02982749231159687\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.53:\n",
      "Loss: 0.02450150065124035\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.54:\n",
      "Loss: 0.026779083535075188\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.55:\n",
      "Loss: 0.02551215887069702\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.56:\n",
      "Loss: 0.02811128832399845\n",
      "Accuracy: 0.995726466178894\n",
      ">>> Training step No.57:\n",
      "Loss: 0.021916117519140244\n",
      "Accuracy: 0.995726466178894\n",
      ">>> Training step No.58:\n",
      "Loss: 0.023967834189534187\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.59:\n",
      "Loss: 0.02954760380089283\n",
      "Accuracy: 0.9957265257835388\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd1d7f267d04ce8a2bf12e9fcf4a8d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.27:\n",
      "Loss: 0.03137540817260742\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Validation step No.28:\n",
      "Loss: 0.030733618885278702\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Validation step No.29:\n",
      "Loss: 0.03213072568178177\n",
      "Accuracy: 0.995726466178894\n",
      ">>> Validation step No.30:\n",
      "Loss: 0.030113110318779945\n",
      "Accuracy: 0.995726466178894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 60: 'val_loss' was not in top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.31:\n",
      "Loss: 0.031316183507442474\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Epoch end loss: 0.029999999329447746\n",
      ">>> Training step No.60:\n",
      "Loss: 0.02793314680457115\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.61:\n",
      "Loss: 0.021278459578752518\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Training step No.62:\n",
      "Loss: 0.021779512986540794\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.63:\n",
      "Loss: 0.02251744642853737\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.64:\n",
      "Loss: 0.0228579118847847\n",
      "Accuracy: 0.9962607026100159\n",
      ">>> Training step No.65:\n",
      "Loss: 0.020932108163833618\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.66:\n",
      "Loss: 0.021492518484592438\n",
      "Accuracy: 0.995726466178894\n",
      ">>> Training step No.67:\n",
      "Loss: 0.023038413375616074\n",
      "Accuracy: 0.995726466178894\n",
      ">>> Training step No.68:\n",
      "Loss: 0.023397106677293777\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.69:\n",
      "Loss: 0.019465647637844086\n",
      "Accuracy: 0.9962607622146606\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7863b00086404d0ba843c6020e120ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.32:\n",
      "Loss: 0.03353780880570412\n",
      "Accuracy: 0.9946581125259399\n",
      ">>> Validation step No.33:\n",
      "Loss: 0.029089398682117462\n",
      "Accuracy: 0.9946581125259399\n",
      ">>> Validation step No.34:\n",
      "Loss: 0.03161735460162163\n",
      "Accuracy: 0.9951923489570618\n",
      ">>> Validation step No.35:\n",
      "Loss: 0.03042745776474476\n",
      "Accuracy: 0.9946581125259399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 70: 'val_loss' was not in top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.36:\n",
      "Loss: 0.03343329578638077\n",
      "Accuracy: 0.9941239356994629\n",
      ">>> Epoch end loss: 0.019999999552965164\n",
      ">>> Training step No.70:\n",
      "Loss: 0.022672448307275772\n",
      "Accuracy: 0.9962607026100159\n",
      ">>> Training step No.71:\n",
      "Loss: 0.02493852749466896\n",
      "Accuracy: 0.995192289352417\n",
      ">>> Training step No.72:\n",
      "Loss: 0.022128595039248466\n",
      "Accuracy: 0.9951923489570618\n",
      ">>> Training step No.73:\n",
      "Loss: 0.02365112490952015\n",
      "Accuracy: 0.9962607622146606\n",
      ">>> Training step No.74:\n",
      "Loss: 0.030309544876217842\n",
      "Accuracy: 0.9951923489570618\n",
      ">>> Training step No.75:\n",
      "Loss: 0.02229502610862255\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.76:\n",
      "Loss: 0.02374919131398201\n",
      "Accuracy: 0.9962607622146606\n",
      ">>> Training step No.77:\n",
      "Loss: 0.027821186929941177\n",
      "Accuracy: 0.9962607026100159\n",
      ">>> Training step No.78:\n",
      "Loss: 0.01947598345577717\n",
      "Accuracy: 0.9967948794364929\n",
      ">>> Training step No.79:\n",
      "Loss: 0.02431837096810341\n",
      "Accuracy: 0.9962607622146606\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52840866eef4a4cad128e3c8b6e1603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.37:\n",
      "Loss: 0.03182431682944298\n",
      "Accuracy: 0.9941239356994629\n",
      ">>> Validation step No.38:\n",
      "Loss: 0.03411296382546425\n",
      "Accuracy: 0.9941239356994629\n",
      ">>> Validation step No.39:\n",
      "Loss: 0.02872391976416111\n",
      "Accuracy: 0.9951923489570618\n",
      ">>> Validation step No.40:\n",
      "Loss: 0.029036955907940865\n",
      "Accuracy: 0.9951923489570618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 80: 'val_loss' was not in top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.41:\n",
      "Loss: 0.03284301981329918\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Epoch end loss: 0.019999999552965164\n",
      ">>> Training step No.80:\n",
      "Loss: 0.022339973598718643\n",
      "Accuracy: 0.9962607026100159\n",
      ">>> Training step No.81:\n",
      "Loss: 0.02498381957411766\n",
      "Accuracy: 0.9962607026100159\n",
      ">>> Training step No.82:\n",
      "Loss: 0.02726437710225582\n",
      "Accuracy: 0.9951924085617065\n",
      ">>> Training step No.83:\n",
      "Loss: 0.02329268679022789\n",
      "Accuracy: 0.9962607622146606\n",
      ">>> Training step No.84:\n",
      "Loss: 0.02727963961660862\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Training step No.85:\n",
      "Loss: 0.02733934484422207\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.86:\n",
      "Loss: 0.025865310803055763\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.87:\n",
      "Loss: 0.02710905857384205\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.88:\n",
      "Loss: 0.023028435185551643\n",
      "Accuracy: 0.9962607026100159\n",
      ">>> Training step No.89:\n",
      "Loss: 0.015965092927217484\n",
      "Accuracy: 0.995726466178894\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ee644f48ee47ee8c3cc09ea3c73f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.42:\n",
      "Loss: 0.030639277771115303\n",
      "Accuracy: 0.9957265853881836\n",
      ">>> Validation step No.43:\n",
      "Loss: 0.0322275273501873\n",
      "Accuracy: 0.9946581721305847\n",
      ">>> Validation step No.44:\n",
      "Loss: 0.03255581855773926\n",
      "Accuracy: 0.995192289352417\n",
      ">>> Validation step No.45:\n",
      "Loss: 0.031289465725421906\n",
      "Accuracy: 0.995192289352417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 90: 'val_loss' was not in top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.46:\n",
      "Loss: 0.031584084033966064\n",
      "Accuracy: 0.9946581125259399\n",
      ">>> Epoch end loss: 0.019999999552965164\n",
      ">>> Training step No.90:\n",
      "Loss: 0.026419449597597122\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.91:\n",
      "Loss: 0.02539360150694847\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.92:\n",
      "Loss: 0.021696528419852257\n",
      "Accuracy: 0.995192289352417\n",
      ">>> Training step No.93:\n",
      "Loss: 0.02195451594889164\n",
      "Accuracy: 0.9962607026100159\n",
      ">>> Training step No.94:\n",
      "Loss: 0.021285224705934525\n",
      "Accuracy: 0.9962607026100159\n",
      ">>> Training step No.95:\n",
      "Loss: 0.025742217898368835\n",
      "Accuracy: 0.9951923489570618\n",
      ">>> Training step No.96:\n",
      "Loss: 0.02808966115117073\n",
      "Accuracy: 0.9962607622146606\n",
      ">>> Training step No.97:\n",
      "Loss: 0.025203196331858635\n",
      "Accuracy: 0.995192289352417\n",
      ">>> Training step No.98:\n",
      "Loss: 0.02247701585292816\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Training step No.99:\n",
      "Loss: 0.020665157586336136\n",
      "Accuracy: 0.9957265257835388\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36644f7472b4910b3643c5ada320c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.47:\n",
      "Loss: 0.031411007046699524\n",
      "Accuracy: 0.9957265257835388\n",
      ">>> Validation step No.48:\n",
      "Loss: 0.031880270689725876\n",
      "Accuracy: 0.995192289352417\n",
      ">>> Validation step No.49:\n",
      "Loss: 0.03245586156845093\n",
      "Accuracy: 0.9946581125259399\n",
      ">>> Validation step No.50:\n",
      "Loss: 0.02989715337753296\n",
      "Accuracy: 0.9957265257835388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 100: 'val_loss' was not in top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Validation step No.51:\n",
      "Loss: 0.031822048127651215\n",
      "Accuracy: 0.995192289352417\n",
      ">>> Epoch end loss: 0.019999999552965164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/toghrul/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "source": [
    "model = VideoModel.load_from_checkpoint(\n",
    "    checkpoint_path=\"/home/toghrul/SLR/sign-lang/checkpoints/last.ckpt\",\n",
    "    hparams_file=\"/home/toghrul/SLR/sign-lang/lightning_logs/version_38/hparams.yaml\",\n",
    "    map_location=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8b317ac4a44b4faa773acb834a87f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multilabel-indicator and continuous-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/toghrul/SLR/sign-lang/eff3d.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/eff3d.ipynb#X51sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtest(model)\n",
      "File \u001b[0;32m~/anaconda3/envs/sign-lang/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:862\u001b[0m, in \u001b[0;36mTrainer.test\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    837\u001b[0m \u001b[39mPerform one evaluation epoch over the test set.\u001b[39;00m\n\u001b[1;32m    838\u001b[0m \u001b[39mIt's separated from fit to make sure you never run on your test set until you want to.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[39m    The length of the list corresponds to the number of test dataloaders used.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module\n\u001b[0;32m--> 862\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_test_impl, model, dataloaders, ckpt_path, verbose, datamodule)\n",
      "File \u001b[0;32m~/anaconda3/envs/sign-lang/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    649\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    651\u001b[0m \u001b[39m# TODO(awaelchli): Unify both exceptions below, where `KeyboardError` doesn't re-raise\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m~/anaconda3/envs/sign-lang/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:909\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tested_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mckpt_path  \u001b[39m# TODO: remove in v1.8\u001b[39;00m\n\u001b[1;32m    908\u001b[0m \u001b[39m# run test\u001b[39;00m\n\u001b[0;32m--> 909\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    911\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    912\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtesting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/sign-lang/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1166\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1166\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1168\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1169\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/anaconda3/envs/sign-lang/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1249\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mdispatch(\u001b[39mself\u001b[39m)\n\u001b[1;32m   1248\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluating:\n\u001b[0;32m-> 1249\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_evaluate()\n\u001b[1;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1251\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n",
      "File \u001b[0;32m~/anaconda3/envs/sign-lang/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1295\u001b[0m, in \u001b[0;36mTrainer._run_evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1292\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m   1294\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrun_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstage\u001b[39m}\u001b[39;00m\u001b[39m_evaluation\u001b[39m\u001b[39m\"\u001b[39m), _evaluation_context(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator):\n\u001b[0;32m-> 1295\u001b[0m     eval_loop_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1297\u001b[0m \u001b[39m# remove the tensors from the eval results\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m eval_loop_results:\n",
      "File \u001b[0;32m~/anaconda3/envs/sign-lang/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py:207\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_run_end()\n\u001b[1;32m    208\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/anaconda3/envs/sign-lang/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:183\u001b[0m, in \u001b[0;36mEvaluationLoop.on_run_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_logger_connector\u001b[39m.\u001b[39mepoch_end_reached()\n\u001b[1;32m    182\u001b[0m \u001b[39m# hook\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_epoch_end(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_outputs)\n\u001b[1;32m    184\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs \u001b[39m=\u001b[39m []  \u001b[39m# free memory\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39m# hook\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/sign-lang/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:293\u001b[0m, in \u001b[0;36mEvaluationLoop._evaluation_epoch_end\u001b[0;34m(self, outputs)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[39m# call the model epoch end\u001b[39;00m\n\u001b[1;32m    292\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_epoch_end\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_epoch_end\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 293\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_lightning_module_hook(hook_name, output_or_outputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/sign-lang/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1550\u001b[0m, in \u001b[0;36mTrainer._call_lightning_module_hook\u001b[0;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1547\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[1;32m   1549\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1550\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1552\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1553\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "\u001b[1;32m/home/toghrul/SLR/sign-lang/eff3d.ipynb Cell 21\u001b[0m in \u001b[0;36mVideoModel.test_epoch_end\u001b[0;34m(self, outputs)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/eff3d.ipynb#X51sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m label\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mcat([x[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m outputs])\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/eff3d.ipynb#X51sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m pred \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([x[\u001b[39m'\u001b[39m\u001b[39mpred\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m outputs])\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/toghrul/SLR/sign-lang/eff3d.ipynb#X51sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m \u001b[39mprint\u001b[39m(classification_report(label, pred))\n",
      "File \u001b[0;32m~/anaconda3/envs/sign-lang/lib/python3.9/site-packages/sklearn/metrics/_classification.py:2125\u001b[0m, in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclassification_report\u001b[39m(\n\u001b[1;32m   2011\u001b[0m     y_true,\n\u001b[1;32m   2012\u001b[0m     y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2019\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   2020\u001b[0m ):\n\u001b[1;32m   2021\u001b[0m     \u001b[39m\"\"\"Build a text report showing the main classification metrics.\u001b[39;00m\n\u001b[1;32m   2022\u001b[0m \n\u001b[1;32m   2023\u001b[0m \u001b[39m    Read more in the :ref:`User Guide <classification_report>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2122\u001b[0m \u001b[39m    <BLANKLINE>\u001b[39;00m\n\u001b[1;32m   2123\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2125\u001b[0m     y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m   2127\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2128\u001b[0m         labels \u001b[39m=\u001b[39m unique_labels(y_true, y_pred)\n",
      "File \u001b[0;32m~/anaconda3/envs/sign-lang/lib/python3.9/site-packages/sklearn/metrics/_classification.py:93\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     90\u001b[0m     y_type \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(y_type) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     94\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mClassification metrics can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt handle a mix of \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m targets\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m     95\u001b[0m             type_true, type_pred\n\u001b[1;32m     96\u001b[0m         )\n\u001b[1;32m     97\u001b[0m     )\n\u001b[1;32m     99\u001b[0m \u001b[39m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[1;32m    100\u001b[0m y_type \u001b[39m=\u001b[39m y_type\u001b[39m.\u001b[39mpop()\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multilabel-indicator and continuous-multioutput targets"
     ]
    }
   ],
   "source": [
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('sign-lang')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96a4e4d8fc4dcb6ce321df308d690f3398dc6d289b3efb6c91f90112c618c739"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
